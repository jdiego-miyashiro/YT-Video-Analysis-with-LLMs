{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHfTUSc_DVoO",
        "outputId": "2d5150f4-6b1a-4a1a-8859-675a415811e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Requirement already satisfied: pytube in c:\\users\\castr\\.conda\\envs\\yt-video-analysis\\lib\\site-packages (15.0.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install langchain\n",
        "!pip install --upgrade --quiet  langchain-openai langchain_community tiktoken chromadb langchain\n",
        "!pip install --upgrade --quiet  youtube-transcript-api\n",
        "!pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j\n",
        "!pip install pytube\n",
        "!pip install langchain-chroma\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "aExh9hAREa-h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_community.document_loaders.youtube import TranscriptFormat\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from io import StringIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0JGV4MtM5Un"
      },
      "source": [
        "# Transcript Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7Z_8X9sDn9N"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"[00:00.000 --> 00:03.900]  2024 is when I started to see a couple of examples of startups,\n",
        "[00:03.900 --> 00:08.680]  especially from the recent YC badge, starting to pivot into web scraping.\n",
        "[00:08.680 --> 00:11.060]  And I'm just here to try to connect the dots here as well.\n",
        "[00:11.060 --> 00:13.140]  This probably has something to do with perplexity and\n",
        "[00:13.140 --> 00:16.220]  the amount of interest in terms of we want to scrape the web so\n",
        "[00:16.220 --> 00:21.220]  we can have the best, I guess, up-to-date answer for your LMs or\n",
        "[00:21.220 --> 00:24.980]  best up-to-date search for a platform.\n",
        "[00:26.260 --> 00:27.580]  Mendable is an example.\n",
        "[00:27.600 --> 00:31.380]  So early days, if you ever were on Lang chain or\n",
        "[00:31.380 --> 00:37.120]  Lama index documentation site, you might see a little robot icon in the corner.\n",
        "[00:37.120 --> 00:40.760]  You click on it, you can do a natural language search query off of\n",
        "[00:40.760 --> 00:41.800]  their documentation site.\n",
        "[00:42.960 --> 00:45.280]  But they came out with this thing recently called Firecrawl,\n",
        "[00:45.280 --> 00:50.560]  which is specifically for scraping the web using large language models.\n",
        "[00:50.560 --> 00:53.580]  And we'll see a live example in a quick second.\n",
        "[00:53.580 --> 00:56.840]  Gina AI, really cool company.\n",
        "[00:56.840 --> 01:00.540]  Their policy is actually, they have embedding models, and\n",
        "[01:00.540 --> 01:02.300]  I think no language models.\n",
        "[01:02.300 --> 01:05.660]  But their embedding models, you can try them without having an API key.\n",
        "[01:05.660 --> 01:07.940]  And I don't know who's footing the bill for this, but\n",
        "[01:07.940 --> 01:11.820]  they keep coming out with really cool, free tools that you can try.\n",
        "[01:11.820 --> 01:17.900]  One of them is reader API, which is, all you gotta do is append\n",
        "[01:17.900 --> 01:22.940]  gnetai.com before any URL, and you're gonna get back some clean data from\n",
        "[01:22.940 --> 01:24.260]  that website.\n",
        "[01:24.260 --> 01:25.300]  It's almost mind-blowing.\n",
        "[01:25.300 --> 01:32.320]  And last, I'm gonna show you this open source project called Scrape Graph AI.\n",
        "[01:32.320 --> 01:36.800]  So this is a very elaborate orchestration of different Python\n",
        "[01:36.800 --> 01:38.480]  modules that create graphs.\n",
        "[01:38.480 --> 01:43.200]  So you can create a pipeline to scrape the web using large language models.\n",
        "[01:43.200 --> 01:48.560]  So these two, it only give you back clean inputs, but\n",
        "[01:48.560 --> 01:53.400]  this one actually incorporate AI in and answer your question at the very end.\n",
        "[01:53.440 --> 01:57.000]  Or you can have ten different steps of what to do when you go to a website and\n",
        "[01:57.000 --> 01:57.880]  scrape it.\n",
        "[01:57.880 --> 02:01.160]  So what I'm gonna do today is I'm actually gonna be scraping my competitor's\n",
        "[02:01.160 --> 02:02.560]  pricing pages.\n",
        "[02:02.560 --> 02:05.720]  I'm doing this for myself, I'm building a product right now.\n",
        "[02:05.720 --> 02:08.040]  And this matters to me.\n",
        "[02:08.040 --> 02:10.560]  This is not a made up use case.\n",
        "[02:10.560 --> 02:14.360]  I'm actually looking forward to see what's gonna come out of this.\n",
        "[02:14.360 --> 02:18.640]  So I'm building in the learning and development space.\n",
        "[02:18.680 --> 02:24.440]  So my competitors are obviously popular tools like Articulate 360,\n",
        "[02:24.440 --> 02:30.040]  some challengers that are new to the market like 7Taps, Mindsmith,\n",
        "[02:30.040 --> 02:30.960]  some other companies.\n",
        "[02:30.960 --> 02:34.080]  But I have about four websites here today, and\n",
        "[02:34.080 --> 02:36.880]  I'm keeping them all in a nice array.\n",
        "[02:37.920 --> 02:41.880]  So what I'm gonna do is I'm just gonna run this to save it in memory.\n",
        "[02:43.560 --> 02:47.120]  And the cool thing about this is that once I give you this, you can go home and\n",
        "[02:47.120 --> 02:50.640]  play around and try to do some market research yourself.\n",
        "[02:50.640 --> 02:58.040]  And now I'm also gonna set up this thing called TickToken.\n",
        "[02:58.040 --> 02:59.800]  Do you guys know what TickToken is?\n",
        "[02:59.800 --> 03:02.480]  I know you know what TickToken is, but I wanna let everyone else know what\n",
        "[03:02.480 --> 03:03.120]  TickToken is.\n",
        "[03:05.280 --> 03:09.800]  Okay, so for a large language model when it's being encoded,\n",
        "[03:09.800 --> 03:12.920]  maybe you can explain this better than me, cuz I'm a software engineer.\n",
        "[03:12.920 --> 03:15.280]  So and it depends on the model too, right?\n",
        "[03:15.360 --> 03:19.600]  Dependent on what kind of tokenization or encoding mechanism that you use.\n",
        "[03:19.600 --> 03:26.400]  For example, GPT-3 had a different encoding scheme than GPT-4 or GPT-4-0.\n",
        "[03:26.400 --> 03:29.240]  That's why they were able to reduce the cost for\n",
        "[03:29.240 --> 03:32.400]  newer generations a little bit just because of the way that tokenization works.\n",
        "[03:33.440 --> 03:35.960]  And you get billed by the number of tokens.\n",
        "[03:35.960 --> 03:37.280]  So less tokens, but\n",
        "[03:37.280 --> 03:41.400]  the same sentence is usually cheaper for you as a consumer.\n",
        "[03:41.400 --> 03:44.560]  So we're using TickToken here, which is straight from OpenAI.\n",
        "[03:44.560 --> 03:49.960]  This is the exact library that OpenAI uses to encode their GPT models.\n",
        "[03:49.960 --> 03:53.680]  We're using this not to create a new model, but we're using this to\n",
        "[03:53.680 --> 03:58.680]  count the number of tokens that we're getting based on the scraped content\n",
        "[03:58.680 --> 04:00.080]  of this website.\n",
        "[04:00.080 --> 04:04.560]  So I wanna know how expensive it is for me to scrape all these things.\n",
        "[04:04.560 --> 04:06.160]  I wanna convert that to a dollar amount.\n",
        "[04:07.520 --> 04:11.880]  Comparing Beautiful Soup and Gina AI and Mendebol.\n",
        "[04:11.880 --> 04:14.680]  See which one will save me the most amount of money.\n",
        "[04:14.680 --> 04:18.880]  So I ran this and this is just an example sentence.\n",
        "[04:18.880 --> 04:22.000]  What's the difference between a Beer Nuts and Deer Nuts?\n",
        "[04:22.000 --> 04:26.400]  Beer Nuts are about $5, Deer Nuts are just under a buck.\n",
        "[04:26.400 --> 04:27.760]  So, anybody get a joke?\n",
        "[04:29.720 --> 04:35.520]  So this one cost about 0.000135 if you're using GPT-4-0.\n",
        "[04:35.520 --> 04:39.320]  And the funny thing about OpenAI is that they didn't update TickToken for\n",
        "[04:39.320 --> 04:40.800]  GPT-4-0 yet.\n",
        "[04:40.800 --> 04:42.360]  So this is just a guesstimate.\n",
        "[04:42.360 --> 04:46.840]  So last thing that I wanna set up is a pretty table.\n",
        "[04:46.840 --> 04:49.200]  So pretty table is just library in Python.\n",
        "[04:49.200 --> 04:51.960]  So you can make tables in your terminal.\n",
        "[04:51.960 --> 04:57.240]  I want rows and columns so I can see which one is cost more than which.\n",
        "[04:58.440 --> 05:01.160]  So that's all I'm doing here, very long function.\n",
        "[05:01.160 --> 05:04.920]  All it's doing here is take into account scraped content,\n",
        "[05:04.920 --> 05:07.760]  put them all in the right columns and rows.\n",
        "[05:07.760 --> 05:08.920]  You can read this at home.\n",
        "[05:09.680 --> 05:13.040]  But yeah, all right, so now let's set up the scrapers.\n",
        "[05:13.040 --> 05:18.120]  First we're gonna install a good old friend, BeautifulSoup4.\n",
        "[05:20.360 --> 05:23.920]  This is the most straightforward way to scrape any website.\n",
        "[05:23.920 --> 05:28.400]  And probably the easiest way for them to detect that you're scraping and\n",
        "[05:28.400 --> 05:29.440]  ban you.\n",
        "[05:29.440 --> 05:34.080]  So it's not very sophisticated unless you bundle on a bunch of other tools.\n",
        "[05:34.080 --> 05:38.360]  Okay, so we got a function to scrape the web with BeautifulSoup.\n",
        "[05:38.400 --> 05:39.960]  We're not gonna use that yet.\n",
        "[05:39.960 --> 05:42.520]  What we're gonna do is we're gonna run all of them at the same time.\n",
        "[05:42.520 --> 05:44.360]  So I'm just gonna set up a bunch of stuff here.\n",
        "[05:44.360 --> 05:46.240]  So here's Gina AI.\n",
        "[05:46.240 --> 05:48.760]  This is what I'm talking about when I say this one is dead simple.\n",
        "[05:50.480 --> 05:54.200]  All you gotta do is just add the string before the actual URL that you wanna\n",
        "[05:54.200 --> 05:55.480]  scrape.\n",
        "[05:55.480 --> 05:56.880]  And it's completely free.\n",
        "[05:56.880 --> 05:59.800]  I don't know what the deal is, but use it, it's free.\n",
        "[06:01.080 --> 06:02.120]  No strings attached.\n",
        "[06:02.120 --> 06:05.960]  I mean, maybe they're trying to do some market research or something.\n",
        "[06:05.960 --> 06:06.880]  But yeah, it's great.\n",
        "[06:06.880 --> 06:08.880]  The jobs of VC money they could-\n",
        "[06:08.880 --> 06:09.480]  Yeah.\n",
        "[06:09.480 --> 06:10.000]  They could blow.\n",
        "[06:11.400 --> 06:12.000]  Absolutely.\n",
        "[06:13.400 --> 06:14.400]  Like free rides and Uber.\n",
        "[06:15.720 --> 06:19.960]  In the early days, you just get free rides over and over.\n",
        "[06:19.960 --> 06:22.560]  All right, last provider is Mendable.\n",
        "[06:22.560 --> 06:24.240]  So Mendable, like I said recently,\n",
        "[06:24.240 --> 06:27.880]  pivoted from, I guess not fully pivoted from, but\n",
        "[06:27.880 --> 06:30.760]  they were doing documentation chatbots.\n",
        "[06:30.760 --> 06:34.120]  So you go on like Langchain, Lama Index, all these companies.\n",
        "[06:34.120 --> 06:36.520]  You can chat with a little chatbot in the bottom.\n",
        "[06:36.520 --> 06:38.080]  So we're gonna try this one too.\n",
        "[06:39.160 --> 06:40.680]  But this one requires an API key.\n",
        "[06:42.760 --> 06:48.440]  And they gave everybody like, I think, 300 tokens for free.\n",
        "[06:48.440 --> 06:52.240]  I'm not sure how many websites you can scrape with that.\n",
        "[06:53.760 --> 06:55.520]  But-\n",
        "[06:55.520 --> 06:56.560]  So from these other tools,\n",
        "[06:56.560 --> 07:01.040]  the output is all text normalized or is it still pretty wrong?\n",
        "[07:01.040 --> 07:03.120]  It's only in markdown.\n",
        "[07:03.120 --> 07:07.880]  So they're all in markdown for some reason.\n",
        "[07:07.880 --> 07:10.280]  Maybe because of large language models.\n",
        "[07:10.280 --> 07:14.640]  Like they made these tools specifically to output markdown from\n",
        "[07:14.640 --> 07:16.120]  a bunch of HTML tags.\n",
        "[07:18.280 --> 07:18.780]  Yeah.\n",
        "[07:21.200 --> 07:22.720]  So it's also like string.\n",
        "[07:22.720 --> 07:24.760]  So there's no like, just pure string.\n",
        "[07:24.760 --> 07:26.040]  You get a string back, like very long.\n",
        "[07:28.440 --> 07:31.160]  Okay, this is the moment that we run everything.\n",
        "[07:33.640 --> 07:34.600]  So I'm just gonna run this.\n",
        "[07:36.040 --> 07:39.440]  All I'm doing here is just go on these websites,\n",
        "[07:39.440 --> 07:42.720]  try to scrape it with all the three different tools that I have.\n",
        "[07:43.720 --> 07:45.080]  It's gonna take about a minute.\n",
        "[07:45.080 --> 07:49.760]  So we can watch this bar going from left to right.\n",
        "[07:49.760 --> 07:50.880]  Are you guys gonna ask questions?\n",
        "[07:50.880 --> 07:52.400]  They got a table back.\n",
        "[07:52.400 --> 07:53.680]  Good thing I got my pretty table so\n",
        "[07:53.680 --> 07:55.720]  I can see the difference between all of them.\n",
        "[07:57.280 --> 07:59.800]  Okay, this is my biggest competitor by the way.\n",
        "[07:59.800 --> 08:01.280]  They have so much money.\n",
        "[08:01.280 --> 08:02.680]  I don't even know what to do with them.\n",
        "[08:03.680 --> 08:04.400]  Beautiful soup.\n",
        "[08:04.400 --> 08:06.600]  You get your regular HTML stuff.\n",
        "[08:08.000 --> 08:09.960]  Very, very clean, but not really.\n",
        "[08:11.480 --> 08:14.200]  Firecrawl, this is firecrawl.\n",
        "[08:14.200 --> 08:19.720]  So for firecrawl you get a little bit better, you know, like very much\n",
        "[08:19.720 --> 08:23.520]  markdown, like you got brackets and you got like links and stuff.\n",
        "[08:23.520 --> 08:27.840]  So you can already see that you can already skip a bunch of stuff here.\n",
        "[08:27.840 --> 08:30.400]  And if your large language model will love you,\n",
        "[08:30.480 --> 08:31.800]  you give it clean data like this.\n",
        "[08:32.840 --> 08:35.480]  But you take a look at Gina on this side.\n",
        "[08:35.480 --> 08:37.840]  Gina is even more human readable.\n",
        "[08:37.840 --> 08:41.240]  Even though they say it's supposed to bring you back markdown,\n",
        "[08:41.240 --> 08:43.720]  they actually took away all the brackets and everything.\n",
        "[08:43.720 --> 08:47.880]  So like you got bad and\n",
        "[08:47.880 --> 08:51.760]  then you got markdown in like actual markdown format.\n",
        "[08:51.760 --> 08:53.800]  And this one is promises you markdown but\n",
        "[08:53.800 --> 08:57.200]  it's actually string in a human readable like format.\n",
        "[08:57.200 --> 09:04.840]  And we can see that for pretty much every single examples here.\n",
        "[09:06.440 --> 09:09.600]  The Gina one is usually the one that you can actually read.\n",
        "[09:09.600 --> 09:14.480]  For example, this one from in the middle by Mendable.\n",
        "[09:14.480 --> 09:16.920]  It's not actually human readable that much.\n",
        "[09:16.920 --> 09:20.920]  Especially if your large language model doesn't care about URLs.\n",
        "[09:20.920 --> 09:23.440]  Maybe you just wanted to know facts.\n",
        "[09:23.440 --> 09:26.800]  Then you might not want all of this stuff in there, right?\n",
        "[09:26.840 --> 09:30.360]  You might want just what's human readable.\n",
        "[09:30.360 --> 09:32.200]  Maybe for a reasoning task, for example.\n",
        "[09:34.280 --> 09:36.200]  Anybody have any questions so far?\n",
        "[09:38.760 --> 09:42.800]  Okay, so that was surprisingly fast.\n",
        "[09:42.800 --> 09:44.880]  All right, so we got also a cost table here.\n",
        "[09:44.880 --> 09:46.800]  Let's take a look at this real quick.\n",
        "[09:48.200 --> 09:50.000]  That's a great question, it reminds me of something.\n",
        "[09:50.000 --> 09:54.040]  I think when I was playing with this, Adobe actually blocked BOS soup.\n",
        "[09:54.040 --> 09:54.560]  So let me show you.\n",
        "[09:57.000 --> 09:59.200]  What I'm gonna do is, I'm gonna just come with these two out.\n",
        "[10:00.240 --> 10:05.240]  And then I wanna show more content and run this again.\n",
        "[10:06.960 --> 10:11.800]  That's a great question, it reminds me of this.\n",
        "[10:11.800 --> 10:12.480]  Okay, let's see.\n",
        "[10:14.240 --> 10:14.840]  Articulate.\n",
        "[10:16.600 --> 10:20.560]  Yeah, so this is like the most barebone, beautiful soup setup.\n",
        "[10:20.560 --> 10:22.920]  Like this is like your intern doing it.\n",
        "[10:22.920 --> 10:25.120]  This is not like very sophisticated.\n",
        "[10:25.120 --> 10:27.440]  So you get four or three.\n",
        "[10:27.440 --> 10:29.600]  That's why there was nothing there.\n",
        "[10:29.600 --> 10:30.920]  So great catch, great catch.\n",
        "[10:30.920 --> 10:32.560]  You can't compare the costs.\n",
        "[10:32.560 --> 10:33.840]  Not enough for the first one.\n",
        "[10:33.840 --> 10:37.400]  The first one was blocked, but actually I'm gonna run this again.\n",
        "[10:38.400 --> 10:43.320]  But the table here, all we're doing is we're comparing the input cost\n",
        "[10:43.320 --> 10:46.280]  between the three, if we were to put this into GPT-4.\n",
        "[10:49.160 --> 10:51.880]  So let's just forget about the first one.\n",
        "[10:52.840 --> 10:59.040]  This one, seven times, seven times less.\n",
        "[11:00.160 --> 11:01.640]  And this one is even way less.\n",
        "[11:03.040 --> 11:07.240]  I compare these two, 100 times, or 70 times.\n",
        "[11:10.000 --> 11:10.720]  Yeah, any questions?\n",
        "[11:10.720 --> 11:15.440]  So right now, these costs are just the cost of just the input tokens and\n",
        "[11:15.440 --> 11:16.520]  output tokens, right?\n",
        "[11:16.520 --> 11:18.200]  Just input tokens.\n",
        "[11:18.200 --> 11:19.440]  For GPT-4.\n",
        "[11:19.440 --> 11:20.520]  Yeah.\n",
        "[11:20.520 --> 11:22.920]  But it has separate cost for input and output, right?\n",
        "[11:22.920 --> 11:25.240]  It does, but this table alone, it's just input.\n",
        "[11:25.240 --> 11:29.280]  Cuz the outputs, I'm gonna show the last step,\n",
        "[11:29.280 --> 11:34.640]  which is using LM to extract our JSON of just the things that I want.\n",
        "[11:34.640 --> 11:38.480]  Which is gonna be pricing tier names and the actual pricing.\n",
        "[11:38.480 --> 11:41.440]  So the output's gonna be the same, regardless.\n",
        "[11:42.560 --> 11:47.040]  But the input, if you use different tools, you get different amount of inputs.\n",
        "[11:47.040 --> 11:54.080]  Yeah, especially this one, for example.\n",
        "[11:54.080 --> 11:54.580]  Yeah.\n",
        "[11:56.080 --> 11:56.840]  It's pretty nuts.\n",
        "[11:56.840 --> 11:59.280]  I guess it's because of the extra tags that are there, right?\n",
        "[11:59.280 --> 12:00.840]  In the fire crawl level.\n",
        "[12:00.840 --> 12:05.880]  Yeah, especially, I think for this one, if you hit an image that has an entire\n",
        "[12:05.880 --> 12:08.720]  binary on it, then you get the entire thing.\n",
        "[12:09.880 --> 12:11.520]  This one also get the entire thing.\n",
        "[12:11.520 --> 12:12.440]  Must have, right?\n",
        "[12:12.440 --> 12:12.960]  Yeah.\n",
        "[12:12.960 --> 12:16.800]  And then, I don't know what they're doing here.\n",
        "[12:16.800 --> 12:18.840]  But they're doing some crazy stuff.\n",
        "[12:20.160 --> 12:22.920]  And this is very clean, by the way.\n",
        "[12:22.920 --> 12:23.840]  It's human readable.\n",
        "[12:25.920 --> 12:29.720]  But again, if you want the actual URLs for\n",
        "[12:29.720 --> 12:32.120]  most things, then you might be better off using this.\n",
        "[12:33.160 --> 12:34.720]  Otherwise, you lose a lot of resolution.\n",
        "[12:36.000 --> 12:40.280]  But again, if you want a reasoning task to be done, then you don't need that.\n",
        "[12:40.280 --> 12:43.320]  You just need factual things.\n",
        "[12:43.320 --> 12:48.080]  All right, so it's pretty obvious which one we should be paying for.\n",
        "[12:49.600 --> 12:51.680]  Not knowing anything else about these companies.\n",
        "[12:51.680 --> 12:58.520]  But now we're going to use OpenAI to kind of like try to do some extraction.\n",
        "[12:58.520 --> 13:00.400]  Because I don't want to look at just the input.\n",
        "[13:00.400 --> 13:05.800]  I want just one JSON with all the data that I was looking for in the beginning.\n",
        "[13:05.800 --> 13:09.480]  Which is, which of these competitors having how many tiers and\n",
        "[13:09.480 --> 13:11.560]  how much does it cost per each tier?\n",
        "[13:11.560 --> 13:12.240]  That's all I want to know.\n",
        "[13:14.040 --> 13:17.960]  Okay, so this part, all I'm doing is setting up an OpenAI client.\n",
        "[13:19.120 --> 13:22.040]  And then I'm going to use my OpenAI key here.\n",
        "[13:23.360 --> 13:25.920]  So we got a fresh key this week.\n",
        "[13:27.920 --> 13:29.400]  Key from last week has been deprecated.\n",
        "[13:31.080 --> 13:34.000]  And I'm using the latest and greatest GPT-4.\n",
        "[13:34.000 --> 13:36.960]  Do you guys know what the O stands for?\n",
        "[13:36.960 --> 13:38.400]  There you go.\n",
        "[13:39.200 --> 13:41.320]  Do you guys know why in the demos,\n",
        "[13:41.320 --> 13:45.400]  like GPT-4, or chat GPT sounds so flirtatious?\n",
        "[13:45.400 --> 13:53.400]  Yeah, there's so many memes about that on Twitter.\n",
        "[13:53.400 --> 13:58.280]  But okay, so this is just a utility function to then display\n",
        "[13:58.280 --> 14:01.160]  the extracted content on another table.\n",
        "[14:02.520 --> 14:07.240]  Because table and console is what we need to compare these things.\n",
        "[14:08.160 --> 14:13.280]  All right, so what I'm doing here is just run GPT-4.0 through each of\n",
        "[14:13.280 --> 14:16.680]  the inputs that we got before, like the entire input.\n",
        "[14:16.680 --> 14:20.320]  So it could be like 50,000 tokens.\n",
        "[14:20.320 --> 14:23.040]  Like it's not my money, it's Invest Ottawa's money.\n",
        "[14:23.040 --> 14:26.760]  So I'm just running it through GPT-4.0 right now.\n",
        "[14:26.760 --> 14:42.680]  Okay, so very, very simple entity extraction task using GPT-4.0.\n",
        "[14:42.680 --> 14:47.760]  All I did was I put in a kind of like a chain,\n",
        "[14:47.760 --> 14:51.880]  an LLM chain, and just be like, do, do, do, do, do, do.\n",
        "[14:51.880 --> 14:56.480]  Give me the three pricing tiers from this website's content.\n",
        "[14:57.160 --> 14:59.120]  So I give one website at a time.\n",
        "[14:59.120 --> 15:01.920]  And then we turn a JSON with three keys.\n",
        "[15:01.920 --> 15:06.400]  Cheapest, which is the cheapest tier, name of it, and then a price.\n",
        "[15:06.400 --> 15:08.440]  And then I just give each one like a type, so\n",
        "[15:08.440 --> 15:10.560]  that it knows what I'm actually looking for.\n",
        "[15:11.600 --> 15:14.800]  And then the middle tier is the most expensive one.\n",
        "[15:14.800 --> 15:17.480]  And that's it, that's my extraction tier.\n",
        "[15:17.480 --> 15:22.280]  And always, what OpenAI does is it tells you that you should always,\n",
        "[15:22.280 --> 15:25.440]  if you want JSON back, use type JSON object, but\n",
        "[15:25.440 --> 15:27.640]  also say in your prompt that you want JSON back.\n",
        "[15:27.640 --> 15:29.680]  So you always need those two things.\n",
        "[15:30.920 --> 15:35.200]  But it's not really surprising that from BeautifulSuit we get nothing, so\n",
        "[15:35.200 --> 15:37.640]  we can't extract anything here.\n",
        "[15:37.640 --> 15:42.160]  Our JSON is completely zero and empty string,\n",
        "[15:42.160 --> 15:47.760]  because we got 403 forbidden, we got forbidden from Adobe Articulate 360.\n",
        "[15:49.240 --> 15:54.040]  Firecrawl seems like it's been able to get personal plan 0199,\n",
        "[15:54.040 --> 16:01.120]  Teams plan 399, and then Reach 360 Pro, which is the enterprise tier.\n",
        "[16:01.120 --> 16:04.240]  And the price is contact sales for pricing.\n",
        "[16:04.240 --> 16:07.960]  This is weird because I gave it a type which was float, and\n",
        "[16:07.960 --> 16:09.640]  it gave me a string here.\n",
        "[16:09.640 --> 16:14.720]  It's still useful, but not technically accurate.\n",
        "[16:16.080 --> 16:20.720]  And it comes out to GNI, price as variable.\n",
        "[16:20.720 --> 16:25.200]  So they're kind of ignoring my instructions a little bit.\n",
        "[16:25.200 --> 16:30.720]  Maybe I should have said float or no if it's not found or something like that.\n",
        "[16:30.720 --> 16:35.000]  It just goes to show if you're working with large language models,\n",
        "[16:35.000 --> 16:37.320]  unless you're working with dspy or something like that,\n",
        "[16:37.320 --> 16:40.240]  you need to be very specific about your prompt.\n",
        "[16:40.240 --> 16:42.480]  And just take care of most edge cases.\n",
        "[16:43.160 --> 16:50.600]  So again, it seems like most of these tools pass the test.\n",
        "[16:52.600 --> 16:54.720]  It's just a matter of whether or\n",
        "[16:54.720 --> 16:57.760]  not you're gonna burn ten times the amount of money and\n",
        "[16:57.760 --> 17:01.480]  do scraping manually using Beelzebub and your custom tools.\n",
        "[17:01.480 --> 17:05.840]  Or you can kind of use one of these third party tools and just get\n",
        "[17:05.840 --> 17:10.080]  clean markdown or clean user human readable text back.\n",
        "[17:10.120 --> 17:14.160]  And just worry about your LLM stack instead of your web scraping.\n",
        "[17:16.160 --> 17:20.080]  One last thing I wanna show you guys, which is scrape graph.\n",
        "[17:21.600 --> 17:23.880]  So this is completely open source.\n",
        "[17:23.880 --> 17:27.320]  This is not like the startups that we just saw earlier.\n",
        "[17:27.320 --> 17:29.080]  This is completely open source and in Python.\n",
        "[17:30.920 --> 17:34.360]  Anybody here familiar with graph data structure?\n",
        "[17:36.000 --> 17:37.640]  I think we had a conversation before about this.\n",
        "[17:37.640 --> 17:41.800]  It seems like you're a big fan of graph data structure too.\n",
        "[17:41.800 --> 17:42.300]  Yeah.\n",
        "[17:44.520 --> 17:47.760]  Okay, so open API key.\n",
        "[17:49.560 --> 17:50.800]  And then link to script.\n",
        "[17:50.800 --> 17:52.400]  Does anyone have a link that they wanna scrape?\n",
        "[17:52.400 --> 17:55.000]  Which model you're gonna use?\n",
        "[17:55.000 --> 17:56.160]  Okay, so this is what the website looks like.\n",
        "[17:58.440 --> 17:59.240]  That's actually pretty cool.\n",
        "[17:59.240 --> 18:02.440]  So what would be a question that you wanna ask?\n",
        "[18:02.440 --> 18:06.320]  What I meant to say is, what do you wanna get out of this website?\n",
        "[18:06.320 --> 18:09.160]  So model and speed graph.\n",
        "[18:09.160 --> 18:09.660]  Yeah.\n",
        "[18:11.160 --> 18:11.800]  What are these called?\n",
        "[18:11.800 --> 18:13.840]  Electric unicycle.\n",
        "[18:13.840 --> 18:14.360]  Okay.\n",
        "[18:15.280 --> 18:15.960]  LA.\n",
        "[18:15.960 --> 18:16.960]  EUC.\n",
        "[18:16.960 --> 18:19.880]  And what color is the fastest model of the universe?\n",
        "[18:19.880 --> 18:24.080]  Electric unicycle and.\n",
        "[18:24.080 --> 18:24.880]  Model and weight.\n",
        "[18:24.880 --> 18:26.960]  Model and name and speed.\n",
        "[18:28.640 --> 18:29.140]  Okay.\n",
        "[18:31.200 --> 18:33.520]  Yeah, initially I thought that you can just ask questions, but\n",
        "[18:34.400 --> 18:38.680]  the prompt here is just what do you wanna get or scrape out of the website.\n",
        "[18:38.680 --> 18:40.600]  Yeah, not like what do you wanna know by the website.\n",
        "[18:42.040 --> 18:45.240]  Okay, so we got our answers back.\n",
        "[18:45.240 --> 18:47.720]  So we got here, how do I get rid of this?\n",
        "[18:49.840 --> 18:52.360]  Okay, so electric unicycle models.\n",
        "[18:53.480 --> 18:55.480]  So you tell me if these models are legit.\n",
        "[18:55.480 --> 18:56.280]  Yeah, yeah.\n",
        "[18:56.280 --> 18:58.920]  I saw the names are correct and speed is correct.\n",
        "[18:58.920 --> 19:00.360]  Which one's yours?\n",
        "[19:00.360 --> 19:01.800]  It's the Gold Master.\n",
        "[19:01.800 --> 19:02.960]  This one.\n",
        "[19:03.480 --> 19:04.480]  50 plus miles per hour.\n",
        "[19:04.480 --> 19:05.280]  Is that true?\n",
        "[19:05.280 --> 19:05.920]  That's true.\n",
        "[19:05.920 --> 19:07.120]  On full battery.\n",
        "[19:07.120 --> 19:07.640]  Okay.\n",
        "[19:07.640 --> 19:08.960]  Are you serious?\n",
        "[19:08.960 --> 19:14.480]  You know, I drove like 72 kilometers per hour, my top speed, and it was scary.\n",
        "[19:16.080 --> 19:20.480]  You see like all the, all everything like flying by you and think,\n",
        "[19:20.480 --> 19:26.400]  if I'm gonna fall and hit something, it's not gonna, my body protection not gonna help.\n",
        "[19:26.400 --> 19:27.000]  No.\n",
        "[19:27.000 --> 19:27.480]  Wow.\n",
        "[19:29.800 --> 19:31.440]  So that's pretty, yeah.\n",
        "[19:31.440 --> 19:32.720]  Jesus.\n",
        "[19:32.840 --> 19:33.880]  So this is accurate.\n",
        "[19:33.880 --> 19:35.360]  That's accurate.\n",
        "[19:35.360 --> 19:37.840]  V13, yeah.\n",
        "[19:37.840 --> 19:39.160]  You know what?\n",
        "[19:39.160 --> 19:44.600]  I actually, I actually went on their project and I asked, cuz I couldn't figure it out.\n",
        "[19:44.600 --> 19:48.120]  Cuz I was trying to build this and I'm like, how many tokens is this consuming?\n",
        "[19:48.120 --> 19:49.360]  Probably a lot.\n",
        "[19:49.360 --> 19:51.080]  So I ask.\n",
        "[19:51.080 --> 19:54.840]  And apparently there's like a function that you can do to get that.\n",
        "[19:57.080 --> 19:58.080]  There's two ways.\n",
        "[19:58.120 --> 20:03.360]  So if you use this library, just go to the discussions tab,\n",
        "[20:03.360 --> 20:05.400]  look at my question and you'll find the answer.\n",
        "[20:07.680 --> 20:09.480]  Yeah, I asked this like last night.\n",
        "[20:09.480 --> 20:10.280]  I'm like, wait, wait.\n",
        "[20:11.440 --> 20:13.760]  Are you trying to like hide this away from us or something?\n",
        "[20:13.760 --> 20:16.520]  Like no, there's two ways to do it, but the documentation doesn't cover it, so.\n",
        "[20:18.520 --> 20:19.800]  But yeah, that's all I got.\n",
        "[20:19.800 --> 20:21.600]  Does anybody have any questions about anything?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FryjW6HoRXf"
      },
      "outputs": [],
      "source": [
        "text_2 = \"\"\"# tactiq.io free youtube transcript\n",
        "# DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\n",
        "# https://www.youtube.com/watch/Hp7P99rsRpo\n",
        "\n",
        "00:00:00.399 uh\n",
        "00:00:01.360 speaker is lag\n",
        "00:00:03.280 he going to talk about\n",
        "00:00:05.839 the vortex ai which is newly released in\n",
        "00:00:08.720 the i think a few months back in the\n",
        "00:00:11.200 google i o\n",
        "00:00:12.480 events it's\n",
        "00:00:14.080 a\n",
        "00:00:14.799 great platforms to unified many of the\n",
        "00:00:18.480 resources for\n",
        "00:00:20.160 machine learning and ai tasks\n",
        "00:00:22.640 lag he is the director for that\n",
        "00:00:26.080 analytics and ai solutions on google\n",
        "00:00:28.480 cloud and his team builds software\n",
        "00:00:31.279 solutions for business problems using\n",
        "00:00:33.280 google cloud data analytics and machine\n",
        "00:00:35.680 learning products\n",
        "00:00:37.280 he founded the google\n",
        "00:00:38.960 adwords solution lab machine learning\n",
        "00:00:42.079 immersion program and it's also of\n",
        "00:00:44.079 several oiling books and correlate\n",
        "00:00:46.800 courses\n",
        "00:00:48.399 without further ado let's welcome\n",
        "00:00:50.800 lack\n",
        "00:00:52.000 thank you bill thank hello everyone\n",
        "00:00:55.039 so today i'm going to be talking about\n",
        "00:00:57.600 machine learning pipelines specifically\n",
        "00:01:00.760 pre-fabricated machine learning\n",
        "00:01:02.399 pipelines in vertex ai\n",
        "00:01:05.040 so what is vertex ai it's essentially a\n",
        "00:01:08.080 set of tools for predictions for\n",
        "00:01:10.159 training\n",
        "00:01:11.280 for evaluation for monitoring so\n",
        "00:01:14.479 essentially a set of tools that supports\n",
        "00:01:16.479 your entire workflow\n",
        "00:01:18.479 across different model types auto ml\n",
        "00:01:21.200 tensorflow pytorch\n",
        "00:01:23.280 scikit learn right whatever framework\n",
        "00:01:25.200 you're using and different levels of ml\n",
        "00:01:28.080 expertise whether you want to use low\n",
        "00:01:30.479 code whether you want to use no code\n",
        "00:01:32.880 with auto ml or you want to basically\n",
        "00:01:35.600 use low code with bigquery ml or you\n",
        "00:01:38.320 want to basically do sophisticated\n",
        "00:01:41.200 programming with tensorflow or pytorch\n",
        "00:01:43.680 it supports all of the different types\n",
        "00:01:45.680 of ml expertise so what's in vertex ai\n",
        "00:01:49.600 if you think about the machine learning\n",
        "00:01:52.399 workflow on the top data readiness\n",
        "00:01:55.119 getting your data ready labeling it for\n",
        "00:01:57.280 example feature engineering right\n",
        "00:01:59.920 creating a data set training a hyper\n",
        "00:02:02.560 parameter tuning\n",
        "00:02:04.079 then serving the model\n",
        "00:02:06.240 understanding it tuning it deploying it\n",
        "00:02:09.280 to the edge monitoring the model\n",
        "00:02:11.840 managing the model\n",
        "00:02:13.520 like there's like different\n",
        "00:02:15.280 tools that support that full end-to-end\n",
        "00:02:18.319 workflow and again as i said you could\n",
        "00:02:21.360 do it with no code with automl for\n",
        "00:02:23.680 things like vision video language\n",
        "00:02:26.800 translation so when we talk about automl\n",
        "00:02:29.120 what we're basically saying is using the\n",
        "00:02:32.080 state-of-the-art models with uh you know\n",
        "00:02:35.120 you know with that with a like point and\n",
        "00:02:37.680 click interface but at the same time you\n",
        "00:02:40.560 can also build things yourself so you\n",
        "00:02:43.360 can you know build a train now you can\n",
        "00:02:45.360 build things that can be trained hyper\n",
        "00:02:47.040 parameter tune run experiments use gpus\n",
        "00:02:50.959 do predictions explainability all of\n",
        "00:02:53.440 these things\n",
        "00:02:54.800 and as you build an ml pipeline right uh\n",
        "00:02:57.920 you will want to orchestrate it you\n",
        "00:02:59.599 don't want to basically do them one at a\n",
        "00:03:02.080 time you want to connect them together\n",
        "00:03:04.000 and that connection is what a pipeline\n",
        "00:03:05.920 is and all of these are a unified\n",
        "00:03:08.800 workflow so once you create a data set\n",
        "00:03:12.480 you can use that same data set to train\n",
        "00:03:14.879 both an automl model and a custom model\n",
        "00:03:17.920 so you're not\n",
        "00:03:19.200 doing completely separate workflows\n",
        "00:03:22.560 just because you're changing the way\n",
        "00:03:24.879 you're doing like for example if you\n",
        "00:03:26.879 have things in a feature store you can\n",
        "00:03:28.799 use it to support both pi torch and\n",
        "00:03:31.120 tensorflow models that's the basic idea\n",
        "00:03:33.840 is that all of these things are are\n",
        "00:03:37.440 loosely connected and that you you can\n",
        "00:03:39.840 combine them in the way that you want to\n",
        "00:03:42.959 solve your problem\n",
        "00:03:44.480 so the tools that you're using can be\n",
        "00:03:47.040 like as i said user interface based low\n",
        "00:03:50.159 code or it can be programmatic so\n",
        "00:03:52.879 everything that you can do on the ui you\n",
        "00:03:54.560 can also do\n",
        "00:03:56.000 with a nice software development kit and\n",
        "00:03:58.640 sdk\n",
        "00:04:00.239 and of course today i'm going to be\n",
        "00:04:01.840 focusing primarily on the right hand\n",
        "00:04:04.000 side using the sdk to develop models\n",
        "00:04:07.360 so but once you develop a model\n",
        "00:04:09.760 that's not it right so\n",
        "00:04:12.000 you develop the model you deploy it but\n",
        "00:04:14.879 then you want to basically monitor it\n",
        "00:04:17.358 retrain it update it and once you have\n",
        "00:04:20.478 these kinds of complex\n",
        "00:04:22.880 different steps that you want to do\n",
        "00:04:25.440 automation becomes extremely important\n",
        "00:04:27.520 and that's where pipelines come in so\n",
        "00:04:29.680 when you think about a very simple view\n",
        "00:04:32.160 of a pipeline it lets you do\n",
        "00:04:34.759 experimentation try out different hyper\n",
        "00:04:37.199 parameters try out different types of\n",
        "00:04:39.919 models etc it allows you to do\n",
        "00:04:43.280 training and later on to retrain the\n",
        "00:04:45.680 model\n",
        "00:04:46.639 it allows you to deploy the model and\n",
        "00:04:49.120 then monitor it and you want to\n",
        "00:04:51.440 basically do a variety of things during\n",
        "00:04:54.160 that entire process and that's what\n",
        "00:04:55.919 pipelines help you do\n",
        "00:04:58.960 now how does vertex ai help there well\n",
        "00:05:02.320 with all of the automation it helps you\n",
        "00:05:04.400 do things like experiments try out\n",
        "00:05:06.240 different things it helps you\n",
        "00:05:08.720 reuse features it helps you do\n",
        "00:05:10.560 continuous monitoring helps you manage\n",
        "00:05:13.280 metadata throughout the entire process\n",
        "00:05:15.919 but most of all it allows you to\n",
        "00:05:18.160 orchestrate all of these things\n",
        "00:05:20.600 automatically with caching with\n",
        "00:05:23.440 dependency tracking in an ml pipeline\n",
        "00:05:28.000 however\n",
        "00:05:29.759 pipelines can be hard\n",
        "00:05:32.400 and one of the things that\n",
        "00:05:34.160 we firmly believe in\n",
        "00:05:36.240 is that simple things should be simple\n",
        "00:05:39.600 and hard things should be possible so\n",
        "00:05:42.720 pipelines make hard things possible\n",
        "00:05:45.680 right so when you talk about a pipeline\n",
        "00:05:47.600 you're talking about hard things like av\n",
        "00:05:50.720 testing performance monitoring uh uh\n",
        "00:05:54.240 comparing different experiments etc so\n",
        "00:05:56.560 that's great\n",
        "00:05:58.000 it's great that lets you do these hard\n",
        "00:05:59.840 things it lets you do feature stores\n",
        "00:06:02.240 experiments continuous monitoring edge\n",
        "00:06:04.639 deployment metadata no tracking\n",
        "00:06:08.319 but what if you want to do something\n",
        "00:06:09.680 simple\n",
        "00:06:11.520 all i want to do is to develop a model\n",
        "00:06:14.160 and in order to do that i want to\n",
        "00:06:15.759 basically create a data set that i can\n",
        "00:06:18.560 version that i know\n",
        "00:06:20.400 what data i'm using to train my model i\n",
        "00:06:22.639 want to train my model\n",
        "00:06:24.479 and not just train one model i want to\n",
        "00:06:26.560 tune the hyper parameters i want to also\n",
        "00:06:29.360 try out auto ml so a little bit of\n",
        "00:06:31.440 experimentation\n",
        "00:06:33.039 deploy that model and get predictions\n",
        "00:06:36.560 but this is the simple workflow\n",
        "00:06:39.280 and i think\n",
        "00:06:40.639 this workflow\n",
        "00:06:42.479 is like 90\n",
        "00:06:45.360 of what\n",
        "00:06:46.800 you will do day to day right so\n",
        "00:06:49.599 how do you do this simple workflow let's\n",
        "00:06:51.599 forget all the complexities let's forget\n",
        "00:06:53.840 all the sophisticated stuff how do you\n",
        "00:06:56.080 do this simple workflow developing a\n",
        "00:06:58.080 model training it tuning it automl\n",
        "00:07:01.039 deployed to an endpoint get predictions\n",
        "00:07:04.000 how do we do that in a simple way right\n",
        "00:07:06.639 so let's start\n",
        "00:07:08.240 first step is that you want to develop a\n",
        "00:07:10.160 model this is the thing that every data\n",
        "00:07:12.639 scientist like you know every tutorial\n",
        "00:07:15.039 that's what it talks about right so you\n",
        "00:07:16.960 go to a notebook you write some code and\n",
        "00:07:20.400 in that code you can basically you know\n",
        "00:07:22.960 have direct connections to things like\n",
        "00:07:25.120 bigquery data proc spark\n",
        "00:07:27.919 other things you want to be able to\n",
        "00:07:29.840 maybe even schedule the notebook you\n",
        "00:07:32.319 want to write the code in pytorch or you\n",
        "00:07:34.960 want to write the code in tensorflow etc\n",
        "00:07:37.360 so you do that in a notebook in in\n",
        "00:07:40.560 vertex ai that's a vertex ai workbench\n",
        "00:07:44.400 so inside the notebook then you\n",
        "00:07:47.199 basically will write keras code that's\n",
        "00:07:49.599 going to be my example so you will read\n",
        "00:07:51.680 a data set create a training data set\n",
        "00:07:53.840 you will create an evaluation data set\n",
        "00:07:56.080 you will create a keras model you will\n",
        "00:07:58.720 basically train the model\n",
        "00:08:01.199 and you will save the model this is your\n",
        "00:08:03.599 normal\n",
        "00:08:04.800 keras workflow\n",
        "00:08:06.560 and like let's just assume that that's\n",
        "00:08:08.639 basically what we're going to do so what\n",
        "00:08:10.479 i'm doing here is that i'm just opening\n",
        "00:08:12.400 up jupiter lab right right from my\n",
        "00:08:15.759 vertex ai and at this point\n",
        "00:08:18.800 i'm basically in the jupyter ui i can\n",
        "00:08:21.440 get clone the repo here\n",
        "00:08:23.759 right but let me just go ahead and do it\n",
        "00:08:25.759 from the command line and let me do git\n",
        "00:08:28.639 clone\n",
        "00:08:30.960 github.com\n",
        "00:08:34.958 um and\n",
        "00:08:36.799 i'll do the data science on gcp\n",
        "00:08:40.880 okay\n",
        "00:08:42.000 and\n",
        "00:08:43.039 move into that\n",
        "00:08:44.800 let me move to the branch edition too\n",
        "00:08:48.640 right so there we are\n",
        "00:08:50.640 and in here\n",
        "00:08:52.480 is\n",
        "00:08:53.519 my notebook\n",
        "00:08:54.959 right so i can basically go ahead and\n",
        "00:08:57.680 say that i want to basically use python\n",
        "00:08:59.680 3 right now i'm in my notebook and this\n",
        "00:09:03.519 is should be pretty familiar to anybody\n",
        "00:09:05.680 right so if i want right i basically out\n",
        "00:09:08.800 here i'm basically saying that i want to\n",
        "00:09:10.160 train my notebook on us central one\n",
        "00:09:12.880 right this is my bucket this is my\n",
        "00:09:15.120 project and here i'm running a bigquery\n",
        "00:09:18.800 query to basically go pull some data do\n",
        "00:09:22.399 some data preparation and create a brand\n",
        "00:09:25.200 new table called training data and a\n",
        "00:09:27.920 brand another new table called\n",
        "00:09:29.680 evaluation data right and then\n",
        "00:09:32.640 having done that i'm basically going\n",
        "00:09:35.200 ahead and extracting it out into cloud\n",
        "00:09:38.320 storage i can read the data directly\n",
        "00:09:40.800 from bigquery there's a bigquery reader\n",
        "00:09:42.800 in tensorflow but it turns out that if\n",
        "00:09:45.600 you have it in\n",
        "00:09:47.440 cloud storage it it gives me a few other\n",
        "00:09:50.080 benefits that again i i go through later\n",
        "00:09:53.279 but let's not worry about it so in this\n",
        "00:09:54.959 case rather than training directly of\n",
        "00:09:57.360 bigquery i'm actually extracting it out\n",
        "00:09:59.360 into a csv file and i basically get\n",
        "00:10:02.000 three csv files evaluation training and\n",
        "00:10:05.760 all\n",
        "00:10:06.720 and basically this is all just normal\n",
        "00:10:10.079 code i'm using tf data\n",
        "00:10:12.399 reading the data set making sure the\n",
        "00:10:14.720 data is okay\n",
        "00:10:16.000 then going ahead and creating my\n",
        "00:10:19.279 wide and deep model doing feature\n",
        "00:10:21.519 engineering on it training the model\n",
        "00:10:24.640 and now i have my model this is my\n",
        "00:10:27.600 entire keras model with a whole bunch of\n",
        "00:10:29.920 inputs with a few dense layers\n",
        "00:10:32.480 and\n",
        "00:10:33.360 having done this\n",
        "00:10:34.959 i now train my model i have my model\n",
        "00:10:38.480 it all works so this is basically the\n",
        "00:10:40.959 first part your part of your data\n",
        "00:10:42.640 science workflow and then if you want\n",
        "00:10:45.360 you can go ahead and take that model and\n",
        "00:10:47.279 deploy it using gcloud right so you can\n",
        "00:10:50.160 deploy the model using gcloud\n",
        "00:10:52.560 into\n",
        "00:10:53.680 vertex ai so\n",
        "00:10:56.000 that's one way this this would work\n",
        "00:10:58.399 and then i can take this entire\n",
        "00:11:01.440 notebook\n",
        "00:11:02.880 and in vertex ai i can basically\n",
        "00:11:04.880 schedule the notebook i can say okay i\n",
        "00:11:07.360 would like to go to this notebook\n",
        "00:11:09.519 and i would like to like schedule it\n",
        "00:11:12.480 right so i would like to run a scheduled\n",
        "00:11:14.240 run of this notebook at certain times\n",
        "00:11:17.120 and then because this notebook does\n",
        "00:11:19.279 everything from\n",
        "00:11:20.959 data preparation\n",
        "00:11:22.959 all the way to model deployment\n",
        "00:11:26.399 i have everything ready and it will\n",
        "00:11:29.440 basically\n",
        "00:11:30.720 uh be be a runnable thing\n",
        "00:11:33.519 so that's obviously one way that i could\n",
        "00:11:35.360 do this\n",
        "00:11:36.480 but it doesn't really support the kind\n",
        "00:11:38.800 of orchestration and\n",
        "00:11:40.720 everything that i want to do i want to\n",
        "00:11:42.079 automate it right so what do i so one of\n",
        "00:11:44.800 the things that i really\n",
        "00:11:47.200 believe\n",
        "00:11:48.240 is that notebooks are great for\n",
        "00:11:49.760 development\n",
        "00:11:51.360 um and you can use the notebook as a\n",
        "00:11:53.760 step of a pipeline you can schedule the\n",
        "00:11:55.839 entire notebook but really\n",
        "00:11:58.480 right for production purposes\n",
        "00:12:01.360 put your code in python files right\n",
        "00:12:03.680 because you can basically manage them\n",
        "00:12:05.680 and maintain them using your traditional\n",
        "00:12:08.639 software workflows they like notebooks\n",
        "00:12:11.519 are great for data scientists but for ml\n",
        "00:12:14.399 ops for operations for maintainability\n",
        "00:12:18.160 nothing beats having flat files having\n",
        "00:12:21.120 python files so what i've also then done\n",
        "00:12:24.399 once i've developed the notebook\n",
        "00:12:26.560 right what i did was i basically took\n",
        "00:12:28.880 all of the code in this notebook\n",
        "00:12:31.120 and moved it into a file called\n",
        "00:12:32.959 model.pipe\n",
        "00:12:34.480 right it's pretty easy to do you can\n",
        "00:12:36.240 basically go into your notebook and you\n",
        "00:12:38.639 can basically export the notebook as a\n",
        "00:12:41.440 uh\n",
        "00:12:42.240 not exported yeah save the notebook as a\n",
        "00:12:44.480 python file right and then basically\n",
        "00:12:47.360 remove all of the display stuff and you\n",
        "00:12:49.839 basically have it so all of this code is\n",
        "00:12:52.399 essentially the code that was in that\n",
        "00:12:54.959 notebook\n",
        "00:12:56.160 but it's essentially now callable\n",
        "00:12:58.880 and i added in\n",
        "00:13:00.480 a main right so that i could basically\n",
        "00:13:02.639 call it with a bunch of parameters like\n",
        "00:13:05.920 specifying the bucket specifying their\n",
        "00:13:08.399 training batch size etc so once i have\n",
        "00:13:12.160 that notebook\n",
        "00:13:14.160 right and i've basically gotten this we\n",
        "00:13:16.800 can now go ahead and try it out so this\n",
        "00:13:19.839 model here\n",
        "00:13:21.279 is is now\n",
        "00:13:23.200 workable\n",
        "00:13:24.720 and i can basically go ahead and call it\n",
        "00:13:28.240 right with uh now python3 model.5 and i\n",
        "00:13:33.040 have a like you know thing called minus\n",
        "00:13:34.880 minus developed and i can go ahead and\n",
        "00:13:37.040 run this make sure that all of this\n",
        "00:13:39.040 works so great\n",
        "00:13:41.199 so at this point i have my code\n",
        "00:13:44.000 but\n",
        "00:13:44.720 remember that in this notebook i was\n",
        "00:13:47.519 doing some weird stuff\n",
        "00:13:49.600 i was also calling gcloud commands to\n",
        "00:13:52.399 deploy the model\n",
        "00:13:53.839 this is not python anymore this is bash\n",
        "00:13:57.920 how do i automate these\n",
        "00:14:00.079 well you could do it in bash but that's\n",
        "00:14:02.560 not what i would recommend keep things\n",
        "00:14:04.639 in python keep this whole automation\n",
        "00:14:07.120 pipeline in python so that's basically\n",
        "00:14:09.680 what i'm going to be talking about here\n",
        "00:14:11.920 so\n",
        "00:14:13.120 remember the the simple thing here that\n",
        "00:14:15.360 we're talking about right we're\n",
        "00:14:17.279 basically saying we want to develop a\n",
        "00:14:18.959 model but having developed the model\n",
        "00:14:21.360 let's look at this this entire workflow\n",
        "00:14:23.839 creating the data set training a model\n",
        "00:14:26.160 deploying to an endpoint getting\n",
        "00:14:27.839 predictions\n",
        "00:14:28.959 how to do that in python so that you can\n",
        "00:14:31.440 automate it really well so this is\n",
        "00:14:33.519 basically the workflow so you've now\n",
        "00:14:36.560 taken the model you've saved it\n",
        "00:14:38.880 now you can basically upload the model\n",
        "00:14:41.920 as a vertex ai model\n",
        "00:14:44.399 and then deploy the model to an endpoint\n",
        "00:14:47.839 in such a way that when a client request\n",
        "00:14:50.639 comes in through json\n",
        "00:14:52.639 this endpoint basically invokes your\n",
        "00:14:55.040 model\n",
        "00:14:56.000 and sends back a json payload and the\n",
        "00:14:58.639 endpoint doesn't contain just one model\n",
        "00:15:01.839 the end point can contain multiple\n",
        "00:15:04.079 models and do a traffic split between\n",
        "00:15:06.720 them\n",
        "00:15:07.600 so that's the basic basic idea so the\n",
        "00:15:10.320 concepts here are you have your model\n",
        "00:15:12.880 just model files you upload them into a\n",
        "00:15:15.440 model object\n",
        "00:15:16.880 you put model objects into endpoints and\n",
        "00:15:20.000 you basically do traffic splits in the\n",
        "00:15:22.639 endpoint between different models so\n",
        "00:15:25.120 let's start with the first thing you\n",
        "00:15:27.199 want to train a model you want to have a\n",
        "00:15:28.959 data set\n",
        "00:15:30.560 so the reason that you want to have a\n",
        "00:15:32.320 data set that is in vertex ai\n",
        "00:15:35.040 we think of four different types of\n",
        "00:15:37.279 objects\n",
        "00:15:38.480 a data set which could be structured or\n",
        "00:15:40.959 unstructured it will have metadata it\n",
        "00:15:43.600 will have versions so that you know\n",
        "00:15:46.079 given a model what version of data was\n",
        "00:15:48.800 used to train that model that's part of\n",
        "00:15:50.560 the metadata of that model\n",
        "00:15:52.880 the next thing that you will often have\n",
        "00:15:54.639 is that you'll have the model itself you\n",
        "00:15:56.399 trained a model now you have model files\n",
        "00:15:58.880 your saved model in tensorflow right you\n",
        "00:16:01.360 have machine learning model artifact so\n",
        "00:16:04.079 those files that's your model object and\n",
        "00:16:07.440 the end point is the thing that gets\n",
        "00:16:09.440 called it contains multiple models and\n",
        "00:16:12.240 you orchestrate all of these things\n",
        "00:16:14.800 using a training job or a pipeline right\n",
        "00:16:17.759 and the pipeline basically is take a\n",
        "00:16:20.320 data set train a model on it deploy to\n",
        "00:16:22.560 the endpoint\n",
        "00:16:24.320 so essentially the way you start is that\n",
        "00:16:27.279 you basically have now in my case a\n",
        "00:16:29.600 train on vertex ai right and i basically\n",
        "00:16:32.720 initialize the ai platform\n",
        "00:16:35.199 and i say\n",
        "00:16:36.399 my data set\n",
        "00:16:38.000 is a tabular data set\n",
        "00:16:40.320 it consists of all of the files that\n",
        "00:16:43.600 meet this wild card thing so i'm\n",
        "00:16:46.000 basically using tensorflow right g\n",
        "00:16:48.839 file.glob find all of the all star.csv\n",
        "00:16:53.279 and using those files to create my\n",
        "00:16:56.560 tabular data set so now this data set\n",
        "00:16:59.680 contains a number of metadata\n",
        "00:17:02.079 information that is going to get passed\n",
        "00:17:04.319 along i don't you will not actually see\n",
        "00:17:06.559 any metadata in any of my code but\n",
        "00:17:08.959 remember that underneath\n",
        "00:17:10.799 vertex ai is taking care of all of that\n",
        "00:17:12.720 for us okay so we're basically getting a\n",
        "00:17:15.679 tabular data set\n",
        "00:17:17.599 and it consists of all of these files so\n",
        "00:17:20.319 that's pretty much it\n",
        "00:17:21.839 so then in model.pi\n",
        "00:17:25.919 earlier we kind of knew what we were\n",
        "00:17:28.319 reading but now\n",
        "00:17:30.320 vertex ai is going to basically set a\n",
        "00:17:32.320 few environment variables for us\n",
        "00:17:34.720 it's going to tell us what the training\n",
        "00:17:37.520 data is\n",
        "00:17:38.720 what the evaluation data is and what the\n",
        "00:17:40.880 test data is\n",
        "00:17:42.480 those are three of our true environment\n",
        "00:17:44.320 variables\n",
        "00:17:45.440 and it's going to tell us\n",
        "00:17:47.280 where to save the output\n",
        "00:17:49.760 okay so what i will do\n",
        "00:17:51.919 is that in my model.pi\n",
        "00:17:54.400 i will look for this environment data\n",
        "00:17:56.960 and if it's not present then i know that\n",
        "00:17:59.919 somebody's running it not in the context\n",
        "00:18:02.320 of a pipeline where these things are set\n",
        "00:18:05.440 but they're probably running it from a\n",
        "00:18:07.360 notebook they're probably running it\n",
        "00:18:09.679 locally right and so you can basically\n",
        "00:18:12.240 say if that's not set\n",
        "00:18:14.559 like\n",
        "00:18:15.360 hardcoded to be where we expect it to be\n",
        "00:18:18.160 okay so once you do that you now have\n",
        "00:18:21.120 your dataset created and now you're able\n",
        "00:18:24.000 to train your model so\n",
        "00:18:26.080 the training the model means\n",
        "00:18:28.559 that you have your model code that's\n",
        "00:18:31.120 your custom training job and you're\n",
        "00:18:33.440 saying go ahead and please run\n",
        "00:18:35.120 model.pipe\n",
        "00:18:37.600 that's your code\n",
        "00:18:39.520 but in order to run it we will need a\n",
        "00:18:42.799 container vertex ai works on containers\n",
        "00:18:45.200 everything is containerized\n",
        "00:18:47.120 fortunately\n",
        "00:18:48.559 there is a pre-built container\n",
        "00:18:51.200 for every version of tensorflow that you\n",
        "00:18:53.200 want to use\n",
        "00:18:54.320 so in this case i'm basically saying\n",
        "00:18:56.880 like i want to train on the gpu version\n",
        "00:18:59.360 of tensorflow and i want to deploy on\n",
        "00:19:01.840 the cpu version of tensorflow right so\n",
        "00:19:04.640 basically i'm getting two\n",
        "00:19:06.720 training image and the deployment image\n",
        "00:19:09.039 that's the training image this is the\n",
        "00:19:10.480 prediction image i have two two\n",
        "00:19:13.120 pre-built containers and so when i'm\n",
        "00:19:16.000 when i'm training it i'm saying use this\n",
        "00:19:18.559 as your training image and serve it on\n",
        "00:19:20.880 the deploy image so pretty much\n",
        "00:19:23.600 i'm can take my model.pi and use it to\n",
        "00:19:26.880 create a custom training job and once\n",
        "00:19:29.679 i've created a custom training job i run\n",
        "00:19:32.559 the job\n",
        "00:19:34.000 passing in the data set\n",
        "00:19:36.160 right and passing in the accelerator\n",
        "00:19:39.039 type the machine type etc that i want to\n",
        "00:19:41.919 do the training on\n",
        "00:19:43.760 so bottom line then you train your model\n",
        "00:19:46.480 you create all the training code and\n",
        "00:19:48.559 then when you're ready to train the\n",
        "00:19:49.919 model on some piece of hardware what you\n",
        "00:19:52.640 do is you create a custom training job\n",
        "00:19:55.039 providing the tensorflow containers this\n",
        "00:19:57.039 is where the prefab\n",
        "00:19:58.720 part of my title comes in it's all\n",
        "00:20:01.200 already there prefabricated so just go\n",
        "00:20:04.159 ahead and use the pre-built container\n",
        "00:20:07.200 pass in the hook for your custom code\n",
        "00:20:09.760 and remember that your custom code right\n",
        "00:20:12.480 will basically you this is the contract\n",
        "00:20:15.200 that your custom code has to honor it\n",
        "00:20:17.679 has to read the data from the path\n",
        "00:20:20.799 pointed by these three environment\n",
        "00:20:22.559 variables and write the data to the path\n",
        "00:20:25.520 pointed by this thing so if you look in\n",
        "00:20:28.000 my code in model.pi\n",
        "00:20:30.400 right so basically that's what i'm doing\n",
        "00:20:33.039 i'm basically setting the output there\n",
        "00:20:35.280 based on the output model there i'm\n",
        "00:20:37.760 setting the training data pattern based\n",
        "00:20:39.919 on the environment variable right and if\n",
        "00:20:42.240 the environment variable isn't set\n",
        "00:20:44.320 i'm basically dropping back to the\n",
        "00:20:47.360 original behavior i had in my notebook\n",
        "00:20:50.000 right my original notebook was basically\n",
        "00:20:52.880 reading train star so i just keep that\n",
        "00:20:55.840 if\n",
        "00:20:57.840 i'm not running in a pipeline if i'm\n",
        "00:20:59.440 running in a pipeline i'm getting the\n",
        "00:21:02.240 the orig the the place to read from\n",
        "00:21:05.440 from the environment variable\n",
        "00:21:07.360 the rest of my code remains exactly the\n",
        "00:21:10.400 same right because all of this is\n",
        "00:21:12.720 basically just processing the training\n",
        "00:21:14.880 data pattern evaluation data pattern and\n",
        "00:21:17.840 that comes either from the environment\n",
        "00:21:19.520 variable or from a hard-coded value\n",
        "00:21:21.919 because i'm calling it from a notebook\n",
        "00:21:23.919 right so given those two things\n",
        "00:21:27.200 at this point\n",
        "00:21:28.720 i now have my\n",
        "00:21:30.480 model training done\n",
        "00:21:32.240 okay\n",
        "00:21:33.039 how does this change if you're not doing\n",
        "00:21:35.200 a custom model remember that this is if\n",
        "00:21:37.120 you are writing tensorflow code myself\n",
        "00:21:40.159 and i'm saying i want to run it here\n",
        "00:21:42.000 well if you're doing auto ml we will\n",
        "00:21:44.559 look at it so we basically change this\n",
        "00:21:47.039 function train custom model to be train\n",
        "00:21:50.159 auto ml model this part will change but\n",
        "00:21:53.039 the rest of my pipeline is going to\n",
        "00:21:55.120 remain the same i'm going to continue to\n",
        "00:21:57.440 use the same data set\n",
        "00:22:00.080 but i will use that same data set\n",
        "00:22:02.960 for training a vertex ai model for\n",
        "00:22:05.440 training an automl model for doing hyper\n",
        "00:22:08.159 parameter tuning and then regardless of\n",
        "00:22:10.480 how i got my model i'll deploy it in\n",
        "00:22:13.360 exactly the same way so this is part of\n",
        "00:22:15.679 that unification that's super important\n",
        "00:22:18.080 it doesn't matter\n",
        "00:22:19.840 how you train your model this could be a\n",
        "00:22:21.679 pie torch model\n",
        "00:22:23.280 and you would still create the data set\n",
        "00:22:24.960 and you would still deploy it in exactly\n",
        "00:22:26.640 the same way that's part of the power of\n",
        "00:22:28.559 this you can basically switch any of\n",
        "00:22:31.200 these things in the middle and you have\n",
        "00:22:32.640 that unified workflow making life a lot\n",
        "00:22:35.200 easier\n",
        "00:22:36.320 so what if you're not using these\n",
        "00:22:37.919 pre-built containers that i did that i\n",
        "00:22:39.840 showed you with uh with tensorflow well\n",
        "00:22:42.720 there's two other two other ways that\n",
        "00:22:44.400 you could do it you could use tube flow\n",
        "00:22:47.200 pipelines and you could basically write\n",
        "00:22:50.000 whatever code you want\n",
        "00:22:52.000 right and basically have a base image\n",
        "00:22:55.039 and\n",
        "00:22:55.840 now create your own docker image write\n",
        "00:22:58.240 your own code and use that to basically\n",
        "00:23:01.760 create a custom training job\n",
        "00:23:04.080 and that custom training job is\n",
        "00:23:05.840 basically the stuff in the middle here\n",
        "00:23:07.679 right you need this custom training job\n",
        "00:23:10.640 and you just do a job at run\n",
        "00:23:12.559 everything remains the same you've just\n",
        "00:23:14.320 gotten a new custom training job out of\n",
        "00:23:17.039 your own container out of your own code\n",
        "00:23:19.760 so that's one way to do it\n",
        "00:23:21.520 the other way to do it is to say forget\n",
        "00:23:23.360 about all this prefab\n",
        "00:23:25.039 pipelines business i will go use tfx and\n",
        "00:23:28.559 tfx is up is a very prescriptive way of\n",
        "00:23:31.440 running things where basically provides\n",
        "00:23:34.000 you\n",
        "00:23:35.039 python libraries for data validation\n",
        "00:23:37.520 python libraries for a tensorflow\n",
        "00:23:40.240 transformation etc you use all of those\n",
        "00:23:43.520 tfx components\n",
        "00:23:45.520 and you use it to create\n",
        "00:23:47.600 a pipeline object and you run that\n",
        "00:23:49.919 pipeline object okay so you have\n",
        "00:23:52.080 multiple ways of creating pipelines i'm\n",
        "00:23:54.640 not going to talk about these two two\n",
        "00:23:56.240 other approaches because today i'm\n",
        "00:23:58.480 talking about the very like prefab\n",
        "00:24:01.520 simple things should be simple approach\n",
        "00:24:03.840 this is the more\n",
        "00:24:05.200 you can do hard things and hard things\n",
        "00:24:06.799 are possible approach\n",
        "00:24:09.120 so once you've gotten your model\n",
        "00:24:12.080 what you do to your model you deploy to\n",
        "00:24:13.840 an endpoint\n",
        "00:24:15.039 so you want to create an endpoint\n",
        "00:24:17.679 now so here's the thing though\n",
        "00:24:19.520 you don't want to create an end point if\n",
        "00:24:21.520 it already exists remember the idea\n",
        "00:24:23.440 behind an end point you want to do\n",
        "00:24:24.880 traffic splits\n",
        "00:24:26.320 if every model gets deployed to a new\n",
        "00:24:28.640 endpoint\n",
        "00:24:30.320 it doesn't make sense so you want to\n",
        "00:24:32.240 keep the end point consistent and you\n",
        "00:24:34.720 want to keep deploying more and more\n",
        "00:24:36.320 models into the same endpoint and\n",
        "00:24:38.799 splitting traffic between\n",
        "00:24:41.520 different trained models\n",
        "00:24:44.159 as you go along so that's what i'm doing\n",
        "00:24:46.240 here i'm saying go ahead and get me all\n",
        "00:24:48.960 the end points with a specific display\n",
        "00:24:51.279 name\n",
        "00:24:52.159 and order it by the descending order of\n",
        "00:24:54.960 create time and take the most recently\n",
        "00:24:57.919 created endpoint\n",
        "00:24:59.760 and that's the end point that i'm going\n",
        "00:25:01.520 to deploy ideally you've only created an\n",
        "00:25:04.080 endpoint once\n",
        "00:25:05.840 and then from then on you're just\n",
        "00:25:07.679 reusing it over and over and over again\n",
        "00:25:09.760 right so create the endpoint with a\n",
        "00:25:11.360 specific name this endpoint name\n",
        "00:25:14.400 and reuse it over again\n",
        "00:25:16.480 so once you do that\n",
        "00:25:18.159 you can basically say model.deploy\n",
        "00:25:20.400 passing in the endpoint passing in the\n",
        "00:25:22.960 traffic split so here i'm saying this\n",
        "00:25:25.440 model that i'm deploying\n",
        "00:25:27.600 given 100 of the traffic\n",
        "00:25:29.919 if you want you can basically pass in\n",
        "00:25:32.320 previously deployed model ids\n",
        "00:25:34.960 and say that model has 30 and i get 70\n",
        "00:25:39.440 or vice versa specify the machine type\n",
        "00:25:42.320 that you want to deploy on\n",
        "00:25:44.159 and that's pretty much it right\n",
        "00:25:47.679 so once you have deployed the model to\n",
        "00:25:50.159 an endpoint you want to use it\n",
        "00:25:53.120 so the way that you can use it is you\n",
        "00:25:55.360 have to basically create a json request\n",
        "00:25:58.240 so this is what the request looks like\n",
        "00:26:00.000 these are all of the input features to\n",
        "00:26:02.000 the model so you basically go through\n",
        "00:26:04.080 and you pass it you create a json\n",
        "00:26:05.840 something and you pass that in\n",
        "00:26:07.840 and\n",
        "00:26:08.720 you basically say endpoints predict and\n",
        "00:26:11.760 pass in your json request so this is how\n",
        "00:26:14.240 you can do it\n",
        "00:26:15.520 with bash just to try things out during\n",
        "00:26:17.919 development\n",
        "00:26:19.120 but obviously your client code is not\n",
        "00:26:21.440 going to use gcloud\n",
        "00:26:23.120 your client code wants to use python\n",
        "00:26:25.600 wants to use rest wants to use java\n",
        "00:26:28.640 wants to use something so if you want to\n",
        "00:26:30.720 use python\n",
        "00:26:32.240 you basically get the end point as\n",
        "00:26:34.240 before\n",
        "00:26:35.360 and then you basically create a no\n",
        "00:26:38.559 python dictionary with the exact same\n",
        "00:26:41.279 json structure\n",
        "00:26:43.039 and you pass in\n",
        "00:26:45.039 endpoint.predict\n",
        "00:26:46.720 you pass in the data and you get the set\n",
        "00:26:49.840 of predictions\n",
        "00:26:51.120 okay so that's pretty much uh how you do\n",
        "00:26:53.679 it and you can also do it using rest you\n",
        "00:26:56.400 can basically use curl and you can pass\n",
        "00:26:58.640 in a specific thing so there's\n",
        "00:27:01.279 it's just a rest api so you can invoke\n",
        "00:27:03.520 it as long as you basically put your\n",
        "00:27:05.520 data in this form and you post it you're\n",
        "00:27:07.919 fine\n",
        "00:27:09.520 so now let's look at a couple of the\n",
        "00:27:11.600 other like\n",
        "00:27:12.960 how does it change if i wanted to tune\n",
        "00:27:14.880 hyper parameters\n",
        "00:27:16.480 well if you want to tune hyper\n",
        "00:27:17.760 parameters\n",
        "00:27:19.120 inside your model.pi you've got to do\n",
        "00:27:21.279 two things\n",
        "00:27:22.640 first thing\n",
        "00:27:24.000 is anything that you want to tune has to\n",
        "00:27:27.360 be a command line parameter to your\n",
        "00:27:29.360 model so if you want to tune the number\n",
        "00:27:31.440 of buckets that you're going to\n",
        "00:27:33.120 discretize your variable into you make\n",
        "00:27:36.000 that an input parameter into your into\n",
        "00:27:38.720 your\n",
        "00:27:39.520 code\n",
        "00:27:40.559 and then you basically\n",
        "00:27:42.559 write\n",
        "00:27:44.320 a keras callback so this is a this is my\n",
        "00:27:47.200 hyper parameter tuning callback in keras\n",
        "00:27:50.240 i say at the end of every epoch\n",
        "00:27:53.360 please report\n",
        "00:27:55.440 the metric that i care about which is\n",
        "00:27:57.600 the\n",
        "00:27:58.320 validation root mean squared error so\n",
        "00:28:01.279 that is my hyperparameter metric\n",
        "00:28:03.679 and say report that\n",
        "00:28:05.760 and then create the model train it then\n",
        "00:28:08.240 as you normally do but when you call the\n",
        "00:28:10.799 model that fit remember to also call it\n",
        "00:28:14.080 pass in the hyper parameter tuning\n",
        "00:28:16.320 callback\n",
        "00:28:17.600 that's all you need to do\n",
        "00:28:19.200 now your keras model is going to be\n",
        "00:28:21.200 reporting metrics as it goes along\n",
        "00:28:24.880 and then in\n",
        "00:28:26.080 in their training pipeline code\n",
        "00:28:29.279 you built the original job that you had\n",
        "00:28:31.760 the custom training job\n",
        "00:28:34.000 it is now just a single trial so you say\n",
        "00:28:37.120 give me a trial job\n",
        "00:28:39.360 from my model.script\n",
        "00:28:41.600 and you use that to create a hyper\n",
        "00:28:43.600 parameter tuning job\n",
        "00:28:45.840 and you train that job\n",
        "00:28:48.240 and the result of it is going to be a\n",
        "00:28:50.640 number of trials\n",
        "00:28:52.399 you sort it find the best trial and\n",
        "00:28:55.760 using the best set of parameters you can\n",
        "00:28:58.399 basically go train it again right to\n",
        "00:29:00.960 train it all the way through typically\n",
        "00:29:03.039 what you do when you do hyper private\n",
        "00:29:04.960 tuning is that you train on a smaller\n",
        "00:29:07.520 set of data faster\n",
        "00:29:09.919 and then like once you've found the best\n",
        "00:29:12.159 set of parameters you tune that\n",
        "00:29:15.200 you take those parameters and you train\n",
        "00:29:17.840 the mod the model on the full data set\n",
        "00:29:20.480 for much longer so that's basically what\n",
        "00:29:23.279 this is showing here so if you go into\n",
        "00:29:27.919 train on vertex ai dot pi\n",
        "00:29:30.320 the hyperbrain retuning job is to\n",
        "00:29:32.799 basically create my custom job for my\n",
        "00:29:35.919 local script passing in the model.pi\n",
        "00:29:39.520 saying please don't evaluate on the full\n",
        "00:29:41.919 data set just train on a smaller number\n",
        "00:29:44.640 of examples\n",
        "00:29:46.159 and go run this code for me that's my\n",
        "00:29:48.720 trial job and then my hyper parameter\n",
        "00:29:51.360 tuning job\n",
        "00:29:52.720 basically says i want to minimize my\n",
        "00:29:55.039 validation rmse and these are the three\n",
        "00:29:58.159 parameters i want you to tune\n",
        "00:30:00.640 the training back size the number of\n",
        "00:30:02.720 buckets and the number of layers and\n",
        "00:30:04.559 nodes in my\n",
        "00:30:05.919 in my dnn model and run it for these\n",
        "00:30:09.520 many number of trials\n",
        "00:30:11.520 and then once you run it you get all of\n",
        "00:30:14.159 your trials you find the best one\n",
        "00:30:16.960 and you go through all of those find all\n",
        "00:30:19.360 of the best parameters and you basically\n",
        "00:30:22.159 go ahead and call the original\n",
        "00:30:24.880 train custom model the one that we first\n",
        "00:30:27.120 wrote with the full set so that's pretty\n",
        "00:30:30.080 much\n",
        "00:30:30.799 hyper grammar tuning\n",
        "00:30:32.960 how do you do auto ml well automl means\n",
        "00:30:36.320 instead of training a custom job you're\n",
        "00:30:39.200 going to use an automl tabular training\n",
        "00:30:41.440 job and run it so here\n",
        "00:30:44.240 right uh the\n",
        "00:30:46.240 train that's a custom model here's my\n",
        "00:30:49.039 trained automl model i'm creating a job\n",
        "00:30:52.640 i'm doing job.run i'm returning the\n",
        "00:30:54.960 model\n",
        "00:30:56.000 so regardless of how i did it i trained\n",
        "00:30:59.279 my data set\n",
        "00:31:00.799 i did one of the three training jobs\n",
        "00:31:03.519 automl or hyperparameter tuning are\n",
        "00:31:06.240 custom modeling\n",
        "00:31:07.760 and then i deploy to the endpoint\n",
        "00:31:10.559 and i'm done\n",
        "00:31:12.080 right i'm done at this point and i can\n",
        "00:31:14.720 take this entire code my main\n",
        "00:31:17.679 and i can basically run that entire\n",
        "00:31:19.760 thing as a pipeline to basically get all\n",
        "00:31:22.559 my metadata tracking and everything\n",
        "00:31:24.399 going\n",
        "00:31:25.200 okay so\n",
        "00:31:27.440 bottom line at this point what you've\n",
        "00:31:28.799 done is you've created this workflow you\n",
        "00:31:31.120 basically take your model in your keras\n",
        "00:31:32.799 code you've done a model that's safe you\n",
        "00:31:34.559 upload the model right you deploy the\n",
        "00:31:37.120 model and you make it available for\n",
        "00:31:38.880 predictions\n",
        "00:31:40.480 so\n",
        "00:31:41.840 to summarize\n",
        "00:31:43.279 prefab training pipelines they make\n",
        "00:31:45.440 simple workflows really simple\n",
        "00:31:47.120 developing a model creating a data set\n",
        "00:31:50.159 training it tuning it deploying the end\n",
        "00:31:53.279 point and getting predictions\n",
        "00:31:56.240 for if you want to do custom or\n",
        "00:31:57.760 continuous evaluation for example all\n",
        "00:32:00.399 that you need to do is when you do the\n",
        "00:32:02.559 job.run\n",
        "00:32:04.000 also export the evaluations into a\n",
        "00:32:06.640 bigquery table and\n",
        "00:32:08.880 provide that bigquery table to vertex ai\n",
        "00:32:11.519 and it will automatically start\n",
        "00:32:13.120 monitoring your metric and show you\n",
        "00:32:16.080 graphs of it and then you can set up\n",
        "00:32:18.320 when this\n",
        "00:32:19.360 metric drops below a certain point\n",
        "00:32:22.320 retrain right that's pretty much it so\n",
        "00:32:24.960 it's it's very very very straightforward\n",
        "00:32:27.760 to start to add many of these amazing\n",
        "00:32:30.240 capabilities to this very simple\n",
        "00:32:33.519 starter point\n",
        "00:32:35.200 so in summary right we talked about uh\n",
        "00:32:38.480 this simple workflow like follow me on\n",
        "00:32:40.880 twitter uh lac underscore gcp\n",
        "00:32:44.000 uh this\n",
        "00:32:45.279 uh\n",
        "00:32:46.159 i i will leave here um\n",
        "00:32:49.519 oh\n",
        "00:32:50.640 okay the uh the github repository which\n",
        "00:32:53.760 is probably the most important thing\n",
        "00:32:55.120 here uh is this guy\n",
        "00:32:57.760 okay\n",
        "00:32:59.279 so go there\n",
        "00:33:01.360 uh just one\n",
        "00:33:03.200 warning\n",
        "00:33:04.799 make sure to switch to edition two right\n",
        "00:33:07.679 because uh you know\n",
        "00:33:09.519 so right there\n",
        "00:33:10.880 there is o9 vertex ai so just remember\n",
        "00:33:14.159 this url i'll also post this in the chat\n",
        "00:33:22.720 all right and with that i'll take\n",
        "00:33:23.919 questions\n",
        "00:33:25.760 awesome great uh wow the vertex ai\n",
        "00:33:28.880 really makes the machine learning tasks\n",
        "00:33:31.360 easy and\n",
        "00:33:32.180 [Music]\n",
        "00:33:33.440 the full stack\n",
        "00:33:35.279 um yeah we have\n",
        "00:33:37.120 quite a lot of questions uh let me bring\n",
        "00:33:40.640 up uh the sli uh sign up for that\n",
        "00:33:44.240 um just a quick reminder\n",
        "00:33:47.679 right off of this right and uh yeah so i\n",
        "00:33:51.679 just posted that and uh if you go to\n",
        "00:33:54.399 github there is the ipython notebook\n",
        "00:33:56.960 okay right right right great yeah\n",
        "00:33:59.279 yeah i just wanted to remind that you\n",
        "00:34:00.960 know for anybody for uh questions\n",
        "00:34:03.760 uh sorry just a quick reminder you know\n",
        "00:34:05.760 if you have questions uh you can post in\n",
        "00:34:07.679 the chat or raise hand we we can uh um\n",
        "00:34:11.280 build you to speak\n",
        "00:34:12.879 uh let's go with\n",
        "00:34:14.399 the i have a few first questions\n",
        "00:34:17.918 um one question is is there any further\n",
        "00:34:20.719 granule level control on where my\n",
        "00:34:23.679 endpoint will be deployed within a\n",
        "00:34:26.000 region\n",
        "00:34:27.119 uh\n",
        "00:34:27.918 you can do it within a zone i would not\n",
        "00:34:30.320 i would not uh go there i mean again a\n",
        "00:34:33.280 region in google cloud is probably the\n",
        "00:34:35.839 most granular that you want to go right\n",
        "00:34:38.000 so so specify a specific region i mean\n",
        "00:34:40.879 why would you ever want to basically\n",
        "00:34:42.399 specify a single zone that doesn't make\n",
        "00:34:44.879 too much of sense right so so stick to a\n",
        "00:34:46.879 region a region is probably the most\n",
        "00:34:48.800 granular that you want that you want to\n",
        "00:34:50.560 be you might want to do a multi-region\n",
        "00:34:52.879 you might want to do for like you know\n",
        "00:34:55.760 resilience purposes but a region is the\n",
        "00:34:58.800 is the lowest ground\n",
        "00:35:02.720 okay great another question from davis\n",
        "00:35:05.680 why are you using jupiter lab so the\n",
        "00:35:08.400 collab is google's version of adobe to\n",
        "00:35:10.880 lab\n",
        "00:35:12.480 that's the absolute colab is our free\n",
        "00:35:15.760 version we have now for some people who\n",
        "00:35:18.480 wanted to use it in enterprise we now\n",
        "00:35:20.320 have a paid version of colab but uh in\n",
        "00:35:24.240 vertex ai what we call vertex ai\n",
        "00:35:26.320 workbench uh is basically our\n",
        "00:35:29.040 development environment that provides\n",
        "00:35:31.680 enterprise capabilities like vpc service\n",
        "00:35:34.400 controls encryption uh like you know\n",
        "00:35:37.880 multi-tenancy a whole bunch of things\n",
        "00:35:40.000 that if you're running it in an\n",
        "00:35:41.680 enterprise if you have confidential data\n",
        "00:35:44.240 i strongly suggest using the vertex ai\n",
        "00:35:46.960 workbench\n",
        "00:35:50.800 another question is asking is the\n",
        "00:35:52.720 voltage ai free like a collab or\n",
        "00:35:56.240 different vertex is part of google cloud\n",
        "00:35:59.040 and you will be a google you know it is\n",
        "00:36:01.200 definitely definitely a a paid service\n",
        "00:36:04.160 it's not free okay uh there is a free\n",
        "00:36:06.880 trial version of google cloud where you\n",
        "00:36:08.640 get 300 free so definitely use it to\n",
        "00:36:11.920 learn but now when you're using in\n",
        "00:36:14.000 production you you know it's not free\n",
        "00:36:19.440 all right another question\n",
        "00:36:21.760 uh\n",
        "00:36:23.839 to clarify the question from chester to\n",
        "00:36:26.240 clarify are you claiming that vertex ai\n",
        "00:36:28.640 can accomplish what a spark plus h2o\n",
        "00:36:32.640 plus ml flow plus ml1 frameworks will do\n",
        "00:36:38.800 spark is larger than vertex ai right\n",
        "00:36:42.400 spark is a combination of data\n",
        "00:36:44.640 processing\n",
        "00:36:46.000 and machine learning so if you're\n",
        "00:36:47.839 talking about spark m light then yes\n",
        "00:36:50.079 right but remember that spark also helps\n",
        "00:36:52.720 you do data preparation so on google\n",
        "00:36:56.400 cloud you would run spark within data\n",
        "00:36:59.119 proc right so you could use data proc to\n",
        "00:37:01.680 prepare your data the way you do like\n",
        "00:37:03.920 large data sets with spark more commonly\n",
        "00:37:06.640 on google cloud people use bigquery to\n",
        "00:37:08.800 prepare data but you can do spark or you\n",
        "00:37:11.119 can do bigquery to prepare the data but\n",
        "00:37:13.440 once that data has been prepared as far\n",
        "00:37:16.960 as feature engineering and training and\n",
        "00:37:20.640 you know no code no code all of these\n",
        "00:37:23.200 different frameworks yeah that unified\n",
        "00:37:25.200 framework unified orchestration you're\n",
        "00:37:28.560 scheduling all of these things\n",
        "00:37:30.240 absolutely don't\n",
        "00:37:32.079 don't buy individual things and glue\n",
        "00:37:34.240 them together buy something that is\n",
        "00:37:36.160 already well integrated would be my\n",
        "00:37:37.760 thing yes i am i am making the claim\n",
        "00:37:40.960 that other than data pre-processing\n",
        "00:37:42.880 which is not which is not\n",
        "00:37:45.200 but everything machine learning related\n",
        "00:37:46.720 you can do in wordpress app bigquery\n",
        "00:37:48.960 plus vertex ai is an extremely powerful\n",
        "00:37:51.119 solution as is data proc plus vertex\n",
        "00:37:57.280 great\n",
        "00:37:58.160 use it for free while learning it yes uh\n",
        "00:38:01.200 again a google cloud offers a lot of\n",
        "00:38:03.520 things free for students so go to\n",
        "00:38:06.800 cloud.google.com\n",
        "00:38:08.720 edu for example right and you will get a\n",
        "00:38:12.480 lot of higher education programs have\n",
        "00:38:14.880 your faculty members if you're a student\n",
        "00:38:17.520 register for it so that all of your\n",
        "00:38:19.359 class can get access to it strongly\n",
        "00:38:21.760 encourage you to do that if you also go\n",
        "00:38:23.920 to cloud.google.com there is a 300 free\n",
        "00:38:26.560 trial that you can use to learn as well\n",
        "00:38:30.240 the third way to learn relatively\n",
        "00:38:33.040 quickly is quicklabs.com\n",
        "00:38:36.079 quick labs has a number of uh all these\n",
        "00:38:39.119 labs so if you go to quick cloud you go\n",
        "00:38:41.440 to the catalog\n",
        "00:38:43.200 and you search for vertex ai you should\n",
        "00:38:46.640 see\n",
        "00:38:47.599 a number of uh\n",
        "00:38:50.320 labs that you can basically build and\n",
        "00:38:52.400 deploy\n",
        "00:38:53.440 uh machine learning solutions and vertex\n",
        "00:38:55.359 ai so that is\n",
        "00:38:56.800 this quest for example is an easy way to\n",
        "00:38:59.440 basically go through and learn\n",
        "00:39:01.920 all of the steps and i know it says 35\n",
        "00:39:04.880 credits but\n",
        "00:39:06.320 quick labs usually runs all of these\n",
        "00:39:08.079 promotions where they basically give you\n",
        "00:39:10.320 like a month free just watch for those\n",
        "00:39:12.960 and that that that's that's another way\n",
        "00:39:15.599 to basically learn this thing for free\n",
        "00:39:21.680 awesome great\n",
        "00:39:23.119 uh\n",
        "00:39:25.119 again uh just remind uh you for\n",
        "00:39:27.280 questions feel free to post in the chat\n",
        "00:39:30.960 i see a lot of messages in the chat so\n",
        "00:39:32.880 if your questions got\n",
        "00:39:34.800 screwed up\n",
        "00:39:36.320 [Music]\n",
        "00:39:37.440 lauren has an interesting question you\n",
        "00:39:38.880 can\n",
        "00:39:43.920 ai are containers instances under the\n",
        "00:39:46.000 hood of the endpoint uniform uh so you\n",
        "00:39:49.680 get to choose so whenever you basically\n",
        "00:39:51.920 deploy to an endpoint you basically get\n",
        "00:39:55.040 to choose how you're basically deploying\n",
        "00:39:57.760 so when you notice uh in my trained at\n",
        "00:40:00.960 vertex right uh i basically specify the\n",
        "00:40:04.720 machine type and i specify the scaling\n",
        "00:40:07.920 i'm in this case i'm saying don't scale\n",
        "00:40:10.079 because i'm not start minimum of one max\n",
        "00:40:12.800 of one but you can obviously specify\n",
        "00:40:14.800 auto scaling here your machine type can\n",
        "00:40:17.280 be any machine temp that's supported on\n",
        "00:40:19.119 google cloud plus any accelerator type\n",
        "00:40:22.160 which you saw when i did the training i\n",
        "00:40:23.839 said train on nvidia t4 so\n",
        "00:40:27.200 yes\n",
        "00:40:28.000 regardless of what the model is you can\n",
        "00:40:30.160 basically do it on all of these\n",
        "00:40:45.599 awesome\n",
        "00:40:46.640 all right\n",
        "00:40:49.680 for inferencing what feature store do\n",
        "00:40:52.800 you use it has to scale\n",
        "00:40:55.040 well we do have this thing called a\n",
        "00:40:56.960 vertex\n",
        "00:40:58.319 ai feature store\n",
        "00:41:05.280 so take a look\n",
        "00:41:07.520 okay\n",
        "00:41:13.839 okay\n",
        "00:41:16.000 uh with automl how do you see this\n",
        "00:41:18.480 disrupting jobs in the data science\n",
        "00:41:20.800 machine learning industry look here's\n",
        "00:41:22.720 the deal though as a data scientist what\n",
        "00:41:25.200 are you spending most of your time on\n",
        "00:41:27.359 it's in understanding the problem it's\n",
        "00:41:30.079 in evaluating models it's in preparing\n",
        "00:41:34.000 data\n",
        "00:41:35.359 i mean\n",
        "00:41:36.319 it's not in creating the model right\n",
        "00:41:38.240 that writing that model could face that\n",
        "00:41:40.240 what everybody does if you're doing\n",
        "00:41:41.440 image classification you're using\n",
        "00:41:43.280 resonant right\n",
        "00:41:44.800 if if that gets replaced by r2ml that's\n",
        "00:41:47.440 replacing a really small part of the\n",
        "00:41:50.160 overall workflow and now automl doesn't\n",
        "00:41:54.160 replace data scientists it basically\n",
        "00:41:56.960 gives you a faster way to go to market\n",
        "00:42:00.560 to benchmark your models etc\n",
        "00:42:13.200 for iot it has been millions per second\n",
        "00:42:15.200 yes uh absolutely so\n",
        "00:42:17.200 now for iot on google cloud folks use\n",
        "00:42:20.079 something called data flow uh and you\n",
        "00:42:22.560 basically do those in in a in a\n",
        "00:42:24.960 streaming pipeline and data flow so look\n",
        "00:42:27.599 up data flow streaming pipelines\n",
        "00:42:31.760 machine learning uh we've had uh for\n",
        "00:42:34.960 example like we've had uh you know\n",
        "00:42:37.920 companies like for example telus does\n",
        "00:42:40.240 the scale that you're talking about\n",
        "00:42:42.240 where they're looking at millions of\n",
        "00:42:44.160 events per second they basically do it\n",
        "00:42:46.640 with like apache beam and tensorflow so\n",
        "00:42:49.599 so take a look\n",
        "00:43:00.319 all right i\n",
        "00:43:01.520 thank you very much uh i think we are uh\n",
        "00:43:05.040 out on times\n",
        "00:43:06.400 uh\n",
        "00:43:07.440 great thanks like for the uh\n",
        "00:43:09.760 presentations and the\n",
        "00:43:12.079 the discussions\n",
        "00:43:13.520 with attendees thanks everyone for\n",
        "00:43:16.400 asking the questions too\n",
        "00:43:18.410 [Music]\n",
        "00:43:19.760 with\n",
        "00:43:20.560 with that we\n",
        "00:43:22.560 conclude these sessions\n",
        "00:43:25.200 i like to have anything to add on before\n",
        "00:43:27.520 we\n",
        "00:43:28.240 nothing thank you very much bill for the\n",
        "00:43:30.079 opportunity thanks\n",
        "00:43:31.599 everyone for the great questions and for\n",
        "00:43:33.760 i know online is really really hard so\n",
        "00:43:36.400 thanks for sticking through it and uh\n",
        "00:43:38.720 and\n",
        "00:43:39.520 you know to\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G_eFkBbM-yW"
      },
      "source": [
        "# Prompt Enginieering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "nUrv0LmbNKhO"
      },
      "outputs": [],
      "source": [
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=M_PbbMVg4ME&t=1s\",\n",
        "    chunk_size_seconds=45,\n",
        "    transcript_format=TranscriptFormat.CHUNKS,\n",
        "    add_video_info=True,\n",
        "\n",
        ")\n",
        "\n",
        "transcript_documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=4000,\n",
        "    chunk_overlap=1000,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_title = transcript_documents[0].metadata['title']\n",
        "video_url = transcript_documents[0].metadata['source']\n",
        "video_author = transcript_documents[0].metadata['author']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6i5wj_VPuUH",
        "outputId": "8feeddf5-55be-494f-9b6c-44a5365e9fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Script written to youtube_script.txt\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:00:00---->00:00:45]:\n",
            "(upbeat music) - Thank you so much Alex for\n",
            "this nice introduction. So, You stole the content\n",
            "of my first few slides, but so hello, Nodes 22. I'm so happy to be here today. Great to see all those\n",
            "people interested in graphs as always. And in my session as Alex said, I would like to show you some\n",
            "cool graph visualization tools for many of the great use\n",
            "cases for a graph database visualization is super valuable tool that you should be aware of and it doesn't have to be complicated as you will see. Specifically I'll talk about\n",
            "visually exploring your name, Neo4j Graphs with Jupyter Notebooks\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:00:45---->00:01:30]:\n",
            "and simple literate programming. Whether you are a software\n",
            "developer or a data scientist or a database administrator\n",
            "or a project manager, my talk should give you some ideas on when you would want to\n",
            "be using graph visualization to better understand the\n",
            "data in your Neo4j databases. After my talk, I bet some of you go and\n",
            "try this on their own data. It's safe and it's easy and it's free and it's always exciting\n",
            "to get to actually see your graph data in a meaningful\n",
            "way for the first time. So some more details about myself. So, together with my team at yWorks, I've been working in the field of graph and diagram visualization for more than what's soon\n",
            "going to be 25 years.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:01:30---->00:02:15]:\n",
            "And our tagline is, \"The\n",
            "diagramming experts,\" and we help teams and\n",
            "enterprises worldwide with their graph visualization tasks. As graph drawing specialists, we have seen a huge number of use cases and data sets over the years. We've been working on and\n",
            "providing high quality tools and support to our customers and the community around that topic. And the tool we'll be looking at today is just one of these tools. So here's the agenda,\n",
            "what will you learn today and what are we going\n",
            "to see in the session? We'll be looking at easy ways to work with Neo4j databases plus the Neo4j\n",
            "graph data science library in the context of Jupyter Notebooks. But don't worry, although\n",
            "this will also mean we'll be looking at some lines of code, this is not going to be really difficult.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:02:15---->00:03:00]:\n",
            "We look at automating visualization tasks with the help of literate\n",
            "programming in notebooks. I'm going to show you how to\n",
            "create custom visualizations that will help us and our users\n",
            "better understand the data hidden in our graph databases. So for this I'll show you\n",
            "how to set up Jupyter, Jupyter Notebooks to connect to Neo4j. Use Cypher Query the database contents and the graph data science library to optionally perform some analysis tasks. We'll see how to create\n",
            "graph visualizations in Jupyter Notebooks and labs. They won't just look beautiful, but we will also make sure\n",
            "to include the information in the visualization that we\n",
            "want to provide our users with. So here's a quick overview of how all the things are connected that I'll be using today.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:03:00---->00:03:45]:\n",
            "Pun intended, the focus is\n",
            "on yFiles graphs for Jupyter, which is at the top and the\n",
            "left half basically explains how all this works and adds some context. So yWorks, the company I work at, creates a software development\n",
            "kit called \"yFiles.\" It allows software developers to build sophisticated graph\n",
            "visualization applications with great automatic layouts\n",
            "and rich custom interactions. And the Jupyter widget\n",
            "we'll be looking at today is also built on top of that library. And by the way, those tools at the center, they are all free to use so\n",
            "they don't require registration and the lower two don't\n",
            "even require installation nor download. You can just use\n",
            "them for free on our website. I will only briefly speak about the data explorer apps in this session.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:03:45---->00:04:30]:\n",
            "These belong to another set of tools to interactively explore connected data and one of them can be\n",
            "launched directly from within the Jupyter widget as we'll see, but they can also be used standalone without the Jupyter context. Actually at last year's Nodes conference, I dedicated a whole session\n",
            "to the data explorer for Neo4j and even more\n",
            "so the app generator. So I won't be covering it this time so, but feel free to browse the archives and watch the recording\n",
            "after the conference. All of these tools have built in support for Neo4j databases. So all you need to get started is a set of credentials\n",
            "to access your database. Read only access is\n",
            "enough for many use cases and the data never leaves your systems or touches our service. So there's a direct connection\n",
            "between your machine\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:04:30---->00:05:15]:\n",
            "and your database and you\n",
            "can run the tools offline behind the firewall if you wish to. So don't be afraid and just try it. Okay, so let's take a look at\n",
            "the Jupyter Notebook plugin. And by the way, the plugin's\n",
            "issue tracker is on GitHub. So when you use it and feel\n",
            "like it's missing a feature, please do file an issue on GitHub and the plug-in is\n",
            "under active development and the more people find it useful, the more time we can actually\n",
            "invest into the tool. So this is what the plug-in does. It provides interactive\n",
            "graph visualization and exploration capabilities\n",
            "in the form of a widget that can be embedded in Jupyter Notebook and Jupyter lab notebook cells. So you can use it to\n",
            "visualize connected graph data while from many different\n",
            "kinds of different data sources\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:05:15---->00:06:00]:\n",
            "that you have access\n",
            "to from your notebooks. And of course this in\n",
            "this includes Neo4j graphs which are supported right out the the box. This enables you to\n",
            "visualize the graph results of cypher queries. So the main difference\n",
            "to the Neo4j browser where you can do similar\n",
            "things is the target audience and thus also the support for\n",
            "more sophisticated use cases. While the Neo4j browser is essential for database\n",
            "administrator tasks, it's not so much a great\n",
            "scripting environment for one, it's basically limited to\n",
            "running cypher queries, but in a Jupyter notebook you can do that but you can do so much more. As a data scientist, you can write complex\n",
            "interactive notebooks that dynamically query\n",
            "data from various sources,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:06:00---->00:06:45]:\n",
            "do sophisticated pre-processing and then combine this with\n",
            "any number of cypher queries to create a far more complex and hopefully helpful visual realization. Also with widget you can apply various automatic\n",
            "graph layouts to the graphs in order to highlight the\n",
            "different aspects of the structure and the connectivity in your data. And with advanced custom\n",
            "rules for the visualization, you can add style to your nodes and relationships in the diagram and make it easier for the user to quickly see the important data. So we'll get to this in a minute. Installing the widget, it is easy. For example, you can\n",
            "just use \"pip install.\" So if you have used\n",
            "Jupyter notebooks before, that should be all familiar to you.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:06:45---->00:07:30]:\n",
            "Here in this example I'm\n",
            "also installing the Neo4j and graph data science\n",
            "packages for later use too. Note that you might need to restart the Jupyter server after this and please be aware that the\n",
            "functionality at this time, unfortunately for compatibility reasons, you should please open them\n",
            "using a regular Jupyter Notebook or Jupyter lab server. It doesn't work in all third party viewers like Visual Studio Code yet. So now let's take a look\n",
            "at the working widget. This over to the live demo. So I have a plain\n",
            "Jupyter Labs installation plus only the packages\n",
            "from that last slide. So the graph data science one isn't actually required\n",
            "to connect to Neo4j,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:07:30---->00:08:15]:\n",
            "but of course having the\n",
            "graph data science library available in a Jupyter notebook really allows for some\n",
            "very cool use cases. For the upcoming examples, I'm using the airports example database that comes with one of the\n",
            "graph data science tutorials. So I have set up a Neo4j (indistinct) with graph data science library enabled and I uploaded the sample\n",
            "database data before. So this means I can now run cypher queries right in my Jupyter Notebook\n",
            "using the Python API. So here I am importing\n",
            "the graph database object from the Neo4j package in\n",
            "my Jupyter Notebook cell. Instantiating a driver and the session with a certain database. Then I run the cypher query over here. So far nothing special.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:08:15---->00:09:00]:\n",
            "And this is all just the regular Neo4j API available in the Python\n",
            "Jupyter Notebook version. What I'm doing here is the\n",
            "result of the cypher query. I'm calling the dot graph\n",
            "function to get the graph as an in-memory data structure. If I execute the cell, I get to see this as the graph which is well not really helpful because there is no built in support for visualizing graphs at the Neo4j level inside Jupyter notebooks. So this is where our\n",
            "plugin comes in head handy. So what I'm doing is I'm just importing this\n",
            "graph widget object from the yFiles Jupyter graphs package\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:09:00---->00:09:45]:\n",
            "and calling the constructor\n",
            "to create a widget. And I'm passing in the graph instance from the cell here above. So this is a Neo4j graph and Neo4j graphs, just along with some other graph forwards, are supported out of the box. So I don't need to do anything special to visualize my Neo4j. All I do is execute this cell and this actually launches the widget. So this is the widget and it consists of two paints where there's the central\n",
            "graph visualization and if the graph is larger, I can actually use this overview here over here to navigate my larger graph. But in this case it's small enough so I can just like collapse it. And then there's this sidebar\n",
            "with additional information. I can also expand my screen,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:09:45---->00:10:30]:\n",
            "so in case my visualization\n",
            "is really large, so to see more and then\n",
            "I can navigate the graph by dragging it around or using the mouse or the buttons to zoom in and zoom out. Hovering over the items shows me the properties that are\n",
            "attached to the in-memory graph. And these are the\n",
            "properties that are actually from the Neo4j database.\n",
            "So if you look at these, these are exactly the ones\n",
            "that you get from the database and the label is the Neo4j. The item named label\n",
            "here is the Neo4j label. So this is a city and\n",
            "airport countries and so on. The same goes for the relationships. And clicking on them, I can use the site panel\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:10:30---->00:11:15]:\n",
            "to take a look at the\n",
            "data in a more static way and for example, to compare items I can with control click and\n",
            "I can select multiple elements and see there and compare their values, right inside this panel over here. But it may be difficult\n",
            "to identify a certain item and that's what the search panel is for. So I, I know that there should\n",
            "be Atlanta somewhere in here. So I just look at this and this lets me focus\n",
            "the note inside the graph because right now all I see is that this is in the airport. So I would have to like\n",
            "hover over all these elements or click them to see which\n",
            "one is the Atlanta port. So we are going to improve\n",
            "the situation in a minute\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:11:15---->00:12:00]:\n",
            "because by default if you show the widget, you will only see get\n",
            "to see the note labels and the the relationship types\n",
            "right in the visualization and you have to, to look at the data . . . panel to see more of the properties. So that's not yet a great visualization and we'll change that in a minute. But before I do that, I'll show you the last\n",
            "tap we have over here, which is a neighborhood view. And this is a contextual view, meaning that whenever I change the context by selecting an element over here, I get to see the immediate\n",
            "neighbors of that selected node, as a small utility graph. And in this case all\n",
            "neighbors one hop away.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:12:00---->00:12:45]:\n",
            "So I can change the number\n",
            "of the death over here interactively to get to see the neighbors of the neighbors and the neighbors of the neighbors and the neighbors and so on. And I can use this to\n",
            "actually navigate my graph. So rather than having\n",
            "to work on this view, I can click on these items and\n",
            "they will both synchronize. So whether I'm clicking\n",
            "over here or there, this lets me navigate my graph and of course that's also\n",
            "the properties view in here. So that's the basic setup that we have for the\n",
            "widget and what it does. And it's just one line of code away. And, but as I said, to\n",
            "make this more useful, we have an API provided\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:12:45---->00:13:30]:\n",
            "because this is actually an powerful API that provides you with\n",
            "methods and functions, and properties to\n",
            "customize the visualization to match your requirements. So let's take a look at\n",
            "at how that will work. So again, I'm running a cypher\n",
            "query here in this example, which just queries a number\n",
            "of airports and their cities, how they are related. And later on I will be\n",
            "passing the graph here to the widget again, but rather than directly\n",
            "showing the widget, which happens when I just write this one, I'm first configuring\n",
            "some of the properties. There's properties that have an influence on the general visualization. So like,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:13:30---->00:14:15]:\n",
            "overall properties that\n",
            "affect all the items like directed, which tells which is whether\n",
            "my graph that I'm displaying should by default contain arrowheads at the end of the edges.\n",
            "So it, for example, in the social networks where\n",
            "all my relationships are like nodes and they are person\n",
            "one nodes, person two, then it's probably a\n",
            "symmetric relationship and I don't want to show them\n",
            "as a directed relationship. And I can also switch the graph layout because this is something I\n",
            "forgot you to tell about here is that apart from the default organic layout that we see here, which is a forced directed layout,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:14:15---->00:15:00]:\n",
            "there's this menu item here which lets me choose\n",
            "different automatic layouts and different layouts allow you to highlight different\n",
            "aspects of the graph. For example, the\n",
            "hierarchic layout shows me that this is actually a (indistinct) graph that I got from the query, which is pretty obvious because the query basically was to match airports and their first degree neighbors. And since the those are all direct edges, we see that there's airports all on the one hand side of\n",
            "the graph and the continent, cities and regions on\n",
            "the second partition. So choosing an automatic layout may help your users better\n",
            "understand the data.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:15:00---->00:15:45]:\n",
            "And to help them choose the right layout. You can do this programmatically too. So here in this example I'm choosing the orthogonal\n",
            "layout to be used initially, I am also configuring\n",
            "the neighborhood view that I just showed you so that it'll be using three levels and initially we'll be\n",
            "using the, in this case, the first node from my graph. And of course I could be using\n",
            "more complex logic in here like finding a certain node and showing that node in the neighborhood. I also configure the visualization to not show the sidebar initially. And once the user opens the sidebar, we want to see the\n",
            "neighborhood immediately. And also just for the sake\n",
            "of showing more features,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:15:45---->00:16:30]:\n",
            "I'm disabling the overview\n",
            "graph visualization in the top left corner here initially. But now for the good\n",
            "parts, the cool parts, the really useful parts is for binding, for changing the visual appearance of the various nodes in here. So as I said before, by default all of the nodes look the same and they have the same,\n",
            "they only have their, the aesthetic label in there and we would like to change that. And they also have the same shape, color, and the edges have have the same thickness and you can change all of this\n",
            "dynamically and data bound. So the most simple form is to, well just change like the\n",
            "color of all the nodes.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:16:30---->00:17:15]:\n",
            "And this what works by\n",
            "using a custom mapping. So this is a mapping\n",
            "function for each node. It'll return blue. Well that's a pretty similar and actually not quite useful mapping because well, this will change\n",
            "the color of all the nodes. But it can get more sophisticated and use the data that\n",
            "is bound and available at the various items to drive the visualization. So in the second line, I'm determining what color to\n",
            "use for the edges in my graph and we'll be using orange if it's actually in city relationship and all other edges will be black. So this is using\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:17:15---->00:18:00]:\n",
            "the properties that are\n",
            "available in memory. And there's this extra label property which is mapped to the\n",
            "Neo4j relationship type, just for convenience. They have both been called\n",
            "label here in this case. So we are checking the relationship type, whether it's the city, and then we'll be using orange lines and all black for the other ones. So, but apart from only\n",
            "specifying constant values, you can also make really dynamic values. And specifically for the label mapping, if we look at the airport, there is other information that\n",
            "we would rather like to see directly in the visualization. For example, I would\n",
            "prefer seeing the IATA code\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:18:00---->00:18:45]:\n",
            "and the city name for each of the airports directly in the visualization. So what I'm doing here is I'm\n",
            "declaring a mapping function for each node from the\n",
            "node properties to a label. And what I'm doing is here I'm querying the IATA code and the city property of the node combining this with a new line character. And I'm only applying\n",
            "this to the airport nodes because the other node\n",
            "don't have these properties. And for all other nodes, I'm\n",
            "just using their name property, which in this case is the city and city actually has a name property. So this should be, should result in an nicer label\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:18:45---->00:19:30]:\n",
            "and a much more usable graph. Last but not least,\n",
            "there's also the option to change the geometry of the elements. For example, if I want to change their size to like indicate the\n",
            "importance of those items in my visualization, I can determine, I can\n",
            "specify a scale factor on a per item basis. So in this case, for example, if we would be looking\n",
            "at the airport nodes, we see that these airports actually have information about the number of runways each airport has in this dataset.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:19:30---->00:20:15]:\n",
            "So what could argue that a\n",
            "larger airport has many runways, and I would like to highlight\n",
            "these larger airports by increasing the size of these nodes. So this is what I'm doing here. I'm accessing the runways\n",
            "property of the nodes. I'm dividing it by four and adding one to get values like 1, 1.25, 1.5, 1.75 and so on. This is just an arbitrary\n",
            "function that I came up with, which worked nice for my example. And all other nodes won't be scaled, so they will stay at their original size. So see, let's see how that works out. So this is the new\n",
            "graph and as you can see it has a different layout. It uses the orthogonal layout.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:20:15---->00:21:00]:\n",
            "The node are blue and the edges are, the in city edges are actually orange. And for the various nodes, you can see that the\n",
            "code and the city name is actually rendered\n",
            "right on top of the node. And if I open the sidebar, it directly starts with the neighborhood and I just accidentally click the element. But once I reload this, you would see the one\n",
            "that I specified in code to be highlighted and shown immediately in the neighborhood view. Okay, so this is is just\n",
            "a little bit of coding and you don't want to like repeat this for all of your experiments. So what you can do of course is\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:21:00---->00:21:45]:\n",
            "with just little helper\n",
            "function that I created here, and I called it \"Create Airport Graph.\" and it does all of the stuff\n",
            "that I just showed you, and encapsulated away in a simple function with some simple configurations of it. It's just very simple python and I won't go into the details, but it basically does the\n",
            "same that I just showed you, with a few more convenience things. So this means that once\n",
            "I execute this cell, I have this function available\n",
            "in my Jupyter notebooks. And now displaying a graph from\n",
            "my database is really easy. I just need to call one\n",
            "function and passing the graph. So again, I'm running a\n",
            "cypher query this time\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:21:45---->00:22:30]:\n",
            "there's airport cities, region, countries and continents in there. And when I run this function, I get this result and now you can see that it's looking much nicer than before because the nodes have different shapes depending on the type\n",
            "we see the right labels, we see different colors and so on. So okay, nice, but still not super\n",
            "useful. You might think. However you can start to see that with the hierarchic layout for example, with the the right layout, it's getting much easier to understand the structure of your data.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:22:30---->00:23:15]:\n",
            "Now you see that in your\n",
            "database there's always this, or at least in the query, there's always these airport\n",
            "nodes connecting to cities, connected to countries and to continent. So that's a nice representation of our data that we got here. But now let's add graph\n",
            "data science to this and make this really powerful. So because since this is in a notebook, I can actually use third party packages and APIs like matplotlib, or pandas, to add more features to this. So this is a little bit of programming. So what I'm doing here is\n",
            "it's not that much code. And what I'm doing here is\n",
            "using the graph data science\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:23:15---->00:24:00]:\n",
            "module to actually first\n",
            "calculate some graph data science, in this case the (indistinct) algorithm to calculate some scores\n",
            "for all the airports because I want to, suppose I want to compare\n",
            "the various airports in my database and see how they compare with regards to the\n",
            "(indistinct) algorithm. So what I do here, and I won't go too much into details, I'm using the graph data science module to project a sub-graph which only consists of the airport nodes and the\n",
            "\"has route\" relationships because that's the network of how the different pairs\n",
            "of airports are connected, whether there's a flight\n",
            "available between them and the has route\n",
            "relationships in our database\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:24:00---->00:24:45]:\n",
            "connects all the database of the airports. And with the page rank, I can get a measure of how important or how central a certain airport will be. So calling the GDS library, I get an in-memory result\n",
            "that I can work with in my Jupyter Notebook cell. And since this is actually a pandas table, I can use the pandas API to, for example, filter the table and\n",
            "only include those nodes that are above a certain score. So I don't get to see the\n",
            "super unimportant airports\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:24:45---->00:25:30]:\n",
            "according to my measurement for example. So what I'm doing is\n",
            "I'm filtering the table and then for display I'm again executing a very simple cypher query. So I'm just loading the nodes that I got from the\n",
            "results from the graph data science algorithm. So these, this will just load the nodes with a score larger than\n",
            "one point greater than 1.5. So we are loading this\n",
            "and displaying this graph. Then I'm again setting\n",
            "the labels over here, but also I'm setting a custom scaling which actually uses the\n",
            "ranks that we just calculated\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:25:30---->00:26:15]:\n",
            "to scale the nodes\n",
            "according to their ranks. So, and I can use a custom function here. So any function that suits my use case, I can just code here because\n",
            "this is all just python codes, simple python code. So I'm, I'm using a square root function. So not to scale the nodes linearly with respect to their scores because that would lead to super huge nodes in the visualization. So, I thought that using\n",
            "square root works well and what I'm also doing is\n",
            "I'm using the rank information to color map the nodes. So I'm using third party library, this is from matplotlib\n",
            "to get a color map, which basically gives me a nice\n",
            "gradient of different colors and I'm using,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:26:15---->00:27:00]:\n",
            "I'm mapping the range of the values that I have in my data\n",
            "set, in my results set, to map them to these colors. Last but not least, you may\n",
            "have seen that in the data set, the airports actually have\n",
            "some interesting properties called \"lon\" and \"lat\" which is actually longitude and latitude. So these are geo coordinates. So I am using another mapping\n",
            "that I can specify here, which is the node position mapping, which allows me for each node, to specify where on the\n",
            "screen I want it to be placed. So why not use the geo\n",
            "coordinates for that? So I'm just using this super\n",
            "scientific formula over here\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:27:00---->00:27:45]:\n",
            "that I came up with multiplying\n",
            "the longitude value by 150 and the latitude by minus 200. And if I run this, we get to see this, DNS hiccup, but there's the results. So, if you look at this, you will probably recognize that over here there's actually Europe,\n",
            "this is the world map, there's Asia over here, somewhere here should be Australia, south America and the\n",
            "US. And if you zoom in, we see that there's\n",
            "these airports over here. For example, this is\n",
            "Paris Charles du Gaulle, important airport, which we see because it has this light\n",
            "green and the larger size according to our measurements.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:27:45---->00:28:30]:\n",
            "So that's some nice combination\n",
            "of graph data science, visualization and geocoding. But it's not really a graphy example. So as a last example, we are going to use graph data science to do a really graphy tasks and that is calculating a shortest path. So we'll be using the\n",
            "shortest path algorithm from the graph data science library to calculate the shortest path between a set of airports. And for this, again, I'm\n",
            "creating a projection for the graph data science library, which contains the airports and has route relationships only. And we're including the distance property,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:28:30---->00:29:15]:\n",
            "which is available in our data set. So there's someone calculated the distances\n",
            "between all those airports, the flight distances, and we used these distances to calculate the shortest path\n",
            "between one pair of airports. And I'm using this utility function from the GDS library\n",
            "to load these airports. In this case it's Stuttgart and ART calling the shortest path method. And what this results in\n",
            "is just a set of nodes, just a set of airport nodes. So because the draft data science library only ran the query the . . .\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:29:15---->00:30:00]:\n",
            "the algorithm on the airport nodes and it doesn't have the information about the other nodes\n",
            "that we have in there like country location, city,\n",
            "country, continent and so on. So what I'm doing next is\n",
            "with those results, again, I'm doing another query to actually load a sub-graph\n",
            "from the full database. And this contains both the airports that we've\n",
            "got from our result nodes as well as the connecting\n",
            "route between them. But for each of the\n",
            "airports we also include the chain of the city, the region, the country and the continent for both the source and the target node. So this will result in a visualization\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:30:00---->00:30:45]:\n",
            "that contains the shortest\n",
            "path between SDR and ART and all the intermediate path\n",
            "elements if there are any. And for each of the\n",
            "intermediate path elements, there's also information about,\n",
            "well, the location of those. Then I'm using the\n",
            "function that I used before and that's the only part where actually our widget comes into play. So this is all unrelated to the widget and if I execute this, I\n",
            "get to see this result. Let's take a look at this. The hierarchic layout is probably best because what we see here is this is our shortest path that's from Stuttgart Airport,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:30:45---->00:31:30]:\n",
            "going to London, Heathrow,\n",
            "Philadelphia, and ART, that's Watertown International Airport. And we also see the distances\n",
            "between those elements and for each of those elements we see, what city, region and country they are in. So we see that Stuttgart\n",
            "and London is both in on the continent, Europe, and the other two airports are in the US and the US is according to this data, is actually North American. And I believe it says it's also Africa, which is probably not correct, but that's a problem with the data and not something that we\n",
            "will have to work with today. It also says that London is\n",
            "both in England and Canada,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:31:30---->00:32:15]:\n",
            "so that's likely an issue\n",
            "that we have in our data. But yeah, this is nice. So this is makes for nice\n",
            "visualization of our results, but we can do more. And as a last step, I'm going to show you an integration that we just added to this plugin and that is an integration with our suite of Data\n",
            "Explorer applications. So with all of these results, I can just open them in\n",
            "an external application that we have hosted on our website, which lets me visualize and explore the\n",
            "graph in a different way. And this is actually outside the scope of the Jupyter lab extension.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:32:15---->00:33:00]:\n",
            "So this could also be used\n",
            "with different data sources and we have the same one for Neo4j. So, but this is different, so it's not going to the database, it's just allowing you to\n",
            "explore the schema and the data in from the query that you\n",
            "had in your Neo4j notebook. So I can just like, iteratively load the other\n",
            "elements in here and explore them just as you may know way\n",
            "from the Neo4j browser. And I can also, in this case\n",
            "just reveal the entire graph. And now what's the difference here? I can get much more sophisticated with regards to visualization.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:33:00---->00:33:45]:\n",
            "So there's options to\n",
            "style those elements and, it took me some time to come\n",
            "up with a nicer styling, so I prepared this one, so\n",
            "let me just load them in. And now we have like\n",
            "super cool visualization that's only possible within\n",
            "the data explorer here where you have like, there's icons in there and\n",
            "there's additional code in there. So that's much more\n",
            "sophisticated than what we, we've seen before, but it's also more helpful\n",
            "and more end user friendly so to say. And what's also unique to this application is the last thing and that is that, if we look at the schema,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:33:45---->00:34:30]:\n",
            "we actually have the\n",
            "geo coordinates in here, so wouldn't it be cool to see those nodes actually on a map view? And this is what what we can do in here. I mark this property to\n",
            "be used as map coordinates and now if I go back to the explorer and hit the map layout, I get finally get to see my route, which goes from Stuttgart,\n",
            "via London, Heathrow, over to Philadelphia and Watertown. Okay, so much for this application. Let's see the summary. We arrived at the summary\n",
            "slide hopefully in time. I hope I was able to show you\n",
            "that creating visualizations\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:34:30---->00:35:15]:\n",
            "from your Neo4j data with\n",
            "the help of Jupyter Notebooks is easy and powerful. If you are a data scientist\n",
            "working with Jupyter Notebooks, you can use the yFiles\n",
            "graph for Jupyter Plugin as an entry point to render\n",
            "your Neo4j cypher query results. And you can combine the powerful tools and scripting capabilities that you get from Jupyter Notebooks to create bespoke helpful visualizations, run graph data science algorithms and interactively explore your graphs both directly in the notebook as well as conveniently in\n",
            "the generic Data Explorer app. That's it for now. We would love to hear your\n",
            "feedback about our tools. It's your input that helps\n",
            "us continuously improve them. So try the tools, share\n",
            "them with the community, leave us a star or a\n",
            "like in the social media\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:35:15---->00:36:00]:\n",
            "or of course contact us directly. I'll also hang around in the chat and I'm happy to answer\n",
            "your questions there. As always, happy diagram with your Neo4j databases and the visualization tools by Webworks. Thanks and everyone enjoy the\n",
            "conference. Back to you Alex. - That was great. People seem to like it as\n",
            "well. I see lots of clapping, so that's always good encouraging because, you know, virtual conference so we don't hear your clapping. We you need to, you\n",
            "need to emoji the claps. (Alex laughing) - Thank you so much. - We have, we are on a\n",
            "little bit over time, so we don't have much time for question, but I think we can squeeze\n",
            "in one comes from Michael, \"Does it work in Google CoLab?\"\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:36:00---->00:36:45]:\n",
            "Because you said there's some\n",
            "restrictions on the usage. - At this very moment it doesn't, but this should be actually doable and this is actually very\n",
            "high up in our roadmap, so I hope we are going to\n",
            "see Google CoLab support in one of the next versions. Probably if all goes well in q1, meaning, yeah, well early in 2023. So yeah, that's, it's up on our roadmap, but at the moment it doesn't\n",
            "so really Jupyter Lab or Jupyter notebooks. Sorry for that. - No problem. And then from Jason, maybe we squeeze this in as well, \"Will you share the, your notebook file with your presentation or\n",
            "make it public somewhere?\" So people can- - Yeah, I can do that.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:36:45---->End of Video]:\n",
            "There's already a simple example which I actually based my slides upon is already on the GitHub\n",
            "repository of the plugin. So there's a generic Neo4j\n",
            "example and the GDS example. So, but I'm going to share\n",
            "this specific one too soon. - Okay, perfect. Well done. Thank you very much Sebastian, thank you for the great\n",
            "presentation and yeah, see you soon on another Nodes\n",
            "Conference or Neo4j event. (Alex laughing) - Thank you, have a great day. - Thank you.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a StringIO object to hold the content in memory\n",
        "memory_file = StringIO()\n",
        "\n",
        "# Define the output file name\n",
        "output_file = 'youtube_script.txt'\n",
        "# Open the file in write mode\n",
        "\n",
        "chunks = transcript_documents\n",
        "with open(output_file, 'w') as file:\n",
        "    # Iterate over each chunk in the list with an index to access the next chunk\n",
        "    for i in range(len(chunks)):\n",
        "        # Extract metadata and content from the current chunk\n",
        "        title = chunks[i].metadata.get('title', 'Unknown Title')\n",
        "        start_timestamp = chunks[i].metadata.get('start_timestamp', 'Unknown Timestamp')\n",
        "        page_content = chunks[i].page_content\n",
        "\n",
        "        # Determine the end timestamp\n",
        "        if i + 1 < len(chunks):\n",
        "            end_timestamp = chunks[i + 1].metadata.get('start_timestamp', 'Unknown Timestamp')\n",
        "        else:\n",
        "            end_timestamp = \"End of Video\"  # or some appropriate end marker\n",
        "\n",
        "        title = chunks[i].metadata.get('title')\n",
        "        # Prepare the content to write\n",
        "        content = (\n",
        "            f\"Video Extract : {title}\\n\"\n",
        "            f\"[{start_timestamp}---->{end_timestamp}]:\\n{page_content}\\n\"\n",
        "            f\"\\n\"\n",
        "        )\n",
        "\n",
        "        # Write the content to both the file and the StringIO object\n",
        "        file.write(content)\n",
        "        memory_file.write(content)\n",
        "\n",
        "# Get the content from the StringIO object as a string\n",
        "memory_content = memory_file.getvalue()\n",
        "\n",
        "# Print confirmation message\n",
        "print(f\"Script written to {output_file}\")\n",
        "\n",
        "# You can use `memory_content` as needed, it's the full script content in memory\n",
        "print(memory_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "UDxB1qn6lDZP"
      },
      "outputs": [],
      "source": [
        "\n",
        "initial_chunk_prompt_template = \"\"\"\n",
        "Task: Here is a partial transcript of a youtube video with timestamps. Your task is to destile the information from  the raw transcript and write a concise well structure article that summarizes the\n",
        "      new or intersting information. The timestamps where the information is being extracted should be added at the end. In case part or the whole of the video is a tutorial it is important to list the steps provided. \n",
        "      The reader might then go to the timestamps to check so include a chronological list or timeline summarizing the sequence of topics discussed in the video.\n",
        "Part of Transcription:\n",
        "{text}\n",
        "PARTIAL ARTICLE:\n",
        "\n",
        "[start_timestamp---->end_timestamp]\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "refine_chunk_template = \"\"\"\n",
        "We are working on processing the information from a long youtube transcript and destile it into one well structure cohesive text either a Medium Article or a scientific paper whatever fits best. The idea is that a reader with a technical backgroud\n",
        "can easily read our composition and understand what new information/technology/insight is being presented in the video.\n",
        "\n",
        "We are working by parts because the transcript it's a very big.\n",
        "We have made a partial document up to a certain point of the transcript: {existing_answer}\n",
        "\n",
        "Your job is to merge and maybe if appropiate summarize this partial articles together into a consice coherent document with an abstract, introduction, body paragraphs and conclusion\n",
        "\n",
        "Requirements for the document:\n",
        "Focus on what new knowledege is the video presenting. Explain what new or intersting, technologies,tool,algorithm,discovery, are being discussed and how do they work and why are they relevant\n",
        "\n",
        "The document should have an abstract,introduction, and a paragraph for each key topic/tool/algorithm/etc.\n",
        "The body paragraphs should  reference the timestamps of the transcript that support what they are saying as if they were citations.\n",
        "The reader might then go to the timestamps to check so include a chronological list or timeline summarizing the sequence of topics discussed in the video\n",
        "\n",
        "In case the video or part of the video is a tutorial list the steps provided.\n",
        "\n",
        "\n",
        "Continue and refine the existing partial article with  more context below if appropiate in a way that is still coherent. Otherwise return the current partial article\n",
        "------------\n",
        "{text}\n",
        "------------\n",
        "\n",
        "\n",
        "PARTIALLY PROCESS DOCUMENT:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ypWCJIEIBJ_"
      },
      "outputs": [],
      "source": [
        "\n",
        "chunks = text_splitter.split_text(memory_content)\n",
        "transcript_chunk_documents = text_splitter.create_documents(chunks)\n",
        "\n",
        "for i,document in enumerate(transcript_chunk_documents):\n",
        "    document.metadata.update({\n",
        "        \"video_title\":transcript_documents[0].metadata['title'],\n",
        "        \"video_url\":transcript_documents[0].metadata['source'],\n",
        "        \"video_author\":transcript_documents[0].metadata['author']\n",
        "    })\n",
        "    \n",
        "\n",
        "intial_prompt = PromptTemplate.from_template(initial_chunk_prompt_template)\n",
        "\n",
        "refine_chunk_prompt = PromptTemplate.from_template(refine_chunk_template)\n",
        "\n",
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    question_prompt=intial_prompt,\n",
        "    refine_prompt=refine_chunk_prompt,\n",
        "    return_intermediate_steps=False,\n",
        "    input_key=\"input_documents\",\n",
        "    output_key=\"output_text\",\n",
        ")\n",
        "result = chain.invoke({\"input_documents\": documents}, return_only_outputs=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jx-ajKAlhtM"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(result['output_text'])\n",
        "\n",
        "summary_content = result['output_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DUhpmyXsycX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Knowledege Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GTpG1Ry3w1X5"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.2,max_tokens=4096)\n",
        "\n",
        "\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=4000,\n",
        "    chunk_overlap=1000,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_document =  result['output_text'] + '/n Transcript' + memory_content\n",
        "\n",
        "chunks_processed = text_splitter.split_text(processed_document)\n",
        "\n",
        "documents = text_splitter.create_documents(chunks_processed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abstract:\n",
            "The video \"Explore Your Graphs Visually With Jupyter Notebooks\" by Sebastian Mller at NODES2022 explores the significance of graph visualization tools, particularly for Neo4j Graphs. It emphasizes the value of visually exploring data in Neo4j databases using Jupyter Notebooks and literate programming. The session covers topics such as automating visualization tasks, creating custom visualizations, connecting Jupyter Notebooks to Neo4j through Cypher Query, and integrating graph data science algorithms for enhanced analysis and visualization of graph data structures.\n",
            "\n",
            "Introduction:\n",
            "Sebastian Mller highlights the importance of graph visualization tools for Neo4j Graphs and introduces the concept of visually exploring data in Neo4j databases using Jupyter Notebooks and literate programming. The session aims to provide insights into automating visualization tasks, creating custom visualizations, and connecting Jupyter Notebooks to Neo4j using Cypher Query to enhance data analysis capabilities. Furthermore, the integration of graph data science algorithms is introduced to enable advanced analysis and visualization of graph data structures.\n",
            "\n",
            "Body:\n",
            "1. Automating Visualization Tasks with Jupyter Notebooks (00:02:15---->00:03:00):\n",
            "Mller demonstrates the process of automating visualization tasks with literate programming in notebooks, creating custom visualizations to understand data in graph databases. By setting up Jupyter Notebooks to connect to Neo4j, using Cypher Query, and leveraging the graph data science library, users can perform analysis tasks and create informative graph visualizations.\n",
            "\n",
            "2. yFiles Graphs for Jupyter (00:03:00---->00:03:45):\n",
            "The focus is on yFiles graphs for Jupyter, a software development kit by yWorks for building sophisticated graph visualization applications with automatic layouts and rich interactions. The Jupyter widget, built on the yFiles library, provides free tools for graph visualization without registration or installation, with support for interactive data exploration and Neo4j databases.\n",
            "\n",
            "3. Jupyter Notebook Plugin (00:04:30---->00:07:30):\n",
            "The Jupyter Notebook plugin offers interactive graph visualization and exploration capabilities through a widget embedded in Jupyter Notebook and Jupyter lab cells. Users can visualize connected graph data from various sources, including Neo4j graphs, and run Cypher queries for complex interactive notebooks. The plugin allows for applying automatic graph layouts, custom visualization rules, and easy installation through \"pip install.\"\n",
            "\n",
            "4. Installing Neo4j and Graph Data Science Packages (00:06:45---->00:07:30):\n",
            "Mller discusses the installation of Neo4j and graph data science packages for later use, highlighting the compatibility issues with third-party viewers like Visual Studio Code. The functionality requires a regular Jupyter Notebook or Jupyter lab server for proper usage.\n",
            "\n",
            "5. Utilizing Graph Data Science Library in Jupyter Notebooks (00:07:30---->00:08:15):\n",
            "The graph data science library enables users to run Cypher queries in Jupyter Notebooks using the Python API, allowing for the visualization of graph data structures. The integration of the graph data science library in Jupyter Notebooks opens up possibilities for various use cases and data analysis tasks.\n",
            "\n",
            "6. Visualizing Neo4j Graphs with the Jupyter Widget (00:08:15---->00:10:30):\n",
            "Mller demonstrates the visualization of Neo4j graphs using the yFiles Jupyter graphs package and the graph widget constructor. The widget provides a central graph visualization area with interactive features for navigating and exploring the graph data. Users can view properties and relationships of nodes in the graph, enhancing their understanding of the data structure.\n",
            "\n",
            "7. Enhancing Graph Visualization with Interactive Features (00:09:45---->00:10:30):\n",
            "The video showcases the interactive features of the graph visualization tool, allowing users to navigate large visualizations, compare items, and search for specific elements within the graph. By hovering over items, users can view properties from the Neo4j database, making it easier to identify and analyze data.\n",
            "\n",
            "8. Customizing Visualization Properties and Neighborhood Views (00:10:30---->00:12:45):\n",
            "Users can customize the visualization properties to improve the clarity of the graph representation, such as displaying additional data and relationships. The neighborhood view feature provides a contextual view of immediate neighbors, allowing for interactive navigation and exploration of the graph structure.\n",
            "\n",
            "9. Leveraging the API for Customization (00:12:45---->00:13:30):\n",
            "The powerful API provided allows users to customize the visualization to match their requirements by providing methods, functions, and properties. By running Cypher queries and configuring visualization properties, users can enhance the visual representation of the graph data in Jupyter Notebooks.\n",
            "\n",
            "10. Integrating Graph Data Science Algorithms for Advanced Analysis (00:21:45---->00:25:30):\n",
            "Mller demonstrates the integration of graph data science algorithms, such as the PageRank algorithm, to analyze and visualize graph data structures in Neo4j databases. By utilizing third-party packages and APIs like matplotlib and pandas, users can enhance the features of their visualizations and perform advanced analysis tasks. The application of graph data science algorithms provides insights into the importance and centrality of nodes within the graph, enabling users to make informed decisions based on the data.\n",
            "\n",
            "11. Custom Scaling and Color Mapping for Graph Visualization (00:24:45---->00:26:15):\n",
            "Mller discusses custom scaling and color mapping techniques for graph visualization based on node ranks and properties. By using Python code, users can apply functions like square root for scaling nodes and utilize third-party libraries like matplotlib for color mapping, enhancing the visual representation of the graph data.\n",
            "\n",
            "12. Geo Mapping and Node Positioning for Enhanced Visualization (00:26:15---->00:27:45):\n",
            "The video explores the use of geo coordinates for node positioning in graph visualization, allowing users to specify node locations on the screen based on longitude and latitude values. By mapping values to colors and utilizing a scientific formula for positioning, users can create visually appealing and informative graph visualizations.\n",
            "\n",
            "13. Calculating Shortest Paths with Graph Data Science Algorithms (00:27:45---->00:28:30):\n",
            "Mller demonstrates the use of the shortest path algorithm from the graph data science library to calculate the shortest path between a set of airports. By creating a projection with route relationships and distance properties, users can efficiently perform graph-based tasks and gain insights into connectivity within the graph structure.\n",
            "\n",
            "14. Integration with Data Explorer Applications (00:30:45---->00:34:30):\n",
            "Mller showcases an integration with Data Explorer applications for visualizing and exploring graph data in a different way. This external application allows for sophisticated visualization and exploration of graph data, including the use of geo coordinates for map views and advanced styling options. The integration with Data Explorer applications enhances the visualization capabilities beyond what is possible within Jupyter Notebooks, providing a more user-friendly and comprehensive experience for exploring graph data.\n",
            "\n",
            "Conclusion:\n",
            "The video provides a comprehensive exploration of tools and techniques for visually analyzing graph data in Neo4j databases. By leveraging Jupyter Notebooks, Cypher Query, and the graph data science library, users can automate visualization tasks, create custom visualizations, and improve their data analysis skills. The integration of graph data science algorithms and external applications like Data Explorer enhances the capabilities of visualizing and analyzing complex graph structures, offering valuable insights for professionals seeking to enhance their graph data visualization skills and gain a deeper understanding of their databases./n TranscriptVideo Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:00:00---->00:00:45]:\n",
            "(upbeat music) - Thank you so much Alex for\n",
            "this nice introduction. So, You stole the content\n",
            "of my first few slides, but so hello, Nodes 22. I'm so happy to be here today. Great to see all those\n",
            "people interested in graphs as always. And in my session as Alex said, I would like to show you some\n",
            "cool graph visualization tools for many of the great use\n",
            "cases for a graph database visualization is super valuable tool that you should be aware of and it doesn't have to be complicated as you will see. Specifically I'll talk about\n",
            "visually exploring your name, Neo4j Graphs with Jupyter Notebooks\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:00:45---->00:01:30]:\n",
            "and simple literate programming. Whether you are a software\n",
            "developer or a data scientist or a database administrator\n",
            "or a project manager, my talk should give you some ideas on when you would want to\n",
            "be using graph visualization to better understand the\n",
            "data in your Neo4j databases. After my talk, I bet some of you go and\n",
            "try this on their own data. It's safe and it's easy and it's free and it's always exciting\n",
            "to get to actually see your graph data in a meaningful\n",
            "way for the first time. So some more details about myself. So, together with my team at yWorks, I've been working in the field of graph and diagram visualization for more than what's soon\n",
            "going to be 25 years.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:01:30---->00:02:15]:\n",
            "And our tagline is, \"The\n",
            "diagramming experts,\" and we help teams and\n",
            "enterprises worldwide with their graph visualization tasks. As graph drawing specialists, we have seen a huge number of use cases and data sets over the years. We've been working on and\n",
            "providing high quality tools and support to our customers and the community around that topic. And the tool we'll be looking at today is just one of these tools. So here's the agenda,\n",
            "what will you learn today and what are we going\n",
            "to see in the session? We'll be looking at easy ways to work with Neo4j databases plus the Neo4j\n",
            "graph data science library in the context of Jupyter Notebooks. But don't worry, although\n",
            "this will also mean we'll be looking at some lines of code, this is not going to be really difficult.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:02:15---->00:03:00]:\n",
            "We look at automating visualization tasks with the help of literate\n",
            "programming in notebooks. I'm going to show you how to\n",
            "create custom visualizations that will help us and our users\n",
            "better understand the data hidden in our graph databases. So for this I'll show you\n",
            "how to set up Jupyter, Jupyter Notebooks to connect to Neo4j. Use Cypher Query the database contents and the graph data science library to optionally perform some analysis tasks. We'll see how to create\n",
            "graph visualizations in Jupyter Notebooks and labs. They won't just look beautiful, but we will also make sure\n",
            "to include the information in the visualization that we\n",
            "want to provide our users with. So here's a quick overview of how all the things are connected that I'll be using today.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:03:00---->00:03:45]:\n",
            "Pun intended, the focus is\n",
            "on yFiles graphs for Jupyter, which is at the top and the\n",
            "left half basically explains how all this works and adds some context. So yWorks, the company I work at, creates a software development\n",
            "kit called \"yFiles.\" It allows software developers to build sophisticated graph\n",
            "visualization applications with great automatic layouts\n",
            "and rich custom interactions. And the Jupyter widget\n",
            "we'll be looking at today is also built on top of that library. And by the way, those tools at the center, they are all free to use so\n",
            "they don't require registration and the lower two don't\n",
            "even require installation nor download. You can just use\n",
            "them for free on our website. I will only briefly speak about the data explorer apps in this session.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:03:45---->00:04:30]:\n",
            "These belong to another set of tools to interactively explore connected data and one of them can be\n",
            "launched directly from within the Jupyter widget as we'll see, but they can also be used standalone without the Jupyter context. Actually at last year's Nodes conference, I dedicated a whole session\n",
            "to the data explorer for Neo4j and even more\n",
            "so the app generator. So I won't be covering it this time so, but feel free to browse the archives and watch the recording\n",
            "after the conference. All of these tools have built in support for Neo4j databases. So all you need to get started is a set of credentials\n",
            "to access your database. Read only access is\n",
            "enough for many use cases and the data never leaves your systems or touches our service. So there's a direct connection\n",
            "between your machine\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:04:30---->00:05:15]:\n",
            "and your database and you\n",
            "can run the tools offline behind the firewall if you wish to. So don't be afraid and just try it. Okay, so let's take a look at\n",
            "the Jupyter Notebook plugin. And by the way, the plugin's\n",
            "issue tracker is on GitHub. So when you use it and feel\n",
            "like it's missing a feature, please do file an issue on GitHub and the plug-in is\n",
            "under active development and the more people find it useful, the more time we can actually\n",
            "invest into the tool. So this is what the plug-in does. It provides interactive\n",
            "graph visualization and exploration capabilities\n",
            "in the form of a widget that can be embedded in Jupyter Notebook and Jupyter lab notebook cells. So you can use it to\n",
            "visualize connected graph data while from many different\n",
            "kinds of different data sources\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:05:15---->00:06:00]:\n",
            "that you have access\n",
            "to from your notebooks. And of course this in\n",
            "this includes Neo4j graphs which are supported right out the the box. This enables you to\n",
            "visualize the graph results of cypher queries. So the main difference\n",
            "to the Neo4j browser where you can do similar\n",
            "things is the target audience and thus also the support for\n",
            "more sophisticated use cases. While the Neo4j browser is essential for database\n",
            "administrator tasks, it's not so much a great\n",
            "scripting environment for one, it's basically limited to\n",
            "running cypher queries, but in a Jupyter notebook you can do that but you can do so much more. As a data scientist, you can write complex\n",
            "interactive notebooks that dynamically query\n",
            "data from various sources,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:06:00---->00:06:45]:\n",
            "do sophisticated pre-processing and then combine this with\n",
            "any number of cypher queries to create a far more complex and hopefully helpful visual realization. Also with widget you can apply various automatic\n",
            "graph layouts to the graphs in order to highlight the\n",
            "different aspects of the structure and the connectivity in your data. And with advanced custom\n",
            "rules for the visualization, you can add style to your nodes and relationships in the diagram and make it easier for the user to quickly see the important data. So we'll get to this in a minute. Installing the widget, it is easy. For example, you can\n",
            "just use \"pip install.\" So if you have used\n",
            "Jupyter notebooks before, that should be all familiar to you.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:06:45---->00:07:30]:\n",
            "Here in this example I'm\n",
            "also installing the Neo4j and graph data science\n",
            "packages for later use too. Note that you might need to restart the Jupyter server after this and please be aware that the\n",
            "functionality at this time, unfortunately for compatibility reasons, you should please open them\n",
            "using a regular Jupyter Notebook or Jupyter lab server. It doesn't work in all third party viewers like Visual Studio Code yet. So now let's take a look\n",
            "at the working widget. This over to the live demo. So I have a plain\n",
            "Jupyter Labs installation plus only the packages\n",
            "from that last slide. So the graph data science one isn't actually required\n",
            "to connect to Neo4j,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:07:30---->00:08:15]:\n",
            "but of course having the\n",
            "graph data science library available in a Jupyter notebook really allows for some\n",
            "very cool use cases. For the upcoming examples, I'm using the airports example database that comes with one of the\n",
            "graph data science tutorials. So I have set up a Neo4j (indistinct) with graph data science library enabled and I uploaded the sample\n",
            "database data before. So this means I can now run cypher queries right in my Jupyter Notebook\n",
            "using the Python API. So here I am importing\n",
            "the graph database object from the Neo4j package in\n",
            "my Jupyter Notebook cell. Instantiating a driver and the session with a certain database. Then I run the cypher query over here. So far nothing special.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:08:15---->00:09:00]:\n",
            "And this is all just the regular Neo4j API available in the Python\n",
            "Jupyter Notebook version. What I'm doing here is the\n",
            "result of the cypher query. I'm calling the dot graph\n",
            "function to get the graph as an in-memory data structure. If I execute the cell, I get to see this as the graph which is well not really helpful because there is no built in support for visualizing graphs at the Neo4j level inside Jupyter notebooks. So this is where our\n",
            "plugin comes in head handy. So what I'm doing is I'm just importing this\n",
            "graph widget object from the yFiles Jupyter graphs package\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:09:00---->00:09:45]:\n",
            "and calling the constructor\n",
            "to create a widget. And I'm passing in the graph instance from the cell here above. So this is a Neo4j graph and Neo4j graphs, just along with some other graph forwards, are supported out of the box. So I don't need to do anything special to visualize my Neo4j. All I do is execute this cell and this actually launches the widget. So this is the widget and it consists of two paints where there's the central\n",
            "graph visualization and if the graph is larger, I can actually use this overview here over here to navigate my larger graph. But in this case it's small enough so I can just like collapse it. And then there's this sidebar\n",
            "with additional information. I can also expand my screen,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:09:45---->00:10:30]:\n",
            "so in case my visualization\n",
            "is really large, so to see more and then\n",
            "I can navigate the graph by dragging it around or using the mouse or the buttons to zoom in and zoom out. Hovering over the items shows me the properties that are\n",
            "attached to the in-memory graph. And these are the\n",
            "properties that are actually from the Neo4j database.\n",
            "So if you look at these, these are exactly the ones\n",
            "that you get from the database and the label is the Neo4j. The item named label\n",
            "here is the Neo4j label. So this is a city and\n",
            "airport countries and so on. The same goes for the relationships. And clicking on them, I can use the site panel\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:10:30---->00:11:15]:\n",
            "to take a look at the\n",
            "data in a more static way and for example, to compare items I can with control click and\n",
            "I can select multiple elements and see there and compare their values, right inside this panel over here. But it may be difficult\n",
            "to identify a certain item and that's what the search panel is for. So I, I know that there should\n",
            "be Atlanta somewhere in here. So I just look at this and this lets me focus\n",
            "the note inside the graph because right now all I see is that this is in the airport. So I would have to like\n",
            "hover over all these elements or click them to see which\n",
            "one is the Atlanta port. So we are going to improve\n",
            "the situation in a minute\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:11:15---->00:12:00]:\n",
            "because by default if you show the widget, you will only see get\n",
            "to see the note labels and the the relationship types\n",
            "right in the visualization and you have to, to look at the data . . . panel to see more of the properties. So that's not yet a great visualization and we'll change that in a minute. But before I do that, I'll show you the last\n",
            "tap we have over here, which is a neighborhood view. And this is a contextual view, meaning that whenever I change the context by selecting an element over here, I get to see the immediate\n",
            "neighbors of that selected node, as a small utility graph. And in this case all\n",
            "neighbors one hop away.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:12:00---->00:12:45]:\n",
            "So I can change the number\n",
            "of the death over here interactively to get to see the neighbors of the neighbors and the neighbors of the neighbors and the neighbors and so on. And I can use this to\n",
            "actually navigate my graph. So rather than having\n",
            "to work on this view, I can click on these items and\n",
            "they will both synchronize. So whether I'm clicking\n",
            "over here or there, this lets me navigate my graph and of course that's also\n",
            "the properties view in here. So that's the basic setup that we have for the\n",
            "widget and what it does. And it's just one line of code away. And, but as I said, to\n",
            "make this more useful, we have an API provided\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:12:45---->00:13:30]:\n",
            "because this is actually an powerful API that provides you with\n",
            "methods and functions, and properties to\n",
            "customize the visualization to match your requirements. So let's take a look at\n",
            "at how that will work. So again, I'm running a cypher\n",
            "query here in this example, which just queries a number\n",
            "of airports and their cities, how they are related. And later on I will be\n",
            "passing the graph here to the widget again, but rather than directly\n",
            "showing the widget, which happens when I just write this one, I'm first configuring\n",
            "some of the properties. There's properties that have an influence on the general visualization. So like,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:13:30---->00:14:15]:\n",
            "overall properties that\n",
            "affect all the items like directed, which tells which is whether\n",
            "my graph that I'm displaying should by default contain arrowheads at the end of the edges.\n",
            "So it, for example, in the social networks where\n",
            "all my relationships are like nodes and they are person\n",
            "one nodes, person two, then it's probably a\n",
            "symmetric relationship and I don't want to show them\n",
            "as a directed relationship. And I can also switch the graph layout because this is something I\n",
            "forgot you to tell about here is that apart from the default organic layout that we see here, which is a forced directed layout,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:14:15---->00:15:00]:\n",
            "there's this menu item here which lets me choose\n",
            "different automatic layouts and different layouts allow you to highlight different\n",
            "aspects of the graph. For example, the\n",
            "hierarchic layout shows me that this is actually a (indistinct) graph that I got from the query, which is pretty obvious because the query basically was to match airports and their first degree neighbors. And since the those are all direct edges, we see that there's airports all on the one hand side of\n",
            "the graph and the continent, cities and regions on\n",
            "the second partition. So choosing an automatic layout may help your users better\n",
            "understand the data.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:15:00---->00:15:45]:\n",
            "And to help them choose the right layout. You can do this programmatically too. So here in this example I'm choosing the orthogonal\n",
            "layout to be used initially, I am also configuring\n",
            "the neighborhood view that I just showed you so that it'll be using three levels and initially we'll be\n",
            "using the, in this case, the first node from my graph. And of course I could be using\n",
            "more complex logic in here like finding a certain node and showing that node in the neighborhood. I also configure the visualization to not show the sidebar initially. And once the user opens the sidebar, we want to see the\n",
            "neighborhood immediately. And also just for the sake\n",
            "of showing more features,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:15:45---->00:16:30]:\n",
            "I'm disabling the overview\n",
            "graph visualization in the top left corner here initially. But now for the good\n",
            "parts, the cool parts, the really useful parts is for binding, for changing the visual appearance of the various nodes in here. So as I said before, by default all of the nodes look the same and they have the same,\n",
            "they only have their, the aesthetic label in there and we would like to change that. And they also have the same shape, color, and the edges have have the same thickness and you can change all of this\n",
            "dynamically and data bound. So the most simple form is to, well just change like the\n",
            "color of all the nodes.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:16:30---->00:17:15]:\n",
            "And this what works by\n",
            "using a custom mapping. So this is a mapping\n",
            "function for each node. It'll return blue. Well that's a pretty similar and actually not quite useful mapping because well, this will change\n",
            "the color of all the nodes. But it can get more sophisticated and use the data that\n",
            "is bound and available at the various items to drive the visualization. So in the second line, I'm determining what color to\n",
            "use for the edges in my graph and we'll be using orange if it's actually in city relationship and all other edges will be black. So this is using\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:17:15---->00:18:00]:\n",
            "the properties that are\n",
            "available in memory. And there's this extra label property which is mapped to the\n",
            "Neo4j relationship type, just for convenience. They have both been called\n",
            "label here in this case. So we are checking the relationship type, whether it's the city, and then we'll be using orange lines and all black for the other ones. So, but apart from only\n",
            "specifying constant values, you can also make really dynamic values. And specifically for the label mapping, if we look at the airport, there is other information that\n",
            "we would rather like to see directly in the visualization. For example, I would\n",
            "prefer seeing the IATA code\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:18:00---->00:18:45]:\n",
            "and the city name for each of the airports directly in the visualization. So what I'm doing here is I'm\n",
            "declaring a mapping function for each node from the\n",
            "node properties to a label. And what I'm doing is here I'm querying the IATA code and the city property of the node combining this with a new line character. And I'm only applying\n",
            "this to the airport nodes because the other node\n",
            "don't have these properties. And for all other nodes, I'm\n",
            "just using their name property, which in this case is the city and city actually has a name property. So this should be, should result in an nicer label\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:18:45---->00:19:30]:\n",
            "and a much more usable graph. Last but not least,\n",
            "there's also the option to change the geometry of the elements. For example, if I want to change their size to like indicate the\n",
            "importance of those items in my visualization, I can determine, I can\n",
            "specify a scale factor on a per item basis. So in this case, for example, if we would be looking\n",
            "at the airport nodes, we see that these airports actually have information about the number of runways each airport has in this dataset.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:19:30---->00:20:15]:\n",
            "So what could argue that a\n",
            "larger airport has many runways, and I would like to highlight\n",
            "these larger airports by increasing the size of these nodes. So this is what I'm doing here. I'm accessing the runways\n",
            "property of the nodes. I'm dividing it by four and adding one to get values like 1, 1.25, 1.5, 1.75 and so on. This is just an arbitrary\n",
            "function that I came up with, which worked nice for my example. And all other nodes won't be scaled, so they will stay at their original size. So see, let's see how that works out. So this is the new\n",
            "graph and as you can see it has a different layout. It uses the orthogonal layout.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:20:15---->00:21:00]:\n",
            "The node are blue and the edges are, the in city edges are actually orange. And for the various nodes, you can see that the\n",
            "code and the city name is actually rendered\n",
            "right on top of the node. And if I open the sidebar, it directly starts with the neighborhood and I just accidentally click the element. But once I reload this, you would see the one\n",
            "that I specified in code to be highlighted and shown immediately in the neighborhood view. Okay, so this is is just\n",
            "a little bit of coding and you don't want to like repeat this for all of your experiments. So what you can do of course is\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:21:00---->00:21:45]:\n",
            "with just little helper\n",
            "function that I created here, and I called it \"Create Airport Graph.\" and it does all of the stuff\n",
            "that I just showed you, and encapsulated away in a simple function with some simple configurations of it. It's just very simple python and I won't go into the details, but it basically does the\n",
            "same that I just showed you, with a few more convenience things. So this means that once\n",
            "I execute this cell, I have this function available\n",
            "in my Jupyter notebooks. And now displaying a graph from\n",
            "my database is really easy. I just need to call one\n",
            "function and passing the graph. So again, I'm running a\n",
            "cypher query this time\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:21:45---->00:22:30]:\n",
            "there's airport cities, region, countries and continents in there. And when I run this function, I get this result and now you can see that it's looking much nicer than before because the nodes have different shapes depending on the type\n",
            "we see the right labels, we see different colors and so on. So okay, nice, but still not super\n",
            "useful. You might think. However you can start to see that with the hierarchic layout for example, with the the right layout, it's getting much easier to understand the structure of your data.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:22:30---->00:23:15]:\n",
            "Now you see that in your\n",
            "database there's always this, or at least in the query, there's always these airport\n",
            "nodes connecting to cities, connected to countries and to continent. So that's a nice representation of our data that we got here. But now let's add graph\n",
            "data science to this and make this really powerful. So because since this is in a notebook, I can actually use third party packages and APIs like matplotlib, or pandas, to add more features to this. So this is a little bit of programming. So what I'm doing here is\n",
            "it's not that much code. And what I'm doing here is\n",
            "using the graph data science\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:23:15---->00:24:00]:\n",
            "module to actually first\n",
            "calculate some graph data science, in this case the (indistinct) algorithm to calculate some scores\n",
            "for all the airports because I want to, suppose I want to compare\n",
            "the various airports in my database and see how they compare with regards to the\n",
            "(indistinct) algorithm. So what I do here, and I won't go too much into details, I'm using the graph data science module to project a sub-graph which only consists of the airport nodes and the\n",
            "\"has route\" relationships because that's the network of how the different pairs\n",
            "of airports are connected, whether there's a flight\n",
            "available between them and the has route\n",
            "relationships in our database\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:24:00---->00:24:45]:\n",
            "connects all the database of the airports. And with the page rank, I can get a measure of how important or how central a certain airport will be. So calling the GDS library, I get an in-memory result\n",
            "that I can work with in my Jupyter Notebook cell. And since this is actually a pandas table, I can use the pandas API to, for example, filter the table and\n",
            "only include those nodes that are above a certain score. So I don't get to see the\n",
            "super unimportant airports\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:24:45---->00:25:30]:\n",
            "according to my measurement for example. So what I'm doing is\n",
            "I'm filtering the table and then for display I'm again executing a very simple cypher query. So I'm just loading the nodes that I got from the\n",
            "results from the graph data science algorithm. So these, this will just load the nodes with a score larger than\n",
            "one point greater than 1.5. So we are loading this\n",
            "and displaying this graph. Then I'm again setting\n",
            "the labels over here, but also I'm setting a custom scaling which actually uses the\n",
            "ranks that we just calculated\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:25:30---->00:26:15]:\n",
            "to scale the nodes\n",
            "according to their ranks. So, and I can use a custom function here. So any function that suits my use case, I can just code here because\n",
            "this is all just python codes, simple python code. So I'm, I'm using a square root function. So not to scale the nodes linearly with respect to their scores because that would lead to super huge nodes in the visualization. So, I thought that using\n",
            "square root works well and what I'm also doing is\n",
            "I'm using the rank information to color map the nodes. So I'm using third party library, this is from matplotlib\n",
            "to get a color map, which basically gives me a nice\n",
            "gradient of different colors and I'm using,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:26:15---->00:27:00]:\n",
            "I'm mapping the range of the values that I have in my data\n",
            "set, in my results set, to map them to these colors. Last but not least, you may\n",
            "have seen that in the data set, the airports actually have\n",
            "some interesting properties called \"lon\" and \"lat\" which is actually longitude and latitude. So these are geo coordinates. So I am using another mapping\n",
            "that I can specify here, which is the node position mapping, which allows me for each node, to specify where on the\n",
            "screen I want it to be placed. So why not use the geo\n",
            "coordinates for that? So I'm just using this super\n",
            "scientific formula over here\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:27:00---->00:27:45]:\n",
            "that I came up with multiplying\n",
            "the longitude value by 150 and the latitude by minus 200. And if I run this, we get to see this, DNS hiccup, but there's the results. So, if you look at this, you will probably recognize that over here there's actually Europe,\n",
            "this is the world map, there's Asia over here, somewhere here should be Australia, south America and the\n",
            "US. And if you zoom in, we see that there's\n",
            "these airports over here. For example, this is\n",
            "Paris Charles du Gaulle, important airport, which we see because it has this light\n",
            "green and the larger size according to our measurements.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:27:45---->00:28:30]:\n",
            "So that's some nice combination\n",
            "of graph data science, visualization and geocoding. But it's not really a graphy example. So as a last example, we are going to use graph data science to do a really graphy tasks and that is calculating a shortest path. So we'll be using the\n",
            "shortest path algorithm from the graph data science library to calculate the shortest path between a set of airports. And for this, again, I'm\n",
            "creating a projection for the graph data science library, which contains the airports and has route relationships only. And we're including the distance property,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:28:30---->00:29:15]:\n",
            "which is available in our data set. So there's someone calculated the distances\n",
            "between all those airports, the flight distances, and we used these distances to calculate the shortest path\n",
            "between one pair of airports. And I'm using this utility function from the GDS library\n",
            "to load these airports. In this case it's Stuttgart and ART calling the shortest path method. And what this results in\n",
            "is just a set of nodes, just a set of airport nodes. So because the draft data science library only ran the query the . . .\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:29:15---->00:30:00]:\n",
            "the algorithm on the airport nodes and it doesn't have the information about the other nodes\n",
            "that we have in there like country location, city,\n",
            "country, continent and so on. So what I'm doing next is\n",
            "with those results, again, I'm doing another query to actually load a sub-graph\n",
            "from the full database. And this contains both the airports that we've\n",
            "got from our result nodes as well as the connecting\n",
            "route between them. But for each of the\n",
            "airports we also include the chain of the city, the region, the country and the continent for both the source and the target node. So this will result in a visualization\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:30:00---->00:30:45]:\n",
            "that contains the shortest\n",
            "path between SDR and ART and all the intermediate path\n",
            "elements if there are any. And for each of the\n",
            "intermediate path elements, there's also information about,\n",
            "well, the location of those. Then I'm using the\n",
            "function that I used before and that's the only part where actually our widget comes into play. So this is all unrelated to the widget and if I execute this, I\n",
            "get to see this result. Let's take a look at this. The hierarchic layout is probably best because what we see here is this is our shortest path that's from Stuttgart Airport,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:30:45---->00:31:30]:\n",
            "going to London, Heathrow,\n",
            "Philadelphia, and ART, that's Watertown International Airport. And we also see the distances\n",
            "between those elements and for each of those elements we see, what city, region and country they are in. So we see that Stuttgart\n",
            "and London is both in on the continent, Europe, and the other two airports are in the US and the US is according to this data, is actually North American. And I believe it says it's also Africa, which is probably not correct, but that's a problem with the data and not something that we\n",
            "will have to work with today. It also says that London is\n",
            "both in England and Canada,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:31:30---->00:32:15]:\n",
            "so that's likely an issue\n",
            "that we have in our data. But yeah, this is nice. So this is makes for nice\n",
            "visualization of our results, but we can do more. And as a last step, I'm going to show you an integration that we just added to this plugin and that is an integration with our suite of Data\n",
            "Explorer applications. So with all of these results, I can just open them in\n",
            "an external application that we have hosted on our website, which lets me visualize and explore the\n",
            "graph in a different way. And this is actually outside the scope of the Jupyter lab extension.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:32:15---->00:33:00]:\n",
            "So this could also be used\n",
            "with different data sources and we have the same one for Neo4j. So, but this is different, so it's not going to the database, it's just allowing you to\n",
            "explore the schema and the data in from the query that you\n",
            "had in your Neo4j notebook. So I can just like, iteratively load the other\n",
            "elements in here and explore them just as you may know way\n",
            "from the Neo4j browser. And I can also, in this case\n",
            "just reveal the entire graph. And now what's the difference here? I can get much more sophisticated with regards to visualization.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:33:00---->00:33:45]:\n",
            "So there's options to\n",
            "style those elements and, it took me some time to come\n",
            "up with a nicer styling, so I prepared this one, so\n",
            "let me just load them in. And now we have like\n",
            "super cool visualization that's only possible within\n",
            "the data explorer here where you have like, there's icons in there and\n",
            "there's additional code in there. So that's much more\n",
            "sophisticated than what we, we've seen before, but it's also more helpful\n",
            "and more end user friendly so to say. And what's also unique to this application is the last thing and that is that, if we look at the schema,\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:33:45---->00:34:30]:\n",
            "we actually have the\n",
            "geo coordinates in here, so wouldn't it be cool to see those nodes actually on a map view? And this is what what we can do in here. I mark this property to\n",
            "be used as map coordinates and now if I go back to the explorer and hit the map layout, I get finally get to see my route, which goes from Stuttgart,\n",
            "via London, Heathrow, over to Philadelphia and Watertown. Okay, so much for this application. Let's see the summary. We arrived at the summary\n",
            "slide hopefully in time. I hope I was able to show you\n",
            "that creating visualizations\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:34:30---->00:35:15]:\n",
            "from your Neo4j data with\n",
            "the help of Jupyter Notebooks is easy and powerful. If you are a data scientist\n",
            "working with Jupyter Notebooks, you can use the yFiles\n",
            "graph for Jupyter Plugin as an entry point to render\n",
            "your Neo4j cypher query results. And you can combine the powerful tools and scripting capabilities that you get from Jupyter Notebooks to create bespoke helpful visualizations, run graph data science algorithms and interactively explore your graphs both directly in the notebook as well as conveniently in\n",
            "the generic Data Explorer app. That's it for now. We would love to hear your\n",
            "feedback about our tools. It's your input that helps\n",
            "us continuously improve them. So try the tools, share\n",
            "them with the community, leave us a star or a\n",
            "like in the social media\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:35:15---->00:36:00]:\n",
            "or of course contact us directly. I'll also hang around in the chat and I'm happy to answer\n",
            "your questions there. As always, happy diagram with your Neo4j databases and the visualization tools by Webworks. Thanks and everyone enjoy the\n",
            "conference. Back to you Alex. - That was great. People seem to like it as\n",
            "well. I see lots of clapping, so that's always good encouraging because, you know, virtual conference so we don't hear your clapping. We you need to, you\n",
            "need to emoji the claps. (Alex laughing) - Thank you so much. - We have, we are on a\n",
            "little bit over time, so we don't have much time for question, but I think we can squeeze\n",
            "in one comes from Michael, \"Does it work in Google CoLab?\"\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:36:00---->00:36:45]:\n",
            "Because you said there's some\n",
            "restrictions on the usage. - At this very moment it doesn't, but this should be actually doable and this is actually very\n",
            "high up in our roadmap, so I hope we are going to\n",
            "see Google CoLab support in one of the next versions. Probably if all goes well in q1, meaning, yeah, well early in 2023. So yeah, that's, it's up on our roadmap, but at the moment it doesn't\n",
            "so really Jupyter Lab or Jupyter notebooks. Sorry for that. - No problem. And then from Jason, maybe we squeeze this in as well, \"Will you share the, your notebook file with your presentation or\n",
            "make it public somewhere?\" So people can- - Yeah, I can do that.\n",
            "\n",
            "Video Extract : 065 Explore Your Graphs Visually With Jupyter Notebooks - NODES2022 - Sebastian Mller\n",
            "[00:36:45---->End of Video]:\n",
            "There's already a simple example which I actually based my slides upon is already on the GitHub\n",
            "repository of the plugin. So there's a generic Neo4j\n",
            "example and the GDS example. So, but I'm going to share\n",
            "this specific one too soon. - Okay, perfect. Well done. Thank you very much Sebastian, thank you for the great\n",
            "presentation and yeah, see you soon on another Nodes\n",
            "Conference or Neo4j event. (Alex laughing) - Thank you, have a great day. - Thank you.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(processed_document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nodes:[Node(id='Sebastian Mller', type='Person'), Node(id='Yworks', type='Organization'), Node(id='Neo4J', type='Database'), Node(id='Jupyter Notebooks', type='Tool'), Node(id='Cypher Query', type='Query language')]\n",
            "Relationships:[Relationship(source=Node(id='Sebastian Mller', type='Person'), target=Node(id='Yworks', type='Organization'), type='WORKS_AT'), Relationship(source=Node(id='Sebastian Mller', type='Person'), target=Node(id='Neo4J', type='Database'), type='SPECIALIZES_IN'), Relationship(source=Node(id='Sebastian Mller', type='Person'), target=Node(id='Jupyter Notebooks', type='Tool'), type='UTILIZES'), Relationship(source=Node(id='Jupyter Notebooks', type='Tool'), target=Node(id='Neo4J', type='Database'), type='CONNECTS_TO'), Relationship(source=Node(id='Cypher Query', type='Query language'), target=Node(id='Neo4J', type='Database'), type='USED_WITH')]\n"
          ]
        }
      ],
      "source": [
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
        "print(f\"Relationships:{graph_documents[0].relationships}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id='Sebastian Mller' type='Person'\n",
            "id='Yworks' type='Organization'\n",
            "id='Neo4J' type='Database'\n",
            "id='Jupyter Notebooks' type='Tool'\n",
            "id='Cypher Query' type='Query language'\n"
          ]
        }
      ],
      "source": [
        "for node in graph_documents[0].nodes:\n",
        "  print(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source=Node(id='Jupyter Graph Widget', type='Feature') target=Node(id='Neo4J Database', type='Database') type='VIEW_PROPERTIES'\n",
            "source=Node(id='Jupyter Graph Widget', type='Feature') target=Node(id='Api', type='Feature') type='CUSTOMIZE_VISUALIZATION'\n",
            "source=Node(id='Jupyter Graph Widget', type='Feature') target=Node(id='Custom Mapping Function', type='Function') type='USE_FOR_CUSTOM_MAPPING'\n",
            "source=Node(id='Custom Mapping Function', type='Function') target=Node(id='Neo4J Database', type='Database') type='BASED_ON_RELATIONSHIP_TYPES'\n",
            "source=Node(id='Dynamic Label Mapping', type='Function') target=Node(id='Neo4J Database', type='Database') type='QUERY_PROPERTIES'\n",
            "source=Node(id='Dynamic Label Mapping', type='Function') target=Node(id='Neo4J Database', type='Database') type='COMBINE_PROPERTIES'\n",
            "source=Node(id='Jupyter Graph Widget', type='Feature') target=Node(id='Create Airport Graph', type='Function') type='STREAMLINE_GRAPH_CREATION'\n",
            "source=Node(id='Create Airport Graph', type='Function') target=Node(id='Api', type='Feature') type='ENHANCE_VISUALS'\n",
            "source=Node(id='Create Airport Graph', type='Function') target=Node(id='Custom Mapping Function', type='Function') type='ENHANCE_VISUALS'\n",
            "source=Node(id='Create Airport Graph', type='Function') target=Node(id='Dynamic Label Mapping', type='Function') type='ENHANCE_VISUALS'\n",
            "source=Node(id='Create Airport Graph', type='Function') target=Node(id='Hierarchic Layout', type='Feature') type='IMPROVE_VISUALIZATION'\n"
          ]
        }
      ],
      "source": [
        "for node in graph_documents[2].relationships:\n",
        "  print(node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a ChromaDB for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=1000,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "chunks = summary_splitter.split_text(summary_content)\n",
        "\n",
        "# Define metadata for summary content\n",
        "metadata={'video_title':video_title,\n",
        "          'doc_type':'summary',\n",
        "          'video_url':video_url,\n",
        "          'video_author':video_author}\n",
        "\n",
        "summary_chunk_documents= summary_splitter.create_documents(chunks,[metadata for chunk in chunks])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conclusion:\n",
            "The video provides a comprehensive overview of graph visualization using Jupyter Notebooks, highlighting the simplicity and accessibility of the process. By leveraging tools like yFiles graphs, data explorer apps, and the Jupyter Notebook plugin, users can explore and analyze graph data effectively. The integration of Neo4j databases with Jupyter Notebooks enhances the capabilities of data scientists to create custom visualizations, run graph data science algorithms, and interactively explore connected data. The discussion on customizing visual elements, leveraging third-party packages, and utilizing graph data science algorithms further enhances the analysis and visualization of graph data, providing users with advanced tools for in-depth exploration and understanding. The video presents new insights into graph visualization techniques and tools that can benefit professionals working with complex data structures.\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# load it into Chroma\n",
        "db = Chroma.from_documents(transcript_chunk_documents+summary_chunk_documents, embeddings)\n",
        "\n",
        "# query it\n",
        "query = \"What is the video about\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "# print results\n",
        "print(docs[0].page_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19. Exploring geographical data visualization and external applications (00:30:45---->00:31:30):\n",
            "The video showcases the visualization of airport data with geographical information, including city, region, and country details. Despite potential data inaccuracies, the visualization provides insights into the locations of airports and their connections.\n",
            "\n",
            "20. Integration with external Data Explorer applications (00:31:30---->00:32:15):\n",
            "Sebastian Mller introduces an integration with external Data Explorer applications for visualizing and exploring graph data in a different way. This integration allows for more sophisticated visualization options and exploration of data schema outside the scope of Jupyter Lab extension.\n",
            "\n",
            "21. Advanced visualization options and map layout features (00:32:15---->00:33:45):\n",
            "The integration with Data Explorer applications offers advanced styling options for elements, including icons and additional code, enhancing the end-user experience. The application also allows for the visualization of nodes on a map view by utilizing geo coordinates, providing a unique and interactive way to explore data sets.\n",
            "\n",
            "22. Geo-coordinate mapping for visualizing nodes on a map view (00:33:45---->00:34:30):\n",
            "Sebastian Mller demonstrates the use of geo coordinates to visualize nodes on a map view, enhancing the representation of routes between airports.\n",
            "\n",
            "23. Summary and feedback request (00:34:30---->00:36:00):\n",
            "Sebastian Mller summarizes the ease and power of creating visualizations from Neo4j data in Jupyter Notebooks, highlighting the use of the yFiles graph for Jupyter Plugin. He encourages feedback on the tools presented and expresses a willingness to improve them based on user input.\n"
          ]
        }
      ],
      "source": [
        "print(docs[2].page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19. Exploring geographical data visualization and external applications (00:30:45---->00:31:30):\n",
            "The video showcases the visualization of airport data with geographical information, including city, region, and country details. Despite potential data inaccuracies, the visualization provides insights into the locations of airports and their connections.\n",
            "\n",
            "20. Integration with external Data Explorer applications (00:31:30---->00:32:15):\n",
            "Sebastian Mller introduces an integration with external Data Explorer applications for visualizing and exploring graph data in a different way. This integration allows for more sophisticated visualization options and exploration of data schema outside the scope of Jupyter Lab extension.\n",
            "\n",
            "21. Advanced visualization options and map layout features (00:32:15---->00:33:45):\n",
            "The integration with Data Explorer applications offers advanced styling options for elements, including icons and additional code, enhancing the end-user experience. The application also allows for the visualization of nodes on a map view by utilizing geo coordinates, providing a unique and interactive way to explore data sets.\n",
            "\n",
            "22. Geo-coordinate mapping for visualizing nodes on a map view (00:33:45---->00:34:30):\n",
            "Sebastian Mller demonstrates the use of geo coordinates to visualize nodes on a map view, enhancing the representation of routes between airports.\n",
            "\n",
            "23. Summary and feedback request (00:34:30---->00:36:00):\n",
            "Sebastian Mller summarizes the ease and power of creating visualizations from Neo4j data in Jupyter Notebooks, highlighting the use of the yFiles graph for Jupyter Plugin. He encourages feedback on the tools presented and expresses a willingness to improve them based on user input.\n"
          ]
        }
      ],
      "source": [
        "print(docs[3].page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Conversational Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\castr\\.conda\\envs\\yt-video-analysis\\lib\\site-packages (from langchainhub) (24.1)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\castr\\.conda\\envs\\yt-video-analysis\\lib\\site-packages (from langchainhub) (2.32.3)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\castr\\.conda\\envs\\yt-video-analysis\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\castr\\.conda\\envs\\yt-video-analysis\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\castr\\.conda\\envs\\yt-video-analysis\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\castr\\.conda\\envs\\yt-video-analysis\\lib\\site-packages (from requests<3,>=2->langchainhub) (2024.7.4)\n",
            "Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
            "Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.20 types-requests-2.32.0.20240712\n"
          ]
        }
      ],
      "source": [
        "!pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm, retrieval_qa_chat_prompt\n",
        ")\n",
        "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "\n",
        "question = \"What is youtube video about?\"\n",
        "ai_msg_1 = retrieval_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
        "print(ai_msg_1[\"answer\"])\n",
        "\n",
        "second_question = \"what specific technologies are being discussed and how do we use them?\"\n",
        "ai_msg_2 = retrieval_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "print(ai_msg_2[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The video delves into automating visualization tasks through literate programming in Jupyter Notebooks, enabling users to create custom visualizations for better data comprehension in graph databases. It also covers advanced features like dynamic color mapping, customizing labels for nodes, and changing the geometry of elements to enhance the visual representation of graph data. Additionally, the speaker plans to share specific notebook files on GitHub for public access and mentions potential Google CoLab support in upcoming versions by early 2023, aiming to expand accessibility and functionality for users.\n"
          ]
        }
      ],
      "source": [
        "chat_history = []\n",
        "\n",
        "question = \"What is youtube video about?\"\n",
        "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
        "\n",
        "second_question = \"tell me more?\"\n",
        "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The YouTube video is about graph visualization using Jupyter Notebooks, showcasing tools like yFiles graphs, data explorer apps, and the Jupyter Notebook plugin for interactive exploration and analysis of graph data. It also discusses integrating Neo4j databases with Jupyter Notebooks to enhance data visualization capabilities and run graph data science algorithms effectively. The video provides insights into customizing visual elements, leveraging third-party packages, and utilizing graph data science algorithms for in-depth exploration and understanding of complex data structures.\n"
          ]
        }
      ],
      "source": [
        "print(ai_msg_1[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The video delves into automating visualization tasks through literate programming in Jupyter Notebooks, enabling users to create custom visualizations for better data comprehension in graph databases. It also covers advanced features like dynamic color mapping, customizing labels for nodes, and changing the geometry of elements to enhance the visual representation of graph data. Additionally, the speaker plans to share specific notebook files on GitHub for public access and mentions potential Google CoLab support in upcoming versions by early 2023, aiming to expand accessibility and functionality for users.\n"
          ]
        }
      ],
      "source": [
        "print(ai_msg_2[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHfTUSc_DVoO",
        "outputId": "2d5150f4-6b1a-4a1a-8859-675a415811e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbconvert 7.16.4 requires beautifulsoup4, which is not installed.\n",
            "nbconvert 7.16.4 requires bleach!=5.0.0, which is not installed.\n",
            "nbconvert 7.16.4 requires defusedxml, which is not installed.\n",
            "nbconvert 7.16.4 requires jupyterlab-pygments, which is not installed.\n",
            "nbconvert 7.16.4 requires nbclient>=0.5.0, which is not installed.\n",
            "nbconvert 7.16.4 requires pandocfilters>=1.4.1, which is not installed.\n",
            "tensorboard 2.10.1 requires absl-py>=0.4, which is not installed.\n",
            "tensorboard 2.10.1 requires google-auth-oauthlib<0.5,>=0.4.1, which is not installed.\n",
            "tensorboard 2.10.1 requires markdown>=2.6.8, which is not installed.\n",
            "tensorboard 2.10.1 requires tensorboard-data-server<0.7.0,>=0.6.0, which is not installed.\n",
            "tensorboard 2.10.1 requires tensorboard-plugin-wit>=1.6.0, which is not installed.\n",
            "tensorboard 2.10.1 requires werkzeug>=1.0.1, which is not installed.\n",
            "tensorflow 2.10.1 requires absl-py>=1.0.0, which is not installed.\n",
            "tensorflow 2.10.1 requires astunparse>=1.6.0, which is not installed.\n",
            "tensorflow 2.10.1 requires gast<=0.4.0,>=0.2.1, which is not installed.\n",
            "tensorflow 2.10.1 requires google-pasta>=0.1.1, which is not installed.\n",
            "tensorflow 2.10.1 requires h5py>=2.9.0, which is not installed.\n",
            "tensorflow 2.10.1 requires keras<2.11,>=2.10.0, which is not installed.\n",
            "tensorflow 2.10.1 requires keras-preprocessing>=1.1.1, which is not installed.\n",
            "tensorflow 2.10.1 requires libclang>=13.0.0, which is not installed.\n",
            "tensorflow 2.10.1 requires opt-einsum>=2.3.2, which is not installed.\n",
            "tensorflow 2.10.1 requires tensorflow-estimator<2.11,>=2.10.0, which is not installed.\n",
            "tensorflow 2.10.1 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\n",
            "tensorflow 2.10.1 requires termcolor>=1.1.0, which is not installed.\n",
            "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\n",
            "tensorflow 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "   ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
            "   ------- -------------------------------- 10.2/57.6 kB ? eta -:--:--\n",
            "   -------------- ------------------------- 20.5/57.6 kB 217.9 kB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 51.2/57.6 kB 372.4 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 57.6/57.6 kB 378.8 kB/s eta 0:00:00\n",
            "Installing collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install langchain\n",
        "!pip install --upgrade --quiet  langchain-openai langchain_community tiktoken chromadb langchain\n",
        "!pip install --upgrade --quiet  youtube-transcript-api\n",
        "!pip install pytube\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aExh9hAREa-h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_community.document_loaders.youtube import TranscriptFormat\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0JGV4MtM5Un"
      },
      "source": [
        "# Transcript Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "T7Z_8X9sDn9N"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"[00:00.000 --> 00:03.900]  2024 is when I started to see a couple of examples of startups,\n",
        "[00:03.900 --> 00:08.680]  especially from the recent YC badge, starting to pivot into web scraping.\n",
        "[00:08.680 --> 00:11.060]  And I'm just here to try to connect the dots here as well.\n",
        "[00:11.060 --> 00:13.140]  This probably has something to do with perplexity and\n",
        "[00:13.140 --> 00:16.220]  the amount of interest in terms of we want to scrape the web so\n",
        "[00:16.220 --> 00:21.220]  we can have the best, I guess, up-to-date answer for your LMs or\n",
        "[00:21.220 --> 00:24.980]  best up-to-date search for a platform.\n",
        "[00:26.260 --> 00:27.580]  Mendable is an example.\n",
        "[00:27.600 --> 00:31.380]  So early days, if you ever were on Lang chain or\n",
        "[00:31.380 --> 00:37.120]  Lama index documentation site, you might see a little robot icon in the corner.\n",
        "[00:37.120 --> 00:40.760]  You click on it, you can do a natural language search query off of\n",
        "[00:40.760 --> 00:41.800]  their documentation site.\n",
        "[00:42.960 --> 00:45.280]  But they came out with this thing recently called Firecrawl,\n",
        "[00:45.280 --> 00:50.560]  which is specifically for scraping the web using large language models.\n",
        "[00:50.560 --> 00:53.580]  And we'll see a live example in a quick second.\n",
        "[00:53.580 --> 00:56.840]  Gina AI, really cool company.\n",
        "[00:56.840 --> 01:00.540]  Their policy is actually, they have embedding models, and\n",
        "[01:00.540 --> 01:02.300]  I think no language models.\n",
        "[01:02.300 --> 01:05.660]  But their embedding models, you can try them without having an API key.\n",
        "[01:05.660 --> 01:07.940]  And I don't know who's footing the bill for this, but\n",
        "[01:07.940 --> 01:11.820]  they keep coming out with really cool, free tools that you can try.\n",
        "[01:11.820 --> 01:17.900]  One of them is reader API, which is, all you gotta do is append\n",
        "[01:17.900 --> 01:22.940]  gnetai.com before any URL, and you're gonna get back some clean data from\n",
        "[01:22.940 --> 01:24.260]  that website.\n",
        "[01:24.260 --> 01:25.300]  It's almost mind-blowing.\n",
        "[01:25.300 --> 01:32.320]  And last, I'm gonna show you this open source project called Scrape Graph AI.\n",
        "[01:32.320 --> 01:36.800]  So this is a very elaborate orchestration of different Python\n",
        "[01:36.800 --> 01:38.480]  modules that create graphs.\n",
        "[01:38.480 --> 01:43.200]  So you can create a pipeline to scrape the web using large language models.\n",
        "[01:43.200 --> 01:48.560]  So these two, it only give you back clean inputs, but\n",
        "[01:48.560 --> 01:53.400]  this one actually incorporate AI in and answer your question at the very end.\n",
        "[01:53.440 --> 01:57.000]  Or you can have ten different steps of what to do when you go to a website and\n",
        "[01:57.000 --> 01:57.880]  scrape it.\n",
        "[01:57.880 --> 02:01.160]  So what I'm gonna do today is I'm actually gonna be scraping my competitor's\n",
        "[02:01.160 --> 02:02.560]  pricing pages.\n",
        "[02:02.560 --> 02:05.720]  I'm doing this for myself, I'm building a product right now.\n",
        "[02:05.720 --> 02:08.040]  And this matters to me.\n",
        "[02:08.040 --> 02:10.560]  This is not a made up use case.\n",
        "[02:10.560 --> 02:14.360]  I'm actually looking forward to see what's gonna come out of this.\n",
        "[02:14.360 --> 02:18.640]  So I'm building in the learning and development space.\n",
        "[02:18.680 --> 02:24.440]  So my competitors are obviously popular tools like Articulate 360,\n",
        "[02:24.440 --> 02:30.040]  some challengers that are new to the market like 7Taps, Mindsmith,\n",
        "[02:30.040 --> 02:30.960]  some other companies.\n",
        "[02:30.960 --> 02:34.080]  But I have about four websites here today, and\n",
        "[02:34.080 --> 02:36.880]  I'm keeping them all in a nice array.\n",
        "[02:37.920 --> 02:41.880]  So what I'm gonna do is I'm just gonna run this to save it in memory.\n",
        "[02:43.560 --> 02:47.120]  And the cool thing about this is that once I give you this, you can go home and\n",
        "[02:47.120 --> 02:50.640]  play around and try to do some market research yourself.\n",
        "[02:50.640 --> 02:58.040]  And now I'm also gonna set up this thing called TickToken.\n",
        "[02:58.040 --> 02:59.800]  Do you guys know what TickToken is?\n",
        "[02:59.800 --> 03:02.480]  I know you know what TickToken is, but I wanna let everyone else know what\n",
        "[03:02.480 --> 03:03.120]  TickToken is.\n",
        "[03:05.280 --> 03:09.800]  Okay, so for a large language model when it's being encoded,\n",
        "[03:09.800 --> 03:12.920]  maybe you can explain this better than me, cuz I'm a software engineer.\n",
        "[03:12.920 --> 03:15.280]  So and it depends on the model too, right?\n",
        "[03:15.360 --> 03:19.600]  Dependent on what kind of tokenization or encoding mechanism that you use.\n",
        "[03:19.600 --> 03:26.400]  For example, GPT-3 had a different encoding scheme than GPT-4 or GPT-4-0.\n",
        "[03:26.400 --> 03:29.240]  That's why they were able to reduce the cost for\n",
        "[03:29.240 --> 03:32.400]  newer generations a little bit just because of the way that tokenization works.\n",
        "[03:33.440 --> 03:35.960]  And you get billed by the number of tokens.\n",
        "[03:35.960 --> 03:37.280]  So less tokens, but\n",
        "[03:37.280 --> 03:41.400]  the same sentence is usually cheaper for you as a consumer.\n",
        "[03:41.400 --> 03:44.560]  So we're using TickToken here, which is straight from OpenAI.\n",
        "[03:44.560 --> 03:49.960]  This is the exact library that OpenAI uses to encode their GPT models.\n",
        "[03:49.960 --> 03:53.680]  We're using this not to create a new model, but we're using this to\n",
        "[03:53.680 --> 03:58.680]  count the number of tokens that we're getting based on the scraped content\n",
        "[03:58.680 --> 04:00.080]  of this website.\n",
        "[04:00.080 --> 04:04.560]  So I wanna know how expensive it is for me to scrape all these things.\n",
        "[04:04.560 --> 04:06.160]  I wanna convert that to a dollar amount.\n",
        "[04:07.520 --> 04:11.880]  Comparing Beautiful Soup and Gina AI and Mendebol.\n",
        "[04:11.880 --> 04:14.680]  See which one will save me the most amount of money.\n",
        "[04:14.680 --> 04:18.880]  So I ran this and this is just an example sentence.\n",
        "[04:18.880 --> 04:22.000]  What's the difference between a Beer Nuts and Deer Nuts?\n",
        "[04:22.000 --> 04:26.400]  Beer Nuts are about $5, Deer Nuts are just under a buck.\n",
        "[04:26.400 --> 04:27.760]  So, anybody get a joke?\n",
        "[04:29.720 --> 04:35.520]  So this one cost about 0.000135 if you're using GPT-4-0.\n",
        "[04:35.520 --> 04:39.320]  And the funny thing about OpenAI is that they didn't update TickToken for\n",
        "[04:39.320 --> 04:40.800]  GPT-4-0 yet.\n",
        "[04:40.800 --> 04:42.360]  So this is just a guesstimate.\n",
        "[04:42.360 --> 04:46.840]  So last thing that I wanna set up is a pretty table.\n",
        "[04:46.840 --> 04:49.200]  So pretty table is just library in Python.\n",
        "[04:49.200 --> 04:51.960]  So you can make tables in your terminal.\n",
        "[04:51.960 --> 04:57.240]  I want rows and columns so I can see which one is cost more than which.\n",
        "[04:58.440 --> 05:01.160]  So that's all I'm doing here, very long function.\n",
        "[05:01.160 --> 05:04.920]  All it's doing here is take into account scraped content,\n",
        "[05:04.920 --> 05:07.760]  put them all in the right columns and rows.\n",
        "[05:07.760 --> 05:08.920]  You can read this at home.\n",
        "[05:09.680 --> 05:13.040]  But yeah, all right, so now let's set up the scrapers.\n",
        "[05:13.040 --> 05:18.120]  First we're gonna install a good old friend, BeautifulSoup4.\n",
        "[05:20.360 --> 05:23.920]  This is the most straightforward way to scrape any website.\n",
        "[05:23.920 --> 05:28.400]  And probably the easiest way for them to detect that you're scraping and\n",
        "[05:28.400 --> 05:29.440]  ban you.\n",
        "[05:29.440 --> 05:34.080]  So it's not very sophisticated unless you bundle on a bunch of other tools.\n",
        "[05:34.080 --> 05:38.360]  Okay, so we got a function to scrape the web with BeautifulSoup.\n",
        "[05:38.400 --> 05:39.960]  We're not gonna use that yet.\n",
        "[05:39.960 --> 05:42.520]  What we're gonna do is we're gonna run all of them at the same time.\n",
        "[05:42.520 --> 05:44.360]  So I'm just gonna set up a bunch of stuff here.\n",
        "[05:44.360 --> 05:46.240]  So here's Gina AI.\n",
        "[05:46.240 --> 05:48.760]  This is what I'm talking about when I say this one is dead simple.\n",
        "[05:50.480 --> 05:54.200]  All you gotta do is just add the string before the actual URL that you wanna\n",
        "[05:54.200 --> 05:55.480]  scrape.\n",
        "[05:55.480 --> 05:56.880]  And it's completely free.\n",
        "[05:56.880 --> 05:59.800]  I don't know what the deal is, but use it, it's free.\n",
        "[06:01.080 --> 06:02.120]  No strings attached.\n",
        "[06:02.120 --> 06:05.960]  I mean, maybe they're trying to do some market research or something.\n",
        "[06:05.960 --> 06:06.880]  But yeah, it's great.\n",
        "[06:06.880 --> 06:08.880]  The jobs of VC money they could-\n",
        "[06:08.880 --> 06:09.480]  Yeah.\n",
        "[06:09.480 --> 06:10.000]  They could blow.\n",
        "[06:11.400 --> 06:12.000]  Absolutely.\n",
        "[06:13.400 --> 06:14.400]  Like free rides and Uber.\n",
        "[06:15.720 --> 06:19.960]  In the early days, you just get free rides over and over.\n",
        "[06:19.960 --> 06:22.560]  All right, last provider is Mendable.\n",
        "[06:22.560 --> 06:24.240]  So Mendable, like I said recently,\n",
        "[06:24.240 --> 06:27.880]  pivoted from, I guess not fully pivoted from, but\n",
        "[06:27.880 --> 06:30.760]  they were doing documentation chatbots.\n",
        "[06:30.760 --> 06:34.120]  So you go on like Langchain, Lama Index, all these companies.\n",
        "[06:34.120 --> 06:36.520]  You can chat with a little chatbot in the bottom.\n",
        "[06:36.520 --> 06:38.080]  So we're gonna try this one too.\n",
        "[06:39.160 --> 06:40.680]  But this one requires an API key.\n",
        "[06:42.760 --> 06:48.440]  And they gave everybody like, I think, 300 tokens for free.\n",
        "[06:48.440 --> 06:52.240]  I'm not sure how many websites you can scrape with that.\n",
        "[06:53.760 --> 06:55.520]  But-\n",
        "[06:55.520 --> 06:56.560]  So from these other tools,\n",
        "[06:56.560 --> 07:01.040]  the output is all text normalized or is it still pretty wrong?\n",
        "[07:01.040 --> 07:03.120]  It's only in markdown.\n",
        "[07:03.120 --> 07:07.880]  So they're all in markdown for some reason.\n",
        "[07:07.880 --> 07:10.280]  Maybe because of large language models.\n",
        "[07:10.280 --> 07:14.640]  Like they made these tools specifically to output markdown from\n",
        "[07:14.640 --> 07:16.120]  a bunch of HTML tags.\n",
        "[07:18.280 --> 07:18.780]  Yeah.\n",
        "[07:21.200 --> 07:22.720]  So it's also like string.\n",
        "[07:22.720 --> 07:24.760]  So there's no like, just pure string.\n",
        "[07:24.760 --> 07:26.040]  You get a string back, like very long.\n",
        "[07:28.440 --> 07:31.160]  Okay, this is the moment that we run everything.\n",
        "[07:33.640 --> 07:34.600]  So I'm just gonna run this.\n",
        "[07:36.040 --> 07:39.440]  All I'm doing here is just go on these websites,\n",
        "[07:39.440 --> 07:42.720]  try to scrape it with all the three different tools that I have.\n",
        "[07:43.720 --> 07:45.080]  It's gonna take about a minute.\n",
        "[07:45.080 --> 07:49.760]  So we can watch this bar going from left to right.\n",
        "[07:49.760 --> 07:50.880]  Are you guys gonna ask questions?\n",
        "[07:50.880 --> 07:52.400]  They got a table back.\n",
        "[07:52.400 --> 07:53.680]  Good thing I got my pretty table so\n",
        "[07:53.680 --> 07:55.720]  I can see the difference between all of them.\n",
        "[07:57.280 --> 07:59.800]  Okay, this is my biggest competitor by the way.\n",
        "[07:59.800 --> 08:01.280]  They have so much money.\n",
        "[08:01.280 --> 08:02.680]  I don't even know what to do with them.\n",
        "[08:03.680 --> 08:04.400]  Beautiful soup.\n",
        "[08:04.400 --> 08:06.600]  You get your regular HTML stuff.\n",
        "[08:08.000 --> 08:09.960]  Very, very clean, but not really.\n",
        "[08:11.480 --> 08:14.200]  Firecrawl, this is firecrawl.\n",
        "[08:14.200 --> 08:19.720]  So for firecrawl you get a little bit better, you know, like very much\n",
        "[08:19.720 --> 08:23.520]  markdown, like you got brackets and you got like links and stuff.\n",
        "[08:23.520 --> 08:27.840]  So you can already see that you can already skip a bunch of stuff here.\n",
        "[08:27.840 --> 08:30.400]  And if your large language model will love you,\n",
        "[08:30.480 --> 08:31.800]  you give it clean data like this.\n",
        "[08:32.840 --> 08:35.480]  But you take a look at Gina on this side.\n",
        "[08:35.480 --> 08:37.840]  Gina is even more human readable.\n",
        "[08:37.840 --> 08:41.240]  Even though they say it's supposed to bring you back markdown,\n",
        "[08:41.240 --> 08:43.720]  they actually took away all the brackets and everything.\n",
        "[08:43.720 --> 08:47.880]  So like you got bad and\n",
        "[08:47.880 --> 08:51.760]  then you got markdown in like actual markdown format.\n",
        "[08:51.760 --> 08:53.800]  And this one is promises you markdown but\n",
        "[08:53.800 --> 08:57.200]  it's actually string in a human readable like format.\n",
        "[08:57.200 --> 09:04.840]  And we can see that for pretty much every single examples here.\n",
        "[09:06.440 --> 09:09.600]  The Gina one is usually the one that you can actually read.\n",
        "[09:09.600 --> 09:14.480]  For example, this one from in the middle by Mendable.\n",
        "[09:14.480 --> 09:16.920]  It's not actually human readable that much.\n",
        "[09:16.920 --> 09:20.920]  Especially if your large language model doesn't care about URLs.\n",
        "[09:20.920 --> 09:23.440]  Maybe you just wanted to know facts.\n",
        "[09:23.440 --> 09:26.800]  Then you might not want all of this stuff in there, right?\n",
        "[09:26.840 --> 09:30.360]  You might want just what's human readable.\n",
        "[09:30.360 --> 09:32.200]  Maybe for a reasoning task, for example.\n",
        "[09:34.280 --> 09:36.200]  Anybody have any questions so far?\n",
        "[09:38.760 --> 09:42.800]  Okay, so that was surprisingly fast.\n",
        "[09:42.800 --> 09:44.880]  All right, so we got also a cost table here.\n",
        "[09:44.880 --> 09:46.800]  Let's take a look at this real quick.\n",
        "[09:48.200 --> 09:50.000]  That's a great question, it reminds me of something.\n",
        "[09:50.000 --> 09:54.040]  I think when I was playing with this, Adobe actually blocked BOS soup.\n",
        "[09:54.040 --> 09:54.560]  So let me show you.\n",
        "[09:57.000 --> 09:59.200]  What I'm gonna do is, I'm gonna just come with these two out.\n",
        "[10:00.240 --> 10:05.240]  And then I wanna show more content and run this again.\n",
        "[10:06.960 --> 10:11.800]  That's a great question, it reminds me of this.\n",
        "[10:11.800 --> 10:12.480]  Okay, let's see.\n",
        "[10:14.240 --> 10:14.840]  Articulate.\n",
        "[10:16.600 --> 10:20.560]  Yeah, so this is like the most barebone, beautiful soup setup.\n",
        "[10:20.560 --> 10:22.920]  Like this is like your intern doing it.\n",
        "[10:22.920 --> 10:25.120]  This is not like very sophisticated.\n",
        "[10:25.120 --> 10:27.440]  So you get four or three.\n",
        "[10:27.440 --> 10:29.600]  That's why there was nothing there.\n",
        "[10:29.600 --> 10:30.920]  So great catch, great catch.\n",
        "[10:30.920 --> 10:32.560]  You can't compare the costs.\n",
        "[10:32.560 --> 10:33.840]  Not enough for the first one.\n",
        "[10:33.840 --> 10:37.400]  The first one was blocked, but actually I'm gonna run this again.\n",
        "[10:38.400 --> 10:43.320]  But the table here, all we're doing is we're comparing the input cost\n",
        "[10:43.320 --> 10:46.280]  between the three, if we were to put this into GPT-4.\n",
        "[10:49.160 --> 10:51.880]  So let's just forget about the first one.\n",
        "[10:52.840 --> 10:59.040]  This one, seven times, seven times less.\n",
        "[11:00.160 --> 11:01.640]  And this one is even way less.\n",
        "[11:03.040 --> 11:07.240]  I compare these two, 100 times, or 70 times.\n",
        "[11:10.000 --> 11:10.720]  Yeah, any questions?\n",
        "[11:10.720 --> 11:15.440]  So right now, these costs are just the cost of just the input tokens and\n",
        "[11:15.440 --> 11:16.520]  output tokens, right?\n",
        "[11:16.520 --> 11:18.200]  Just input tokens.\n",
        "[11:18.200 --> 11:19.440]  For GPT-4.\n",
        "[11:19.440 --> 11:20.520]  Yeah.\n",
        "[11:20.520 --> 11:22.920]  But it has separate cost for input and output, right?\n",
        "[11:22.920 --> 11:25.240]  It does, but this table alone, it's just input.\n",
        "[11:25.240 --> 11:29.280]  Cuz the outputs, I'm gonna show the last step,\n",
        "[11:29.280 --> 11:34.640]  which is using LM to extract our JSON of just the things that I want.\n",
        "[11:34.640 --> 11:38.480]  Which is gonna be pricing tier names and the actual pricing.\n",
        "[11:38.480 --> 11:41.440]  So the output's gonna be the same, regardless.\n",
        "[11:42.560 --> 11:47.040]  But the input, if you use different tools, you get different amount of inputs.\n",
        "[11:47.040 --> 11:54.080]  Yeah, especially this one, for example.\n",
        "[11:54.080 --> 11:54.580]  Yeah.\n",
        "[11:56.080 --> 11:56.840]  It's pretty nuts.\n",
        "[11:56.840 --> 11:59.280]  I guess it's because of the extra tags that are there, right?\n",
        "[11:59.280 --> 12:00.840]  In the fire crawl level.\n",
        "[12:00.840 --> 12:05.880]  Yeah, especially, I think for this one, if you hit an image that has an entire\n",
        "[12:05.880 --> 12:08.720]  binary on it, then you get the entire thing.\n",
        "[12:09.880 --> 12:11.520]  This one also get the entire thing.\n",
        "[12:11.520 --> 12:12.440]  Must have, right?\n",
        "[12:12.440 --> 12:12.960]  Yeah.\n",
        "[12:12.960 --> 12:16.800]  And then, I don't know what they're doing here.\n",
        "[12:16.800 --> 12:18.840]  But they're doing some crazy stuff.\n",
        "[12:20.160 --> 12:22.920]  And this is very clean, by the way.\n",
        "[12:22.920 --> 12:23.840]  It's human readable.\n",
        "[12:25.920 --> 12:29.720]  But again, if you want the actual URLs for\n",
        "[12:29.720 --> 12:32.120]  most things, then you might be better off using this.\n",
        "[12:33.160 --> 12:34.720]  Otherwise, you lose a lot of resolution.\n",
        "[12:36.000 --> 12:40.280]  But again, if you want a reasoning task to be done, then you don't need that.\n",
        "[12:40.280 --> 12:43.320]  You just need factual things.\n",
        "[12:43.320 --> 12:48.080]  All right, so it's pretty obvious which one we should be paying for.\n",
        "[12:49.600 --> 12:51.680]  Not knowing anything else about these companies.\n",
        "[12:51.680 --> 12:58.520]  But now we're going to use OpenAI to kind of like try to do some extraction.\n",
        "[12:58.520 --> 13:00.400]  Because I don't want to look at just the input.\n",
        "[13:00.400 --> 13:05.800]  I want just one JSON with all the data that I was looking for in the beginning.\n",
        "[13:05.800 --> 13:09.480]  Which is, which of these competitors having how many tiers and\n",
        "[13:09.480 --> 13:11.560]  how much does it cost per each tier?\n",
        "[13:11.560 --> 13:12.240]  That's all I want to know.\n",
        "[13:14.040 --> 13:17.960]  Okay, so this part, all I'm doing is setting up an OpenAI client.\n",
        "[13:19.120 --> 13:22.040]  And then I'm going to use my OpenAI key here.\n",
        "[13:23.360 --> 13:25.920]  So we got a fresh key this week.\n",
        "[13:27.920 --> 13:29.400]  Key from last week has been deprecated.\n",
        "[13:31.080 --> 13:34.000]  And I'm using the latest and greatest GPT-4.\n",
        "[13:34.000 --> 13:36.960]  Do you guys know what the O stands for?\n",
        "[13:36.960 --> 13:38.400]  There you go.\n",
        "[13:39.200 --> 13:41.320]  Do you guys know why in the demos,\n",
        "[13:41.320 --> 13:45.400]  like GPT-4, or chat GPT sounds so flirtatious?\n",
        "[13:45.400 --> 13:53.400]  Yeah, there's so many memes about that on Twitter.\n",
        "[13:53.400 --> 13:58.280]  But okay, so this is just a utility function to then display\n",
        "[13:58.280 --> 14:01.160]  the extracted content on another table.\n",
        "[14:02.520 --> 14:07.240]  Because table and console is what we need to compare these things.\n",
        "[14:08.160 --> 14:13.280]  All right, so what I'm doing here is just run GPT-4.0 through each of\n",
        "[14:13.280 --> 14:16.680]  the inputs that we got before, like the entire input.\n",
        "[14:16.680 --> 14:20.320]  So it could be like 50,000 tokens.\n",
        "[14:20.320 --> 14:23.040]  Like it's not my money, it's Invest Ottawa's money.\n",
        "[14:23.040 --> 14:26.760]  So I'm just running it through GPT-4.0 right now.\n",
        "[14:26.760 --> 14:42.680]  Okay, so very, very simple entity extraction task using GPT-4.0.\n",
        "[14:42.680 --> 14:47.760]  All I did was I put in a kind of like a chain,\n",
        "[14:47.760 --> 14:51.880]  an LLM chain, and just be like, do, do, do, do, do, do.\n",
        "[14:51.880 --> 14:56.480]  Give me the three pricing tiers from this website's content.\n",
        "[14:57.160 --> 14:59.120]  So I give one website at a time.\n",
        "[14:59.120 --> 15:01.920]  And then we turn a JSON with three keys.\n",
        "[15:01.920 --> 15:06.400]  Cheapest, which is the cheapest tier, name of it, and then a price.\n",
        "[15:06.400 --> 15:08.440]  And then I just give each one like a type, so\n",
        "[15:08.440 --> 15:10.560]  that it knows what I'm actually looking for.\n",
        "[15:11.600 --> 15:14.800]  And then the middle tier is the most expensive one.\n",
        "[15:14.800 --> 15:17.480]  And that's it, that's my extraction tier.\n",
        "[15:17.480 --> 15:22.280]  And always, what OpenAI does is it tells you that you should always,\n",
        "[15:22.280 --> 15:25.440]  if you want JSON back, use type JSON object, but\n",
        "[15:25.440 --> 15:27.640]  also say in your prompt that you want JSON back.\n",
        "[15:27.640 --> 15:29.680]  So you always need those two things.\n",
        "[15:30.920 --> 15:35.200]  But it's not really surprising that from BeautifulSuit we get nothing, so\n",
        "[15:35.200 --> 15:37.640]  we can't extract anything here.\n",
        "[15:37.640 --> 15:42.160]  Our JSON is completely zero and empty string,\n",
        "[15:42.160 --> 15:47.760]  because we got 403 forbidden, we got forbidden from Adobe Articulate 360.\n",
        "[15:49.240 --> 15:54.040]  Firecrawl seems like it's been able to get personal plan 0199,\n",
        "[15:54.040 --> 16:01.120]  Teams plan 399, and then Reach 360 Pro, which is the enterprise tier.\n",
        "[16:01.120 --> 16:04.240]  And the price is contact sales for pricing.\n",
        "[16:04.240 --> 16:07.960]  This is weird because I gave it a type which was float, and\n",
        "[16:07.960 --> 16:09.640]  it gave me a string here.\n",
        "[16:09.640 --> 16:14.720]  It's still useful, but not technically accurate.\n",
        "[16:16.080 --> 16:20.720]  And it comes out to GNI, price as variable.\n",
        "[16:20.720 --> 16:25.200]  So they're kind of ignoring my instructions a little bit.\n",
        "[16:25.200 --> 16:30.720]  Maybe I should have said float or no if it's not found or something like that.\n",
        "[16:30.720 --> 16:35.000]  It just goes to show if you're working with large language models,\n",
        "[16:35.000 --> 16:37.320]  unless you're working with dspy or something like that,\n",
        "[16:37.320 --> 16:40.240]  you need to be very specific about your prompt.\n",
        "[16:40.240 --> 16:42.480]  And just take care of most edge cases.\n",
        "[16:43.160 --> 16:50.600]  So again, it seems like most of these tools pass the test.\n",
        "[16:52.600 --> 16:54.720]  It's just a matter of whether or\n",
        "[16:54.720 --> 16:57.760]  not you're gonna burn ten times the amount of money and\n",
        "[16:57.760 --> 17:01.480]  do scraping manually using Beelzebub and your custom tools.\n",
        "[17:01.480 --> 17:05.840]  Or you can kind of use one of these third party tools and just get\n",
        "[17:05.840 --> 17:10.080]  clean markdown or clean user human readable text back.\n",
        "[17:10.120 --> 17:14.160]  And just worry about your LLM stack instead of your web scraping.\n",
        "[17:16.160 --> 17:20.080]  One last thing I wanna show you guys, which is scrape graph.\n",
        "[17:21.600 --> 17:23.880]  So this is completely open source.\n",
        "[17:23.880 --> 17:27.320]  This is not like the startups that we just saw earlier.\n",
        "[17:27.320 --> 17:29.080]  This is completely open source and in Python.\n",
        "[17:30.920 --> 17:34.360]  Anybody here familiar with graph data structure?\n",
        "[17:36.000 --> 17:37.640]  I think we had a conversation before about this.\n",
        "[17:37.640 --> 17:41.800]  It seems like you're a big fan of graph data structure too.\n",
        "[17:41.800 --> 17:42.300]  Yeah.\n",
        "[17:44.520 --> 17:47.760]  Okay, so open API key.\n",
        "[17:49.560 --> 17:50.800]  And then link to script.\n",
        "[17:50.800 --> 17:52.400]  Does anyone have a link that they wanna scrape?\n",
        "[17:52.400 --> 17:55.000]  Which model you're gonna use?\n",
        "[17:55.000 --> 17:56.160]  Okay, so this is what the website looks like.\n",
        "[17:58.440 --> 17:59.240]  That's actually pretty cool.\n",
        "[17:59.240 --> 18:02.440]  So what would be a question that you wanna ask?\n",
        "[18:02.440 --> 18:06.320]  What I meant to say is, what do you wanna get out of this website?\n",
        "[18:06.320 --> 18:09.160]  So model and speed graph.\n",
        "[18:09.160 --> 18:09.660]  Yeah.\n",
        "[18:11.160 --> 18:11.800]  What are these called?\n",
        "[18:11.800 --> 18:13.840]  Electric unicycle.\n",
        "[18:13.840 --> 18:14.360]  Okay.\n",
        "[18:15.280 --> 18:15.960]  LA.\n",
        "[18:15.960 --> 18:16.960]  EUC.\n",
        "[18:16.960 --> 18:19.880]  And what color is the fastest model of the universe?\n",
        "[18:19.880 --> 18:24.080]  Electric unicycle and.\n",
        "[18:24.080 --> 18:24.880]  Model and weight.\n",
        "[18:24.880 --> 18:26.960]  Model and name and speed.\n",
        "[18:28.640 --> 18:29.140]  Okay.\n",
        "[18:31.200 --> 18:33.520]  Yeah, initially I thought that you can just ask questions, but\n",
        "[18:34.400 --> 18:38.680]  the prompt here is just what do you wanna get or scrape out of the website.\n",
        "[18:38.680 --> 18:40.600]  Yeah, not like what do you wanna know by the website.\n",
        "[18:42.040 --> 18:45.240]  Okay, so we got our answers back.\n",
        "[18:45.240 --> 18:47.720]  So we got here, how do I get rid of this?\n",
        "[18:49.840 --> 18:52.360]  Okay, so electric unicycle models.\n",
        "[18:53.480 --> 18:55.480]  So you tell me if these models are legit.\n",
        "[18:55.480 --> 18:56.280]  Yeah, yeah.\n",
        "[18:56.280 --> 18:58.920]  I saw the names are correct and speed is correct.\n",
        "[18:58.920 --> 19:00.360]  Which one's yours?\n",
        "[19:00.360 --> 19:01.800]  It's the Gold Master.\n",
        "[19:01.800 --> 19:02.960]  This one.\n",
        "[19:03.480 --> 19:04.480]  50 plus miles per hour.\n",
        "[19:04.480 --> 19:05.280]  Is that true?\n",
        "[19:05.280 --> 19:05.920]  That's true.\n",
        "[19:05.920 --> 19:07.120]  On full battery.\n",
        "[19:07.120 --> 19:07.640]  Okay.\n",
        "[19:07.640 --> 19:08.960]  Are you serious?\n",
        "[19:08.960 --> 19:14.480]  You know, I drove like 72 kilometers per hour, my top speed, and it was scary.\n",
        "[19:16.080 --> 19:20.480]  You see like all the, all everything like flying by you and think,\n",
        "[19:20.480 --> 19:26.400]  if I'm gonna fall and hit something, it's not gonna, my body protection not gonna help.\n",
        "[19:26.400 --> 19:27.000]  No.\n",
        "[19:27.000 --> 19:27.480]  Wow.\n",
        "[19:29.800 --> 19:31.440]  So that's pretty, yeah.\n",
        "[19:31.440 --> 19:32.720]  Jesus.\n",
        "[19:32.840 --> 19:33.880]  So this is accurate.\n",
        "[19:33.880 --> 19:35.360]  That's accurate.\n",
        "[19:35.360 --> 19:37.840]  V13, yeah.\n",
        "[19:37.840 --> 19:39.160]  You know what?\n",
        "[19:39.160 --> 19:44.600]  I actually, I actually went on their project and I asked, cuz I couldn't figure it out.\n",
        "[19:44.600 --> 19:48.120]  Cuz I was trying to build this and I'm like, how many tokens is this consuming?\n",
        "[19:48.120 --> 19:49.360]  Probably a lot.\n",
        "[19:49.360 --> 19:51.080]  So I ask.\n",
        "[19:51.080 --> 19:54.840]  And apparently there's like a function that you can do to get that.\n",
        "[19:57.080 --> 19:58.080]  There's two ways.\n",
        "[19:58.120 --> 20:03.360]  So if you use this library, just go to the discussions tab,\n",
        "[20:03.360 --> 20:05.400]  look at my question and you'll find the answer.\n",
        "[20:07.680 --> 20:09.480]  Yeah, I asked this like last night.\n",
        "[20:09.480 --> 20:10.280]  I'm like, wait, wait.\n",
        "[20:11.440 --> 20:13.760]  Are you trying to like hide this away from us or something?\n",
        "[20:13.760 --> 20:16.520]  Like no, there's two ways to do it, but the documentation doesn't cover it, so.\n",
        "[20:18.520 --> 20:19.800]  But yeah, that's all I got.\n",
        "[20:19.800 --> 20:21.600]  Does anybody have any questions about anything?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7FryjW6HoRXf"
      },
      "outputs": [],
      "source": [
        "text_2 = \"\"\"# tactiq.io free youtube transcript\n",
        "# DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\n",
        "# https://www.youtube.com/watch/Hp7P99rsRpo\n",
        "\n",
        "00:00:00.399 uh\n",
        "00:00:01.360 speaker is lag\n",
        "00:00:03.280 he going to talk about\n",
        "00:00:05.839 the vortex ai which is newly released in\n",
        "00:00:08.720 the i think a few months back in the\n",
        "00:00:11.200 google i o\n",
        "00:00:12.480 events it's\n",
        "00:00:14.080 a\n",
        "00:00:14.799 great platforms to unified many of the\n",
        "00:00:18.480 resources for\n",
        "00:00:20.160 machine learning and ai tasks\n",
        "00:00:22.640 lag he is the director for that\n",
        "00:00:26.080 analytics and ai solutions on google\n",
        "00:00:28.480 cloud and his team builds software\n",
        "00:00:31.279 solutions for business problems using\n",
        "00:00:33.280 google cloud data analytics and machine\n",
        "00:00:35.680 learning products\n",
        "00:00:37.280 he founded the google\n",
        "00:00:38.960 adwords solution lab machine learning\n",
        "00:00:42.079 immersion program and it's also of\n",
        "00:00:44.079 several oiling books and correlate\n",
        "00:00:46.800 courses\n",
        "00:00:48.399 without further ado let's welcome\n",
        "00:00:50.800 lack\n",
        "00:00:52.000 thank you bill thank hello everyone\n",
        "00:00:55.039 so today i'm going to be talking about\n",
        "00:00:57.600 machine learning pipelines specifically\n",
        "00:01:00.760 pre-fabricated machine learning\n",
        "00:01:02.399 pipelines in vertex ai\n",
        "00:01:05.040 so what is vertex ai it's essentially a\n",
        "00:01:08.080 set of tools for predictions for\n",
        "00:01:10.159 training\n",
        "00:01:11.280 for evaluation for monitoring so\n",
        "00:01:14.479 essentially a set of tools that supports\n",
        "00:01:16.479 your entire workflow\n",
        "00:01:18.479 across different model types auto ml\n",
        "00:01:21.200 tensorflow pytorch\n",
        "00:01:23.280 scikit learn right whatever framework\n",
        "00:01:25.200 you're using and different levels of ml\n",
        "00:01:28.080 expertise whether you want to use low\n",
        "00:01:30.479 code whether you want to use no code\n",
        "00:01:32.880 with auto ml or you want to basically\n",
        "00:01:35.600 use low code with bigquery ml or you\n",
        "00:01:38.320 want to basically do sophisticated\n",
        "00:01:41.200 programming with tensorflow or pytorch\n",
        "00:01:43.680 it supports all of the different types\n",
        "00:01:45.680 of ml expertise so what's in vertex ai\n",
        "00:01:49.600 if you think about the machine learning\n",
        "00:01:52.399 workflow on the top data readiness\n",
        "00:01:55.119 getting your data ready labeling it for\n",
        "00:01:57.280 example feature engineering right\n",
        "00:01:59.920 creating a data set training a hyper\n",
        "00:02:02.560 parameter tuning\n",
        "00:02:04.079 then serving the model\n",
        "00:02:06.240 understanding it tuning it deploying it\n",
        "00:02:09.280 to the edge monitoring the model\n",
        "00:02:11.840 managing the model\n",
        "00:02:13.520 like there's like different\n",
        "00:02:15.280 tools that support that full end-to-end\n",
        "00:02:18.319 workflow and again as i said you could\n",
        "00:02:21.360 do it with no code with automl for\n",
        "00:02:23.680 things like vision video language\n",
        "00:02:26.800 translation so when we talk about automl\n",
        "00:02:29.120 what we're basically saying is using the\n",
        "00:02:32.080 state-of-the-art models with uh you know\n",
        "00:02:35.120 you know with that with a like point and\n",
        "00:02:37.680 click interface but at the same time you\n",
        "00:02:40.560 can also build things yourself so you\n",
        "00:02:43.360 can you know build a train now you can\n",
        "00:02:45.360 build things that can be trained hyper\n",
        "00:02:47.040 parameter tune run experiments use gpus\n",
        "00:02:50.959 do predictions explainability all of\n",
        "00:02:53.440 these things\n",
        "00:02:54.800 and as you build an ml pipeline right uh\n",
        "00:02:57.920 you will want to orchestrate it you\n",
        "00:02:59.599 don't want to basically do them one at a\n",
        "00:03:02.080 time you want to connect them together\n",
        "00:03:04.000 and that connection is what a pipeline\n",
        "00:03:05.920 is and all of these are a unified\n",
        "00:03:08.800 workflow so once you create a data set\n",
        "00:03:12.480 you can use that same data set to train\n",
        "00:03:14.879 both an automl model and a custom model\n",
        "00:03:17.920 so you're not\n",
        "00:03:19.200 doing completely separate workflows\n",
        "00:03:22.560 just because you're changing the way\n",
        "00:03:24.879 you're doing like for example if you\n",
        "00:03:26.879 have things in a feature store you can\n",
        "00:03:28.799 use it to support both pi torch and\n",
        "00:03:31.120 tensorflow models that's the basic idea\n",
        "00:03:33.840 is that all of these things are are\n",
        "00:03:37.440 loosely connected and that you you can\n",
        "00:03:39.840 combine them in the way that you want to\n",
        "00:03:42.959 solve your problem\n",
        "00:03:44.480 so the tools that you're using can be\n",
        "00:03:47.040 like as i said user interface based low\n",
        "00:03:50.159 code or it can be programmatic so\n",
        "00:03:52.879 everything that you can do on the ui you\n",
        "00:03:54.560 can also do\n",
        "00:03:56.000 with a nice software development kit and\n",
        "00:03:58.640 sdk\n",
        "00:04:00.239 and of course today i'm going to be\n",
        "00:04:01.840 focusing primarily on the right hand\n",
        "00:04:04.000 side using the sdk to develop models\n",
        "00:04:07.360 so but once you develop a model\n",
        "00:04:09.760 that's not it right so\n",
        "00:04:12.000 you develop the model you deploy it but\n",
        "00:04:14.879 then you want to basically monitor it\n",
        "00:04:17.358 retrain it update it and once you have\n",
        "00:04:20.478 these kinds of complex\n",
        "00:04:22.880 different steps that you want to do\n",
        "00:04:25.440 automation becomes extremely important\n",
        "00:04:27.520 and that's where pipelines come in so\n",
        "00:04:29.680 when you think about a very simple view\n",
        "00:04:32.160 of a pipeline it lets you do\n",
        "00:04:34.759 experimentation try out different hyper\n",
        "00:04:37.199 parameters try out different types of\n",
        "00:04:39.919 models etc it allows you to do\n",
        "00:04:43.280 training and later on to retrain the\n",
        "00:04:45.680 model\n",
        "00:04:46.639 it allows you to deploy the model and\n",
        "00:04:49.120 then monitor it and you want to\n",
        "00:04:51.440 basically do a variety of things during\n",
        "00:04:54.160 that entire process and that's what\n",
        "00:04:55.919 pipelines help you do\n",
        "00:04:58.960 now how does vertex ai help there well\n",
        "00:05:02.320 with all of the automation it helps you\n",
        "00:05:04.400 do things like experiments try out\n",
        "00:05:06.240 different things it helps you\n",
        "00:05:08.720 reuse features it helps you do\n",
        "00:05:10.560 continuous monitoring helps you manage\n",
        "00:05:13.280 metadata throughout the entire process\n",
        "00:05:15.919 but most of all it allows you to\n",
        "00:05:18.160 orchestrate all of these things\n",
        "00:05:20.600 automatically with caching with\n",
        "00:05:23.440 dependency tracking in an ml pipeline\n",
        "00:05:28.000 however\n",
        "00:05:29.759 pipelines can be hard\n",
        "00:05:32.400 and one of the things that\n",
        "00:05:34.160 we firmly believe in\n",
        "00:05:36.240 is that simple things should be simple\n",
        "00:05:39.600 and hard things should be possible so\n",
        "00:05:42.720 pipelines make hard things possible\n",
        "00:05:45.680 right so when you talk about a pipeline\n",
        "00:05:47.600 you're talking about hard things like av\n",
        "00:05:50.720 testing performance monitoring uh uh\n",
        "00:05:54.240 comparing different experiments etc so\n",
        "00:05:56.560 that's great\n",
        "00:05:58.000 it's great that lets you do these hard\n",
        "00:05:59.840 things it lets you do feature stores\n",
        "00:06:02.240 experiments continuous monitoring edge\n",
        "00:06:04.639 deployment metadata no tracking\n",
        "00:06:08.319 but what if you want to do something\n",
        "00:06:09.680 simple\n",
        "00:06:11.520 all i want to do is to develop a model\n",
        "00:06:14.160 and in order to do that i want to\n",
        "00:06:15.759 basically create a data set that i can\n",
        "00:06:18.560 version that i know\n",
        "00:06:20.400 what data i'm using to train my model i\n",
        "00:06:22.639 want to train my model\n",
        "00:06:24.479 and not just train one model i want to\n",
        "00:06:26.560 tune the hyper parameters i want to also\n",
        "00:06:29.360 try out auto ml so a little bit of\n",
        "00:06:31.440 experimentation\n",
        "00:06:33.039 deploy that model and get predictions\n",
        "00:06:36.560 but this is the simple workflow\n",
        "00:06:39.280 and i think\n",
        "00:06:40.639 this workflow\n",
        "00:06:42.479 is like 90\n",
        "00:06:45.360 of what\n",
        "00:06:46.800 you will do day to day right so\n",
        "00:06:49.599 how do you do this simple workflow let's\n",
        "00:06:51.599 forget all the complexities let's forget\n",
        "00:06:53.840 all the sophisticated stuff how do you\n",
        "00:06:56.080 do this simple workflow developing a\n",
        "00:06:58.080 model training it tuning it automl\n",
        "00:07:01.039 deployed to an endpoint get predictions\n",
        "00:07:04.000 how do we do that in a simple way right\n",
        "00:07:06.639 so let's start\n",
        "00:07:08.240 first step is that you want to develop a\n",
        "00:07:10.160 model this is the thing that every data\n",
        "00:07:12.639 scientist like you know every tutorial\n",
        "00:07:15.039 that's what it talks about right so you\n",
        "00:07:16.960 go to a notebook you write some code and\n",
        "00:07:20.400 in that code you can basically you know\n",
        "00:07:22.960 have direct connections to things like\n",
        "00:07:25.120 bigquery data proc spark\n",
        "00:07:27.919 other things you want to be able to\n",
        "00:07:29.840 maybe even schedule the notebook you\n",
        "00:07:32.319 want to write the code in pytorch or you\n",
        "00:07:34.960 want to write the code in tensorflow etc\n",
        "00:07:37.360 so you do that in a notebook in in\n",
        "00:07:40.560 vertex ai that's a vertex ai workbench\n",
        "00:07:44.400 so inside the notebook then you\n",
        "00:07:47.199 basically will write keras code that's\n",
        "00:07:49.599 going to be my example so you will read\n",
        "00:07:51.680 a data set create a training data set\n",
        "00:07:53.840 you will create an evaluation data set\n",
        "00:07:56.080 you will create a keras model you will\n",
        "00:07:58.720 basically train the model\n",
        "00:08:01.199 and you will save the model this is your\n",
        "00:08:03.599 normal\n",
        "00:08:04.800 keras workflow\n",
        "00:08:06.560 and like let's just assume that that's\n",
        "00:08:08.639 basically what we're going to do so what\n",
        "00:08:10.479 i'm doing here is that i'm just opening\n",
        "00:08:12.400 up jupiter lab right right from my\n",
        "00:08:15.759 vertex ai and at this point\n",
        "00:08:18.800 i'm basically in the jupyter ui i can\n",
        "00:08:21.440 get clone the repo here\n",
        "00:08:23.759 right but let me just go ahead and do it\n",
        "00:08:25.759 from the command line and let me do git\n",
        "00:08:28.639 clone\n",
        "00:08:30.960 github.com\n",
        "00:08:34.958 um and\n",
        "00:08:36.799 i'll do the data science on gcp\n",
        "00:08:40.880 okay\n",
        "00:08:42.000 and\n",
        "00:08:43.039 move into that\n",
        "00:08:44.800 let me move to the branch edition too\n",
        "00:08:48.640 right so there we are\n",
        "00:08:50.640 and in here\n",
        "00:08:52.480 is\n",
        "00:08:53.519 my notebook\n",
        "00:08:54.959 right so i can basically go ahead and\n",
        "00:08:57.680 say that i want to basically use python\n",
        "00:08:59.680 3 right now i'm in my notebook and this\n",
        "00:09:03.519 is should be pretty familiar to anybody\n",
        "00:09:05.680 right so if i want right i basically out\n",
        "00:09:08.800 here i'm basically saying that i want to\n",
        "00:09:10.160 train my notebook on us central one\n",
        "00:09:12.880 right this is my bucket this is my\n",
        "00:09:15.120 project and here i'm running a bigquery\n",
        "00:09:18.800 query to basically go pull some data do\n",
        "00:09:22.399 some data preparation and create a brand\n",
        "00:09:25.200 new table called training data and a\n",
        "00:09:27.920 brand another new table called\n",
        "00:09:29.680 evaluation data right and then\n",
        "00:09:32.640 having done that i'm basically going\n",
        "00:09:35.200 ahead and extracting it out into cloud\n",
        "00:09:38.320 storage i can read the data directly\n",
        "00:09:40.800 from bigquery there's a bigquery reader\n",
        "00:09:42.800 in tensorflow but it turns out that if\n",
        "00:09:45.600 you have it in\n",
        "00:09:47.440 cloud storage it it gives me a few other\n",
        "00:09:50.080 benefits that again i i go through later\n",
        "00:09:53.279 but let's not worry about it so in this\n",
        "00:09:54.959 case rather than training directly of\n",
        "00:09:57.360 bigquery i'm actually extracting it out\n",
        "00:09:59.360 into a csv file and i basically get\n",
        "00:10:02.000 three csv files evaluation training and\n",
        "00:10:05.760 all\n",
        "00:10:06.720 and basically this is all just normal\n",
        "00:10:10.079 code i'm using tf data\n",
        "00:10:12.399 reading the data set making sure the\n",
        "00:10:14.720 data is okay\n",
        "00:10:16.000 then going ahead and creating my\n",
        "00:10:19.279 wide and deep model doing feature\n",
        "00:10:21.519 engineering on it training the model\n",
        "00:10:24.640 and now i have my model this is my\n",
        "00:10:27.600 entire keras model with a whole bunch of\n",
        "00:10:29.920 inputs with a few dense layers\n",
        "00:10:32.480 and\n",
        "00:10:33.360 having done this\n",
        "00:10:34.959 i now train my model i have my model\n",
        "00:10:38.480 it all works so this is basically the\n",
        "00:10:40.959 first part your part of your data\n",
        "00:10:42.640 science workflow and then if you want\n",
        "00:10:45.360 you can go ahead and take that model and\n",
        "00:10:47.279 deploy it using gcloud right so you can\n",
        "00:10:50.160 deploy the model using gcloud\n",
        "00:10:52.560 into\n",
        "00:10:53.680 vertex ai so\n",
        "00:10:56.000 that's one way this this would work\n",
        "00:10:58.399 and then i can take this entire\n",
        "00:11:01.440 notebook\n",
        "00:11:02.880 and in vertex ai i can basically\n",
        "00:11:04.880 schedule the notebook i can say okay i\n",
        "00:11:07.360 would like to go to this notebook\n",
        "00:11:09.519 and i would like to like schedule it\n",
        "00:11:12.480 right so i would like to run a scheduled\n",
        "00:11:14.240 run of this notebook at certain times\n",
        "00:11:17.120 and then because this notebook does\n",
        "00:11:19.279 everything from\n",
        "00:11:20.959 data preparation\n",
        "00:11:22.959 all the way to model deployment\n",
        "00:11:26.399 i have everything ready and it will\n",
        "00:11:29.440 basically\n",
        "00:11:30.720 uh be be a runnable thing\n",
        "00:11:33.519 so that's obviously one way that i could\n",
        "00:11:35.360 do this\n",
        "00:11:36.480 but it doesn't really support the kind\n",
        "00:11:38.800 of orchestration and\n",
        "00:11:40.720 everything that i want to do i want to\n",
        "00:11:42.079 automate it right so what do i so one of\n",
        "00:11:44.800 the things that i really\n",
        "00:11:47.200 believe\n",
        "00:11:48.240 is that notebooks are great for\n",
        "00:11:49.760 development\n",
        "00:11:51.360 um and you can use the notebook as a\n",
        "00:11:53.760 step of a pipeline you can schedule the\n",
        "00:11:55.839 entire notebook but really\n",
        "00:11:58.480 right for production purposes\n",
        "00:12:01.360 put your code in python files right\n",
        "00:12:03.680 because you can basically manage them\n",
        "00:12:05.680 and maintain them using your traditional\n",
        "00:12:08.639 software workflows they like notebooks\n",
        "00:12:11.519 are great for data scientists but for ml\n",
        "00:12:14.399 ops for operations for maintainability\n",
        "00:12:18.160 nothing beats having flat files having\n",
        "00:12:21.120 python files so what i've also then done\n",
        "00:12:24.399 once i've developed the notebook\n",
        "00:12:26.560 right what i did was i basically took\n",
        "00:12:28.880 all of the code in this notebook\n",
        "00:12:31.120 and moved it into a file called\n",
        "00:12:32.959 model.pipe\n",
        "00:12:34.480 right it's pretty easy to do you can\n",
        "00:12:36.240 basically go into your notebook and you\n",
        "00:12:38.639 can basically export the notebook as a\n",
        "00:12:41.440 uh\n",
        "00:12:42.240 not exported yeah save the notebook as a\n",
        "00:12:44.480 python file right and then basically\n",
        "00:12:47.360 remove all of the display stuff and you\n",
        "00:12:49.839 basically have it so all of this code is\n",
        "00:12:52.399 essentially the code that was in that\n",
        "00:12:54.959 notebook\n",
        "00:12:56.160 but it's essentially now callable\n",
        "00:12:58.880 and i added in\n",
        "00:13:00.480 a main right so that i could basically\n",
        "00:13:02.639 call it with a bunch of parameters like\n",
        "00:13:05.920 specifying the bucket specifying their\n",
        "00:13:08.399 training batch size etc so once i have\n",
        "00:13:12.160 that notebook\n",
        "00:13:14.160 right and i've basically gotten this we\n",
        "00:13:16.800 can now go ahead and try it out so this\n",
        "00:13:19.839 model here\n",
        "00:13:21.279 is is now\n",
        "00:13:23.200 workable\n",
        "00:13:24.720 and i can basically go ahead and call it\n",
        "00:13:28.240 right with uh now python3 model.5 and i\n",
        "00:13:33.040 have a like you know thing called minus\n",
        "00:13:34.880 minus developed and i can go ahead and\n",
        "00:13:37.040 run this make sure that all of this\n",
        "00:13:39.040 works so great\n",
        "00:13:41.199 so at this point i have my code\n",
        "00:13:44.000 but\n",
        "00:13:44.720 remember that in this notebook i was\n",
        "00:13:47.519 doing some weird stuff\n",
        "00:13:49.600 i was also calling gcloud commands to\n",
        "00:13:52.399 deploy the model\n",
        "00:13:53.839 this is not python anymore this is bash\n",
        "00:13:57.920 how do i automate these\n",
        "00:14:00.079 well you could do it in bash but that's\n",
        "00:14:02.560 not what i would recommend keep things\n",
        "00:14:04.639 in python keep this whole automation\n",
        "00:14:07.120 pipeline in python so that's basically\n",
        "00:14:09.680 what i'm going to be talking about here\n",
        "00:14:11.920 so\n",
        "00:14:13.120 remember the the simple thing here that\n",
        "00:14:15.360 we're talking about right we're\n",
        "00:14:17.279 basically saying we want to develop a\n",
        "00:14:18.959 model but having developed the model\n",
        "00:14:21.360 let's look at this this entire workflow\n",
        "00:14:23.839 creating the data set training a model\n",
        "00:14:26.160 deploying to an endpoint getting\n",
        "00:14:27.839 predictions\n",
        "00:14:28.959 how to do that in python so that you can\n",
        "00:14:31.440 automate it really well so this is\n",
        "00:14:33.519 basically the workflow so you've now\n",
        "00:14:36.560 taken the model you've saved it\n",
        "00:14:38.880 now you can basically upload the model\n",
        "00:14:41.920 as a vertex ai model\n",
        "00:14:44.399 and then deploy the model to an endpoint\n",
        "00:14:47.839 in such a way that when a client request\n",
        "00:14:50.639 comes in through json\n",
        "00:14:52.639 this endpoint basically invokes your\n",
        "00:14:55.040 model\n",
        "00:14:56.000 and sends back a json payload and the\n",
        "00:14:58.639 endpoint doesn't contain just one model\n",
        "00:15:01.839 the end point can contain multiple\n",
        "00:15:04.079 models and do a traffic split between\n",
        "00:15:06.720 them\n",
        "00:15:07.600 so that's the basic basic idea so the\n",
        "00:15:10.320 concepts here are you have your model\n",
        "00:15:12.880 just model files you upload them into a\n",
        "00:15:15.440 model object\n",
        "00:15:16.880 you put model objects into endpoints and\n",
        "00:15:20.000 you basically do traffic splits in the\n",
        "00:15:22.639 endpoint between different models so\n",
        "00:15:25.120 let's start with the first thing you\n",
        "00:15:27.199 want to train a model you want to have a\n",
        "00:15:28.959 data set\n",
        "00:15:30.560 so the reason that you want to have a\n",
        "00:15:32.320 data set that is in vertex ai\n",
        "00:15:35.040 we think of four different types of\n",
        "00:15:37.279 objects\n",
        "00:15:38.480 a data set which could be structured or\n",
        "00:15:40.959 unstructured it will have metadata it\n",
        "00:15:43.600 will have versions so that you know\n",
        "00:15:46.079 given a model what version of data was\n",
        "00:15:48.800 used to train that model that's part of\n",
        "00:15:50.560 the metadata of that model\n",
        "00:15:52.880 the next thing that you will often have\n",
        "00:15:54.639 is that you'll have the model itself you\n",
        "00:15:56.399 trained a model now you have model files\n",
        "00:15:58.880 your saved model in tensorflow right you\n",
        "00:16:01.360 have machine learning model artifact so\n",
        "00:16:04.079 those files that's your model object and\n",
        "00:16:07.440 the end point is the thing that gets\n",
        "00:16:09.440 called it contains multiple models and\n",
        "00:16:12.240 you orchestrate all of these things\n",
        "00:16:14.800 using a training job or a pipeline right\n",
        "00:16:17.759 and the pipeline basically is take a\n",
        "00:16:20.320 data set train a model on it deploy to\n",
        "00:16:22.560 the endpoint\n",
        "00:16:24.320 so essentially the way you start is that\n",
        "00:16:27.279 you basically have now in my case a\n",
        "00:16:29.600 train on vertex ai right and i basically\n",
        "00:16:32.720 initialize the ai platform\n",
        "00:16:35.199 and i say\n",
        "00:16:36.399 my data set\n",
        "00:16:38.000 is a tabular data set\n",
        "00:16:40.320 it consists of all of the files that\n",
        "00:16:43.600 meet this wild card thing so i'm\n",
        "00:16:46.000 basically using tensorflow right g\n",
        "00:16:48.839 file.glob find all of the all star.csv\n",
        "00:16:53.279 and using those files to create my\n",
        "00:16:56.560 tabular data set so now this data set\n",
        "00:16:59.680 contains a number of metadata\n",
        "00:17:02.079 information that is going to get passed\n",
        "00:17:04.319 along i don't you will not actually see\n",
        "00:17:06.559 any metadata in any of my code but\n",
        "00:17:08.959 remember that underneath\n",
        "00:17:10.799 vertex ai is taking care of all of that\n",
        "00:17:12.720 for us okay so we're basically getting a\n",
        "00:17:15.679 tabular data set\n",
        "00:17:17.599 and it consists of all of these files so\n",
        "00:17:20.319 that's pretty much it\n",
        "00:17:21.839 so then in model.pi\n",
        "00:17:25.919 earlier we kind of knew what we were\n",
        "00:17:28.319 reading but now\n",
        "00:17:30.320 vertex ai is going to basically set a\n",
        "00:17:32.320 few environment variables for us\n",
        "00:17:34.720 it's going to tell us what the training\n",
        "00:17:37.520 data is\n",
        "00:17:38.720 what the evaluation data is and what the\n",
        "00:17:40.880 test data is\n",
        "00:17:42.480 those are three of our true environment\n",
        "00:17:44.320 variables\n",
        "00:17:45.440 and it's going to tell us\n",
        "00:17:47.280 where to save the output\n",
        "00:17:49.760 okay so what i will do\n",
        "00:17:51.919 is that in my model.pi\n",
        "00:17:54.400 i will look for this environment data\n",
        "00:17:56.960 and if it's not present then i know that\n",
        "00:17:59.919 somebody's running it not in the context\n",
        "00:18:02.320 of a pipeline where these things are set\n",
        "00:18:05.440 but they're probably running it from a\n",
        "00:18:07.360 notebook they're probably running it\n",
        "00:18:09.679 locally right and so you can basically\n",
        "00:18:12.240 say if that's not set\n",
        "00:18:14.559 like\n",
        "00:18:15.360 hardcoded to be where we expect it to be\n",
        "00:18:18.160 okay so once you do that you now have\n",
        "00:18:21.120 your dataset created and now you're able\n",
        "00:18:24.000 to train your model so\n",
        "00:18:26.080 the training the model means\n",
        "00:18:28.559 that you have your model code that's\n",
        "00:18:31.120 your custom training job and you're\n",
        "00:18:33.440 saying go ahead and please run\n",
        "00:18:35.120 model.pipe\n",
        "00:18:37.600 that's your code\n",
        "00:18:39.520 but in order to run it we will need a\n",
        "00:18:42.799 container vertex ai works on containers\n",
        "00:18:45.200 everything is containerized\n",
        "00:18:47.120 fortunately\n",
        "00:18:48.559 there is a pre-built container\n",
        "00:18:51.200 for every version of tensorflow that you\n",
        "00:18:53.200 want to use\n",
        "00:18:54.320 so in this case i'm basically saying\n",
        "00:18:56.880 like i want to train on the gpu version\n",
        "00:18:59.360 of tensorflow and i want to deploy on\n",
        "00:19:01.840 the cpu version of tensorflow right so\n",
        "00:19:04.640 basically i'm getting two\n",
        "00:19:06.720 training image and the deployment image\n",
        "00:19:09.039 that's the training image this is the\n",
        "00:19:10.480 prediction image i have two two\n",
        "00:19:13.120 pre-built containers and so when i'm\n",
        "00:19:16.000 when i'm training it i'm saying use this\n",
        "00:19:18.559 as your training image and serve it on\n",
        "00:19:20.880 the deploy image so pretty much\n",
        "00:19:23.600 i'm can take my model.pi and use it to\n",
        "00:19:26.880 create a custom training job and once\n",
        "00:19:29.679 i've created a custom training job i run\n",
        "00:19:32.559 the job\n",
        "00:19:34.000 passing in the data set\n",
        "00:19:36.160 right and passing in the accelerator\n",
        "00:19:39.039 type the machine type etc that i want to\n",
        "00:19:41.919 do the training on\n",
        "00:19:43.760 so bottom line then you train your model\n",
        "00:19:46.480 you create all the training code and\n",
        "00:19:48.559 then when you're ready to train the\n",
        "00:19:49.919 model on some piece of hardware what you\n",
        "00:19:52.640 do is you create a custom training job\n",
        "00:19:55.039 providing the tensorflow containers this\n",
        "00:19:57.039 is where the prefab\n",
        "00:19:58.720 part of my title comes in it's all\n",
        "00:20:01.200 already there prefabricated so just go\n",
        "00:20:04.159 ahead and use the pre-built container\n",
        "00:20:07.200 pass in the hook for your custom code\n",
        "00:20:09.760 and remember that your custom code right\n",
        "00:20:12.480 will basically you this is the contract\n",
        "00:20:15.200 that your custom code has to honor it\n",
        "00:20:17.679 has to read the data from the path\n",
        "00:20:20.799 pointed by these three environment\n",
        "00:20:22.559 variables and write the data to the path\n",
        "00:20:25.520 pointed by this thing so if you look in\n",
        "00:20:28.000 my code in model.pi\n",
        "00:20:30.400 right so basically that's what i'm doing\n",
        "00:20:33.039 i'm basically setting the output there\n",
        "00:20:35.280 based on the output model there i'm\n",
        "00:20:37.760 setting the training data pattern based\n",
        "00:20:39.919 on the environment variable right and if\n",
        "00:20:42.240 the environment variable isn't set\n",
        "00:20:44.320 i'm basically dropping back to the\n",
        "00:20:47.360 original behavior i had in my notebook\n",
        "00:20:50.000 right my original notebook was basically\n",
        "00:20:52.880 reading train star so i just keep that\n",
        "00:20:55.840 if\n",
        "00:20:57.840 i'm not running in a pipeline if i'm\n",
        "00:20:59.440 running in a pipeline i'm getting the\n",
        "00:21:02.240 the orig the the place to read from\n",
        "00:21:05.440 from the environment variable\n",
        "00:21:07.360 the rest of my code remains exactly the\n",
        "00:21:10.400 same right because all of this is\n",
        "00:21:12.720 basically just processing the training\n",
        "00:21:14.880 data pattern evaluation data pattern and\n",
        "00:21:17.840 that comes either from the environment\n",
        "00:21:19.520 variable or from a hard-coded value\n",
        "00:21:21.919 because i'm calling it from a notebook\n",
        "00:21:23.919 right so given those two things\n",
        "00:21:27.200 at this point\n",
        "00:21:28.720 i now have my\n",
        "00:21:30.480 model training done\n",
        "00:21:32.240 okay\n",
        "00:21:33.039 how does this change if you're not doing\n",
        "00:21:35.200 a custom model remember that this is if\n",
        "00:21:37.120 you are writing tensorflow code myself\n",
        "00:21:40.159 and i'm saying i want to run it here\n",
        "00:21:42.000 well if you're doing auto ml we will\n",
        "00:21:44.559 look at it so we basically change this\n",
        "00:21:47.039 function train custom model to be train\n",
        "00:21:50.159 auto ml model this part will change but\n",
        "00:21:53.039 the rest of my pipeline is going to\n",
        "00:21:55.120 remain the same i'm going to continue to\n",
        "00:21:57.440 use the same data set\n",
        "00:22:00.080 but i will use that same data set\n",
        "00:22:02.960 for training a vertex ai model for\n",
        "00:22:05.440 training an automl model for doing hyper\n",
        "00:22:08.159 parameter tuning and then regardless of\n",
        "00:22:10.480 how i got my model i'll deploy it in\n",
        "00:22:13.360 exactly the same way so this is part of\n",
        "00:22:15.679 that unification that's super important\n",
        "00:22:18.080 it doesn't matter\n",
        "00:22:19.840 how you train your model this could be a\n",
        "00:22:21.679 pie torch model\n",
        "00:22:23.280 and you would still create the data set\n",
        "00:22:24.960 and you would still deploy it in exactly\n",
        "00:22:26.640 the same way that's part of the power of\n",
        "00:22:28.559 this you can basically switch any of\n",
        "00:22:31.200 these things in the middle and you have\n",
        "00:22:32.640 that unified workflow making life a lot\n",
        "00:22:35.200 easier\n",
        "00:22:36.320 so what if you're not using these\n",
        "00:22:37.919 pre-built containers that i did that i\n",
        "00:22:39.840 showed you with uh with tensorflow well\n",
        "00:22:42.720 there's two other two other ways that\n",
        "00:22:44.400 you could do it you could use tube flow\n",
        "00:22:47.200 pipelines and you could basically write\n",
        "00:22:50.000 whatever code you want\n",
        "00:22:52.000 right and basically have a base image\n",
        "00:22:55.039 and\n",
        "00:22:55.840 now create your own docker image write\n",
        "00:22:58.240 your own code and use that to basically\n",
        "00:23:01.760 create a custom training job\n",
        "00:23:04.080 and that custom training job is\n",
        "00:23:05.840 basically the stuff in the middle here\n",
        "00:23:07.679 right you need this custom training job\n",
        "00:23:10.640 and you just do a job at run\n",
        "00:23:12.559 everything remains the same you've just\n",
        "00:23:14.320 gotten a new custom training job out of\n",
        "00:23:17.039 your own container out of your own code\n",
        "00:23:19.760 so that's one way to do it\n",
        "00:23:21.520 the other way to do it is to say forget\n",
        "00:23:23.360 about all this prefab\n",
        "00:23:25.039 pipelines business i will go use tfx and\n",
        "00:23:28.559 tfx is up is a very prescriptive way of\n",
        "00:23:31.440 running things where basically provides\n",
        "00:23:34.000 you\n",
        "00:23:35.039 python libraries for data validation\n",
        "00:23:37.520 python libraries for a tensorflow\n",
        "00:23:40.240 transformation etc you use all of those\n",
        "00:23:43.520 tfx components\n",
        "00:23:45.520 and you use it to create\n",
        "00:23:47.600 a pipeline object and you run that\n",
        "00:23:49.919 pipeline object okay so you have\n",
        "00:23:52.080 multiple ways of creating pipelines i'm\n",
        "00:23:54.640 not going to talk about these two two\n",
        "00:23:56.240 other approaches because today i'm\n",
        "00:23:58.480 talking about the very like prefab\n",
        "00:24:01.520 simple things should be simple approach\n",
        "00:24:03.840 this is the more\n",
        "00:24:05.200 you can do hard things and hard things\n",
        "00:24:06.799 are possible approach\n",
        "00:24:09.120 so once you've gotten your model\n",
        "00:24:12.080 what you do to your model you deploy to\n",
        "00:24:13.840 an endpoint\n",
        "00:24:15.039 so you want to create an endpoint\n",
        "00:24:17.679 now so here's the thing though\n",
        "00:24:19.520 you don't want to create an end point if\n",
        "00:24:21.520 it already exists remember the idea\n",
        "00:24:23.440 behind an end point you want to do\n",
        "00:24:24.880 traffic splits\n",
        "00:24:26.320 if every model gets deployed to a new\n",
        "00:24:28.640 endpoint\n",
        "00:24:30.320 it doesn't make sense so you want to\n",
        "00:24:32.240 keep the end point consistent and you\n",
        "00:24:34.720 want to keep deploying more and more\n",
        "00:24:36.320 models into the same endpoint and\n",
        "00:24:38.799 splitting traffic between\n",
        "00:24:41.520 different trained models\n",
        "00:24:44.159 as you go along so that's what i'm doing\n",
        "00:24:46.240 here i'm saying go ahead and get me all\n",
        "00:24:48.960 the end points with a specific display\n",
        "00:24:51.279 name\n",
        "00:24:52.159 and order it by the descending order of\n",
        "00:24:54.960 create time and take the most recently\n",
        "00:24:57.919 created endpoint\n",
        "00:24:59.760 and that's the end point that i'm going\n",
        "00:25:01.520 to deploy ideally you've only created an\n",
        "00:25:04.080 endpoint once\n",
        "00:25:05.840 and then from then on you're just\n",
        "00:25:07.679 reusing it over and over and over again\n",
        "00:25:09.760 right so create the endpoint with a\n",
        "00:25:11.360 specific name this endpoint name\n",
        "00:25:14.400 and reuse it over again\n",
        "00:25:16.480 so once you do that\n",
        "00:25:18.159 you can basically say model.deploy\n",
        "00:25:20.400 passing in the endpoint passing in the\n",
        "00:25:22.960 traffic split so here i'm saying this\n",
        "00:25:25.440 model that i'm deploying\n",
        "00:25:27.600 given 100 of the traffic\n",
        "00:25:29.919 if you want you can basically pass in\n",
        "00:25:32.320 previously deployed model ids\n",
        "00:25:34.960 and say that model has 30 and i get 70\n",
        "00:25:39.440 or vice versa specify the machine type\n",
        "00:25:42.320 that you want to deploy on\n",
        "00:25:44.159 and that's pretty much it right\n",
        "00:25:47.679 so once you have deployed the model to\n",
        "00:25:50.159 an endpoint you want to use it\n",
        "00:25:53.120 so the way that you can use it is you\n",
        "00:25:55.360 have to basically create a json request\n",
        "00:25:58.240 so this is what the request looks like\n",
        "00:26:00.000 these are all of the input features to\n",
        "00:26:02.000 the model so you basically go through\n",
        "00:26:04.080 and you pass it you create a json\n",
        "00:26:05.840 something and you pass that in\n",
        "00:26:07.840 and\n",
        "00:26:08.720 you basically say endpoints predict and\n",
        "00:26:11.760 pass in your json request so this is how\n",
        "00:26:14.240 you can do it\n",
        "00:26:15.520 with bash just to try things out during\n",
        "00:26:17.919 development\n",
        "00:26:19.120 but obviously your client code is not\n",
        "00:26:21.440 going to use gcloud\n",
        "00:26:23.120 your client code wants to use python\n",
        "00:26:25.600 wants to use rest wants to use java\n",
        "00:26:28.640 wants to use something so if you want to\n",
        "00:26:30.720 use python\n",
        "00:26:32.240 you basically get the end point as\n",
        "00:26:34.240 before\n",
        "00:26:35.360 and then you basically create a no\n",
        "00:26:38.559 python dictionary with the exact same\n",
        "00:26:41.279 json structure\n",
        "00:26:43.039 and you pass in\n",
        "00:26:45.039 endpoint.predict\n",
        "00:26:46.720 you pass in the data and you get the set\n",
        "00:26:49.840 of predictions\n",
        "00:26:51.120 okay so that's pretty much uh how you do\n",
        "00:26:53.679 it and you can also do it using rest you\n",
        "00:26:56.400 can basically use curl and you can pass\n",
        "00:26:58.640 in a specific thing so there's\n",
        "00:27:01.279 it's just a rest api so you can invoke\n",
        "00:27:03.520 it as long as you basically put your\n",
        "00:27:05.520 data in this form and you post it you're\n",
        "00:27:07.919 fine\n",
        "00:27:09.520 so now let's look at a couple of the\n",
        "00:27:11.600 other like\n",
        "00:27:12.960 how does it change if i wanted to tune\n",
        "00:27:14.880 hyper parameters\n",
        "00:27:16.480 well if you want to tune hyper\n",
        "00:27:17.760 parameters\n",
        "00:27:19.120 inside your model.pi you've got to do\n",
        "00:27:21.279 two things\n",
        "00:27:22.640 first thing\n",
        "00:27:24.000 is anything that you want to tune has to\n",
        "00:27:27.360 be a command line parameter to your\n",
        "00:27:29.360 model so if you want to tune the number\n",
        "00:27:31.440 of buckets that you're going to\n",
        "00:27:33.120 discretize your variable into you make\n",
        "00:27:36.000 that an input parameter into your into\n",
        "00:27:38.720 your\n",
        "00:27:39.520 code\n",
        "00:27:40.559 and then you basically\n",
        "00:27:42.559 write\n",
        "00:27:44.320 a keras callback so this is a this is my\n",
        "00:27:47.200 hyper parameter tuning callback in keras\n",
        "00:27:50.240 i say at the end of every epoch\n",
        "00:27:53.360 please report\n",
        "00:27:55.440 the metric that i care about which is\n",
        "00:27:57.600 the\n",
        "00:27:58.320 validation root mean squared error so\n",
        "00:28:01.279 that is my hyperparameter metric\n",
        "00:28:03.679 and say report that\n",
        "00:28:05.760 and then create the model train it then\n",
        "00:28:08.240 as you normally do but when you call the\n",
        "00:28:10.799 model that fit remember to also call it\n",
        "00:28:14.080 pass in the hyper parameter tuning\n",
        "00:28:16.320 callback\n",
        "00:28:17.600 that's all you need to do\n",
        "00:28:19.200 now your keras model is going to be\n",
        "00:28:21.200 reporting metrics as it goes along\n",
        "00:28:24.880 and then in\n",
        "00:28:26.080 in their training pipeline code\n",
        "00:28:29.279 you built the original job that you had\n",
        "00:28:31.760 the custom training job\n",
        "00:28:34.000 it is now just a single trial so you say\n",
        "00:28:37.120 give me a trial job\n",
        "00:28:39.360 from my model.script\n",
        "00:28:41.600 and you use that to create a hyper\n",
        "00:28:43.600 parameter tuning job\n",
        "00:28:45.840 and you train that job\n",
        "00:28:48.240 and the result of it is going to be a\n",
        "00:28:50.640 number of trials\n",
        "00:28:52.399 you sort it find the best trial and\n",
        "00:28:55.760 using the best set of parameters you can\n",
        "00:28:58.399 basically go train it again right to\n",
        "00:29:00.960 train it all the way through typically\n",
        "00:29:03.039 what you do when you do hyper private\n",
        "00:29:04.960 tuning is that you train on a smaller\n",
        "00:29:07.520 set of data faster\n",
        "00:29:09.919 and then like once you've found the best\n",
        "00:29:12.159 set of parameters you tune that\n",
        "00:29:15.200 you take those parameters and you train\n",
        "00:29:17.840 the mod the model on the full data set\n",
        "00:29:20.480 for much longer so that's basically what\n",
        "00:29:23.279 this is showing here so if you go into\n",
        "00:29:27.919 train on vertex ai dot pi\n",
        "00:29:30.320 the hyperbrain retuning job is to\n",
        "00:29:32.799 basically create my custom job for my\n",
        "00:29:35.919 local script passing in the model.pi\n",
        "00:29:39.520 saying please don't evaluate on the full\n",
        "00:29:41.919 data set just train on a smaller number\n",
        "00:29:44.640 of examples\n",
        "00:29:46.159 and go run this code for me that's my\n",
        "00:29:48.720 trial job and then my hyper parameter\n",
        "00:29:51.360 tuning job\n",
        "00:29:52.720 basically says i want to minimize my\n",
        "00:29:55.039 validation rmse and these are the three\n",
        "00:29:58.159 parameters i want you to tune\n",
        "00:30:00.640 the training back size the number of\n",
        "00:30:02.720 buckets and the number of layers and\n",
        "00:30:04.559 nodes in my\n",
        "00:30:05.919 in my dnn model and run it for these\n",
        "00:30:09.520 many number of trials\n",
        "00:30:11.520 and then once you run it you get all of\n",
        "00:30:14.159 your trials you find the best one\n",
        "00:30:16.960 and you go through all of those find all\n",
        "00:30:19.360 of the best parameters and you basically\n",
        "00:30:22.159 go ahead and call the original\n",
        "00:30:24.880 train custom model the one that we first\n",
        "00:30:27.120 wrote with the full set so that's pretty\n",
        "00:30:30.080 much\n",
        "00:30:30.799 hyper grammar tuning\n",
        "00:30:32.960 how do you do auto ml well automl means\n",
        "00:30:36.320 instead of training a custom job you're\n",
        "00:30:39.200 going to use an automl tabular training\n",
        "00:30:41.440 job and run it so here\n",
        "00:30:44.240 right uh the\n",
        "00:30:46.240 train that's a custom model here's my\n",
        "00:30:49.039 trained automl model i'm creating a job\n",
        "00:30:52.640 i'm doing job.run i'm returning the\n",
        "00:30:54.960 model\n",
        "00:30:56.000 so regardless of how i did it i trained\n",
        "00:30:59.279 my data set\n",
        "00:31:00.799 i did one of the three training jobs\n",
        "00:31:03.519 automl or hyperparameter tuning are\n",
        "00:31:06.240 custom modeling\n",
        "00:31:07.760 and then i deploy to the endpoint\n",
        "00:31:10.559 and i'm done\n",
        "00:31:12.080 right i'm done at this point and i can\n",
        "00:31:14.720 take this entire code my main\n",
        "00:31:17.679 and i can basically run that entire\n",
        "00:31:19.760 thing as a pipeline to basically get all\n",
        "00:31:22.559 my metadata tracking and everything\n",
        "00:31:24.399 going\n",
        "00:31:25.200 okay so\n",
        "00:31:27.440 bottom line at this point what you've\n",
        "00:31:28.799 done is you've created this workflow you\n",
        "00:31:31.120 basically take your model in your keras\n",
        "00:31:32.799 code you've done a model that's safe you\n",
        "00:31:34.559 upload the model right you deploy the\n",
        "00:31:37.120 model and you make it available for\n",
        "00:31:38.880 predictions\n",
        "00:31:40.480 so\n",
        "00:31:41.840 to summarize\n",
        "00:31:43.279 prefab training pipelines they make\n",
        "00:31:45.440 simple workflows really simple\n",
        "00:31:47.120 developing a model creating a data set\n",
        "00:31:50.159 training it tuning it deploying the end\n",
        "00:31:53.279 point and getting predictions\n",
        "00:31:56.240 for if you want to do custom or\n",
        "00:31:57.760 continuous evaluation for example all\n",
        "00:32:00.399 that you need to do is when you do the\n",
        "00:32:02.559 job.run\n",
        "00:32:04.000 also export the evaluations into a\n",
        "00:32:06.640 bigquery table and\n",
        "00:32:08.880 provide that bigquery table to vertex ai\n",
        "00:32:11.519 and it will automatically start\n",
        "00:32:13.120 monitoring your metric and show you\n",
        "00:32:16.080 graphs of it and then you can set up\n",
        "00:32:18.320 when this\n",
        "00:32:19.360 metric drops below a certain point\n",
        "00:32:22.320 retrain right that's pretty much it so\n",
        "00:32:24.960 it's it's very very very straightforward\n",
        "00:32:27.760 to start to add many of these amazing\n",
        "00:32:30.240 capabilities to this very simple\n",
        "00:32:33.519 starter point\n",
        "00:32:35.200 so in summary right we talked about uh\n",
        "00:32:38.480 this simple workflow like follow me on\n",
        "00:32:40.880 twitter uh lac underscore gcp\n",
        "00:32:44.000 uh this\n",
        "00:32:45.279 uh\n",
        "00:32:46.159 i i will leave here um\n",
        "00:32:49.519 oh\n",
        "00:32:50.640 okay the uh the github repository which\n",
        "00:32:53.760 is probably the most important thing\n",
        "00:32:55.120 here uh is this guy\n",
        "00:32:57.760 okay\n",
        "00:32:59.279 so go there\n",
        "00:33:01.360 uh just one\n",
        "00:33:03.200 warning\n",
        "00:33:04.799 make sure to switch to edition two right\n",
        "00:33:07.679 because uh you know\n",
        "00:33:09.519 so right there\n",
        "00:33:10.880 there is o9 vertex ai so just remember\n",
        "00:33:14.159 this url i'll also post this in the chat\n",
        "00:33:22.720 all right and with that i'll take\n",
        "00:33:23.919 questions\n",
        "00:33:25.760 awesome great uh wow the vertex ai\n",
        "00:33:28.880 really makes the machine learning tasks\n",
        "00:33:31.360 easy and\n",
        "00:33:32.180 [Music]\n",
        "00:33:33.440 the full stack\n",
        "00:33:35.279 um yeah we have\n",
        "00:33:37.120 quite a lot of questions uh let me bring\n",
        "00:33:40.640 up uh the sli uh sign up for that\n",
        "00:33:44.240 um just a quick reminder\n",
        "00:33:47.679 right off of this right and uh yeah so i\n",
        "00:33:51.679 just posted that and uh if you go to\n",
        "00:33:54.399 github there is the ipython notebook\n",
        "00:33:56.960 okay right right right great yeah\n",
        "00:33:59.279 yeah i just wanted to remind that you\n",
        "00:34:00.960 know for anybody for uh questions\n",
        "00:34:03.760 uh sorry just a quick reminder you know\n",
        "00:34:05.760 if you have questions uh you can post in\n",
        "00:34:07.679 the chat or raise hand we we can uh um\n",
        "00:34:11.280 build you to speak\n",
        "00:34:12.879 uh let's go with\n",
        "00:34:14.399 the i have a few first questions\n",
        "00:34:17.918 um one question is is there any further\n",
        "00:34:20.719 granule level control on where my\n",
        "00:34:23.679 endpoint will be deployed within a\n",
        "00:34:26.000 region\n",
        "00:34:27.119 uh\n",
        "00:34:27.918 you can do it within a zone i would not\n",
        "00:34:30.320 i would not uh go there i mean again a\n",
        "00:34:33.280 region in google cloud is probably the\n",
        "00:34:35.839 most granular that you want to go right\n",
        "00:34:38.000 so so specify a specific region i mean\n",
        "00:34:40.879 why would you ever want to basically\n",
        "00:34:42.399 specify a single zone that doesn't make\n",
        "00:34:44.879 too much of sense right so so stick to a\n",
        "00:34:46.879 region a region is probably the most\n",
        "00:34:48.800 granular that you want that you want to\n",
        "00:34:50.560 be you might want to do a multi-region\n",
        "00:34:52.879 you might want to do for like you know\n",
        "00:34:55.760 resilience purposes but a region is the\n",
        "00:34:58.800 is the lowest ground\n",
        "00:35:02.720 okay great another question from davis\n",
        "00:35:05.680 why are you using jupiter lab so the\n",
        "00:35:08.400 collab is google's version of adobe to\n",
        "00:35:10.880 lab\n",
        "00:35:12.480 that's the absolute colab is our free\n",
        "00:35:15.760 version we have now for some people who\n",
        "00:35:18.480 wanted to use it in enterprise we now\n",
        "00:35:20.320 have a paid version of colab but uh in\n",
        "00:35:24.240 vertex ai what we call vertex ai\n",
        "00:35:26.320 workbench uh is basically our\n",
        "00:35:29.040 development environment that provides\n",
        "00:35:31.680 enterprise capabilities like vpc service\n",
        "00:35:34.400 controls encryption uh like you know\n",
        "00:35:37.880 multi-tenancy a whole bunch of things\n",
        "00:35:40.000 that if you're running it in an\n",
        "00:35:41.680 enterprise if you have confidential data\n",
        "00:35:44.240 i strongly suggest using the vertex ai\n",
        "00:35:46.960 workbench\n",
        "00:35:50.800 another question is asking is the\n",
        "00:35:52.720 voltage ai free like a collab or\n",
        "00:35:56.240 different vertex is part of google cloud\n",
        "00:35:59.040 and you will be a google you know it is\n",
        "00:36:01.200 definitely definitely a a paid service\n",
        "00:36:04.160 it's not free okay uh there is a free\n",
        "00:36:06.880 trial version of google cloud where you\n",
        "00:36:08.640 get 300 free so definitely use it to\n",
        "00:36:11.920 learn but now when you're using in\n",
        "00:36:14.000 production you you know it's not free\n",
        "00:36:19.440 all right another question\n",
        "00:36:21.760 uh\n",
        "00:36:23.839 to clarify the question from chester to\n",
        "00:36:26.240 clarify are you claiming that vertex ai\n",
        "00:36:28.640 can accomplish what a spark plus h2o\n",
        "00:36:32.640 plus ml flow plus ml1 frameworks will do\n",
        "00:36:38.800 spark is larger than vertex ai right\n",
        "00:36:42.400 spark is a combination of data\n",
        "00:36:44.640 processing\n",
        "00:36:46.000 and machine learning so if you're\n",
        "00:36:47.839 talking about spark m light then yes\n",
        "00:36:50.079 right but remember that spark also helps\n",
        "00:36:52.720 you do data preparation so on google\n",
        "00:36:56.400 cloud you would run spark within data\n",
        "00:36:59.119 proc right so you could use data proc to\n",
        "00:37:01.680 prepare your data the way you do like\n",
        "00:37:03.920 large data sets with spark more commonly\n",
        "00:37:06.640 on google cloud people use bigquery to\n",
        "00:37:08.800 prepare data but you can do spark or you\n",
        "00:37:11.119 can do bigquery to prepare the data but\n",
        "00:37:13.440 once that data has been prepared as far\n",
        "00:37:16.960 as feature engineering and training and\n",
        "00:37:20.640 you know no code no code all of these\n",
        "00:37:23.200 different frameworks yeah that unified\n",
        "00:37:25.200 framework unified orchestration you're\n",
        "00:37:28.560 scheduling all of these things\n",
        "00:37:30.240 absolutely don't\n",
        "00:37:32.079 don't buy individual things and glue\n",
        "00:37:34.240 them together buy something that is\n",
        "00:37:36.160 already well integrated would be my\n",
        "00:37:37.760 thing yes i am i am making the claim\n",
        "00:37:40.960 that other than data pre-processing\n",
        "00:37:42.880 which is not which is not\n",
        "00:37:45.200 but everything machine learning related\n",
        "00:37:46.720 you can do in wordpress app bigquery\n",
        "00:37:48.960 plus vertex ai is an extremely powerful\n",
        "00:37:51.119 solution as is data proc plus vertex\n",
        "00:37:57.280 great\n",
        "00:37:58.160 use it for free while learning it yes uh\n",
        "00:38:01.200 again a google cloud offers a lot of\n",
        "00:38:03.520 things free for students so go to\n",
        "00:38:06.800 cloud.google.com\n",
        "00:38:08.720 edu for example right and you will get a\n",
        "00:38:12.480 lot of higher education programs have\n",
        "00:38:14.880 your faculty members if you're a student\n",
        "00:38:17.520 register for it so that all of your\n",
        "00:38:19.359 class can get access to it strongly\n",
        "00:38:21.760 encourage you to do that if you also go\n",
        "00:38:23.920 to cloud.google.com there is a 300 free\n",
        "00:38:26.560 trial that you can use to learn as well\n",
        "00:38:30.240 the third way to learn relatively\n",
        "00:38:33.040 quickly is quicklabs.com\n",
        "00:38:36.079 quick labs has a number of uh all these\n",
        "00:38:39.119 labs so if you go to quick cloud you go\n",
        "00:38:41.440 to the catalog\n",
        "00:38:43.200 and you search for vertex ai you should\n",
        "00:38:46.640 see\n",
        "00:38:47.599 a number of uh\n",
        "00:38:50.320 labs that you can basically build and\n",
        "00:38:52.400 deploy\n",
        "00:38:53.440 uh machine learning solutions and vertex\n",
        "00:38:55.359 ai so that is\n",
        "00:38:56.800 this quest for example is an easy way to\n",
        "00:38:59.440 basically go through and learn\n",
        "00:39:01.920 all of the steps and i know it says 35\n",
        "00:39:04.880 credits but\n",
        "00:39:06.320 quick labs usually runs all of these\n",
        "00:39:08.079 promotions where they basically give you\n",
        "00:39:10.320 like a month free just watch for those\n",
        "00:39:12.960 and that that that's that's another way\n",
        "00:39:15.599 to basically learn this thing for free\n",
        "00:39:21.680 awesome great\n",
        "00:39:23.119 uh\n",
        "00:39:25.119 again uh just remind uh you for\n",
        "00:39:27.280 questions feel free to post in the chat\n",
        "00:39:30.960 i see a lot of messages in the chat so\n",
        "00:39:32.880 if your questions got\n",
        "00:39:34.800 screwed up\n",
        "00:39:36.320 [Music]\n",
        "00:39:37.440 lauren has an interesting question you\n",
        "00:39:38.880 can\n",
        "00:39:43.920 ai are containers instances under the\n",
        "00:39:46.000 hood of the endpoint uniform uh so you\n",
        "00:39:49.680 get to choose so whenever you basically\n",
        "00:39:51.920 deploy to an endpoint you basically get\n",
        "00:39:55.040 to choose how you're basically deploying\n",
        "00:39:57.760 so when you notice uh in my trained at\n",
        "00:40:00.960 vertex right uh i basically specify the\n",
        "00:40:04.720 machine type and i specify the scaling\n",
        "00:40:07.920 i'm in this case i'm saying don't scale\n",
        "00:40:10.079 because i'm not start minimum of one max\n",
        "00:40:12.800 of one but you can obviously specify\n",
        "00:40:14.800 auto scaling here your machine type can\n",
        "00:40:17.280 be any machine temp that's supported on\n",
        "00:40:19.119 google cloud plus any accelerator type\n",
        "00:40:22.160 which you saw when i did the training i\n",
        "00:40:23.839 said train on nvidia t4 so\n",
        "00:40:27.200 yes\n",
        "00:40:28.000 regardless of what the model is you can\n",
        "00:40:30.160 basically do it on all of these\n",
        "00:40:45.599 awesome\n",
        "00:40:46.640 all right\n",
        "00:40:49.680 for inferencing what feature store do\n",
        "00:40:52.800 you use it has to scale\n",
        "00:40:55.040 well we do have this thing called a\n",
        "00:40:56.960 vertex\n",
        "00:40:58.319 ai feature store\n",
        "00:41:05.280 so take a look\n",
        "00:41:07.520 okay\n",
        "00:41:13.839 okay\n",
        "00:41:16.000 uh with automl how do you see this\n",
        "00:41:18.480 disrupting jobs in the data science\n",
        "00:41:20.800 machine learning industry look here's\n",
        "00:41:22.720 the deal though as a data scientist what\n",
        "00:41:25.200 are you spending most of your time on\n",
        "00:41:27.359 it's in understanding the problem it's\n",
        "00:41:30.079 in evaluating models it's in preparing\n",
        "00:41:34.000 data\n",
        "00:41:35.359 i mean\n",
        "00:41:36.319 it's not in creating the model right\n",
        "00:41:38.240 that writing that model could face that\n",
        "00:41:40.240 what everybody does if you're doing\n",
        "00:41:41.440 image classification you're using\n",
        "00:41:43.280 resonant right\n",
        "00:41:44.800 if if that gets replaced by r2ml that's\n",
        "00:41:47.440 replacing a really small part of the\n",
        "00:41:50.160 overall workflow and now automl doesn't\n",
        "00:41:54.160 replace data scientists it basically\n",
        "00:41:56.960 gives you a faster way to go to market\n",
        "00:42:00.560 to benchmark your models etc\n",
        "00:42:13.200 for iot it has been millions per second\n",
        "00:42:15.200 yes uh absolutely so\n",
        "00:42:17.200 now for iot on google cloud folks use\n",
        "00:42:20.079 something called data flow uh and you\n",
        "00:42:22.560 basically do those in in a in a\n",
        "00:42:24.960 streaming pipeline and data flow so look\n",
        "00:42:27.599 up data flow streaming pipelines\n",
        "00:42:31.760 machine learning uh we've had uh for\n",
        "00:42:34.960 example like we've had uh you know\n",
        "00:42:37.920 companies like for example telus does\n",
        "00:42:40.240 the scale that you're talking about\n",
        "00:42:42.240 where they're looking at millions of\n",
        "00:42:44.160 events per second they basically do it\n",
        "00:42:46.640 with like apache beam and tensorflow so\n",
        "00:42:49.599 so take a look\n",
        "00:43:00.319 all right i\n",
        "00:43:01.520 thank you very much uh i think we are uh\n",
        "00:43:05.040 out on times\n",
        "00:43:06.400 uh\n",
        "00:43:07.440 great thanks like for the uh\n",
        "00:43:09.760 presentations and the\n",
        "00:43:12.079 the discussions\n",
        "00:43:13.520 with attendees thanks everyone for\n",
        "00:43:16.400 asking the questions too\n",
        "00:43:18.410 [Music]\n",
        "00:43:19.760 with\n",
        "00:43:20.560 with that we\n",
        "00:43:22.560 conclude these sessions\n",
        "00:43:25.200 i like to have anything to add on before\n",
        "00:43:27.520 we\n",
        "00:43:28.240 nothing thank you very much bill for the\n",
        "00:43:30.079 opportunity thanks\n",
        "00:43:31.599 everyone for the great questions and for\n",
        "00:43:33.760 i know online is really really hard so\n",
        "00:43:36.400 thanks for sticking through it and uh\n",
        "00:43:38.720 and\n",
        "00:43:39.520 you know to\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G_eFkBbM-yW"
      },
      "source": [
        "# Prompt Enginieering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nUrv0LmbNKhO"
      },
      "outputs": [],
      "source": [
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=M_PbbMVg4ME&t=1s\",\n",
        "    chunk_size_seconds=45,\n",
        "    transcript_format=TranscriptFormat.CHUNKS,\n",
        "    add_video_info=True,\n",
        "\n",
        ")\n",
        "\n",
        "transcript_documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=4000,\n",
        "    chunk_overlap=1000,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUpG69AYP9w4",
        "outputId": "5c355bc6-0b72-4c78-80f7-8a49c0716495"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'https://www.youtube.com/watch?v=neBZ6huolkg&t=45s',\n",
              " 'title': 'Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python',\n",
              " 'description': 'Unknown',\n",
              " 'view_count': 369713,\n",
              " 'thumbnail_url': 'https://i.ytimg.com/vi/neBZ6huolkg/hq720.jpg',\n",
              " 'publish_date': '2024-04-18 00:00:00',\n",
              " 'length': 16116,\n",
              " 'author': 'Code In a Jiffy',\n",
              " 'start_seconds': 45,\n",
              " 'start_timestamp': '00:00:45'}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcript_documents[1].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6i5wj_VPuUH",
        "outputId": "8feeddf5-55be-494f-9b6c-44a5365e9fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Script written to youtube_script.txt\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:00:00---->00:00:45]:\n",
            "hello in this video I will teach you how you can build this football analysis project from scratch this project detects and tracks players referees and footballs across the whole video using YOLO YOLO is one of the best AI object detection models right now we will also be training it to improve the output of the outof thee boox models then we'll assign players to teams according to the T-shirt colors that they are wearing this will require us to segment and cluster pixels with K means to only choose t-shirt pixels from a player's bounding box from here we can measure a team's ball acquisition percentage in a match then using optical flow we will measure how much camera movement from\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:00:45---->00:01:30]:\n",
            "one frame to another so we can precisely measure a player's movement afterwards we will work on prospective transformation that will take a camera's distorted Viewpoint of the 3D World and accurately represent the scene's depth and perspective this will help us know how much a player moved in meters and not only in pixels and lastly we will measure a player speed and meters covered this project will cover a lot of Concepts and deal with real world problems so whether you're a beginner or an experienced machine learning engineer this project will totally make your resume shine so let's Jump Right In so let's start by getting the video that we are going to use to develop this project so in gagle I found a data set with\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:01:30---->00:02:15]:\n",
            "football matches that is called dfl Bundesliga data shootout um if you scroll down a little bit you're going to find a lot of videos and if you open any one of them you're going to find that it is like um U an Eagle Eye uh camera point of view like this and the one that we are going to use is going to be 08 F d334 uh remember to log in before you download it it's um it's going to be for free so just click on it and this is the video that we are going to use on it so just uh press here and press download\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:02:15---->00:03:00]:\n",
            "shouldn't take long because it's not a long video so I just have um an empty folder called football analysis this is my project folder and then you can go and make a new folder right here called input videos that we are going to put put our example video in it so in your downloads you should find the video just cut it and paste it in um and then we can open this uh open this project in Visual Studio code so I'm just going to uh open it like this and I'll close all opened all previously opened uh files um so the first step in this\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:03:00---->00:03:45]:\n",
            "project is going to detect the players and the ball inside the uh inside the video and you can uh like write object detection in order for me to um uh explain what object detection real is so um in here you can find that the object detection is just um a neural network that is able to uh draw a bounding box um around an object and it can tell what object it is so you can say that this is a bicycle and this is a person and this is a car and it knows where it is inside of an\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:03:45---->00:04:30]:\n",
            "image um one of the main uh like good uh convolutional neural networks and it's State ofth art right now it is called YOLO um and we can use YOLO uh with ease by using a library called tics so you can just pip install Ultra litics like this uh it it doesn't take any time on my machine because I already have it but uh it should take a little bit more time on your end and we can just play around with this uh YOLO models and ultral litics in a new um file we can call it yolo inference py it's just going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:04:30---->00:05:15]:\n",
            "a file will we play around with ultral litics get to know it and um understand how it works so uh in order for us to use this uh YOLO model which is the state-ofthe-art object detection model we can just uh import it from altic alra litics so from alter litics import YOLO and uh to to to like load the model we can just write YOLO and then give it uh the model that we want so we have uh YOLO like till V8 and V8 is is the latest one that we can use conveniently there is Yolo v9 but\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:05:15---->00:06:00]:\n",
            "it's not convenient yet um so you can have YOLO V8 and then you can write whatever model you want so um uh we have different models for YOLO V8 so you can just write like this GitHub and you can see the different model for YOLO V8 so we have YOLO V8 Nano small medium large and x and U like the difference between them is the number of parameters so if you can see it this is 3.2 million this is 11 million and of course the the higher the number of parameters the higher the accuracy so you can see that the accuracy is increasing but also more\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:06:00---->00:06:45]:\n",
            "uh Computer Resources are being consumed so uh I have um like a good amount of ram I don't have a GPU but I do have uh good RAM and good CPU so I'm going to use the YOLO v8x which is the largest one to have the best accuracy uh if it doesn't work on your end you can try the medium you can try the small and it should all have the same code uh now to run the model um it's going to be uh simple we just run Mo like model which is uh the YOLO model Dash do predict and then give it the path for the video so you can write input\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:06:45---->00:07:30]:\n",
            "videos slash and then you can uh paste in the mp4 file uh you can also specify that we want to save the results so save equal and this save just saves a video an output video of the results and it's by default false because it Returns the results back here back to um a variable and this variable is going to have the detections um in it so but yeah let's let's also put the results here and also have the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:07:30---->00:08:15]:\n",
            "save video equal true uh let me also show you the what the results have so we can just print results of zero uh which is going to be uh the first frame and then we can uh basically Loop over the boxes in the first frame and I'm going to show you what boxes is so results do boxes uh of zero do boxes and then print box and we can also have uh some sort of line in between so that we can understand which is which and then we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:08:15---->00:09:00]:\n",
            "can just run it uh so this is all to it uh for the object detection um this is how we do the inference for it um and the first time it's not going to find the YOLO V8 model locally so it's going to download it but after words it should find it locally and um it it shouldn't download it again so it will be a little bit faster um so yeah after it downloads it it will start predicting the uh on the video and it should take a little bit uh of time to predict on all the video but let me show you uh the prediction uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:09:00---->00:09:45]:\n",
            "outputs right now so yeah you can see that the it it's predict ing on one video and this video has 750 frames and in the first frame it found 23 persons one sports ball and it did it in 500 uh around 500 milliseconds um you can see that it's it's going to go through all the frames and uh if you have a GPU it might like it will be way faster uh but right now I'm just going to cut the video and come back when it's finished okay so now it's done uh as you can see we have the outputs for the boxes and output for the results but we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:09:45---->00:10:30]:\n",
            "can go through it after we see the output video so you can see that after you run the script the model do predict and uh when you save equal true it automatically creates a new folder called runs and in inside it you're going to find the output video so we can open it from here uh you can see it runs then detect then predict so let me explain first the uh the output that you see right here so the output right here is uh consists of for each uh object that the that the uh model is\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:10:30---->00:11:15]:\n",
            "detecting uh the output is uh basically two things uh it's going to be the bounding box which is the B like the Box um around the object which is this one and then uh the class name and the confidence uh but the uh the class name already has the confidence so it's going to be uh only two things as output um so the person right here uh or the uh like the object um and here is for example the object is a sports ball um so the object is going to be um an ID the the model is going to uh output a\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:11:15---->00:12:00]:\n",
            "number and we are going to map it to whatever number that was trained on so um uh and I'll show you where we can get this map and then we also have a bounding box and this bounding box basically defines where in the image the the object is so this can be defined uh in multiple ways we can Define it as let's uh choose a blue color first uh we can Define it as the center point like this like here and it's going to be the X and Y coordinates which is going to be uh this one and this one so it's going to be the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:12:00---->00:12:45]:\n",
            "X and Y coordinates of the uh on the image and then we are going to put the width and the height so it's going to be the width of the bounding box and then the height of the bounding box um another way we can represent the uh bounding box is by those two points uh this point at the uh top uh left and the point at the bottom right so this is another way to represent a bounding box we call this X1 y1 X2 Y2 those are different formats to represent the same bounding box so you like you don't need to um like confuse\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:12:45---->00:13:30]:\n",
            "yourself but uh we are going to stick by this uh it's going to be called xyxy format um this is uh like this format for for me it's straightforward and it defines the bounding box positions um and we can also uh like when we draw it we can just provide those xyxy positions and it will draw a bounding box instead of this one uh so yeah this is what object detection is uh is it's just uh detecting um like a convolution network that detects bounding boxes and classes\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:13:30---->00:14:15]:\n",
            "um and class names and class IDs uh so let's also so see the results and then map the what we see with the uh uh with the output of the results so you can see here that it was running on all the frames first and then I pr like printed out the results so the results here have first it has boxes that we are going to print later on and uh you can see that boxes is an object and we can and we will open it uh like uh in a couple of seconds then we have key points and masks but we are not using those because key points is for like um uh post\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:14:15---->00:15:00]:\n",
            "detection which is detecting joints and mask is for segmentation so we're we're using neither of those so it's going to be none and then the names names is just going to map the uh ID of the user uh to the uh actual name so so zero right here so when the model predicts zero it means person so it goes in the frame and writes instead of writing zero it writes per so that the so that we can understand it basically uh one bicycle two car and so on and so forth you can see the whole U like 79 classes here then we have the original image in pixels you can basically ignore that and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:15:00---->00:15:45]:\n",
            "yeah you have the save Direct the and other statistics about the uh runtime so yeah you can ignore those basically uh then we have the bounding box so this is one bounding box and let's see what does it have so first off it has uh a CLS which is a class ID it is zero the first one is zero so zero if you can scroll up you can map it to the name so zero means person so this bounding box is for a person and then um you'll have the confidence of it so how\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:15:45---->00:16:30]:\n",
            "confident the model is is um is basically U saying that this is um uh this is a person for example so if the confidence decreases like this one then the model is quite not certain whether this is a person or not uh but the higher the like the higher the confidence like this 0.8 0.7 um the more like the more likely it is it's going to be an actual person and not going to be for example um a background or anything like that um and afterwards uh you're going to find uh something that is called\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:16:30---->00:17:15]:\n",
            "track um we are going to cover tracking later on um but right now we don't track we're just predicting so it's going to be false and then we have the bounding box we have first the bounding box which is XY width and height which is going to be the center point which is uh what I showed you before and then width and height you also have different other uh like ways of representing the same bounding box but we are going to use xyxy which is going to be the far end of the uh rectangle like I showed you and yeah so this is one bounding box and um yeah we have also other bounding\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:17:15---->00:18:00]:\n",
            "boxes as well I am looping all over them uh at the first uh in the first frame and this is the output of it so yeah back again to our results right here um so uh in our results right here uh we have multiple uh uh like inconveniences so the first one you can see that the sports ball uh right here is being detected but after a while right right now it's not being detected and it's going to be very rarely detected afterwards like you can see that it's almost being ignored most of\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:18:00---->00:18:45]:\n",
            "the frames so we will need a model to basically uh protect the sports ball or this uh uh football a little bit better that's one uh thing that we need to do so that we can like analyze ball acquisition by players and we can also analyze different things about the ball um and yeah having very few detections is going to hurt us so uh increasing the accuracy for this is important and then another thing is that we also see that people outside the cord that are not players like this one and this one uh this one are being detected um it's going to be good for us if the model out of the box can ignore them so that we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:18:45---->00:19:30]:\n",
            "don't write code to detect the lines of the court and then um ignore the people that are outside the lines um so yeah if the model can does do this for us automatically it will save us a bunch of code and also the referees right here so for example this is a referee that is wearing black like this one uh if the model can also differentiate between referees and players that would be also awesome um I know that we can do it by hand like we can uh see the color that the player is wearing uh sorry the person is wearing and say if it's black then this is a referee uh but yeah it's going to save\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:19:30---->00:20:15]:\n",
            "us a little bit of code if we if the model can do this for us basically so for this I have found a data set on Robo flow on Robo flu um that is called football player detection image data set and roof flow is um like it's basically a library uh a python library that you can use to download any data set that you want and also the website helps you to um upload data sets and um annotate them even um and yeah so uh and it also provides you to download the data set in different\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:20:15---->00:21:00]:\n",
            "formats so that you don't have to deal with wrangling the data and putting it into different formats to accommodate different um uh let's say models so let's open one of the images of the data set and see what we get so we get the player right here so all players are having a bounding box uh the people outside the court are not being detected like you see here uh the referee is being also detected like this one um and is like it has a different class so this is a player and this is a referee also the sideline referees are also being annotated uh the ball is also being\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:21:00---->00:21:45]:\n",
            "annotated and the goalkeeper is being annotated as a goalkeeper and not a player now this is a very good uh annotations for us it's going to help us with out of the box understanding which is referees and ignoring the people outside so yeah this is exactly what we need um this data set is only 612 images um this is um a little bit low for us to start training on it uh but um yeah uh it it it will help us to it will be good enough for us to for our for our project so yeah so right now our task is to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:21:45---->00:22:30]:\n",
            "basically um run a training um a training trial on YOLO and fine-tune it on this data set so that we can have this neat output um yeah um and before we proceed you can also create um uh an account with roof flow so that we can uh download this um download this later so in order for us uh to to start training the model we can create a new folder called training like this and we can train it in a notebook you can write football training\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:22:30---->00:23:15]:\n",
            "YOLO V5 uh Dash uh ipynb uh ipynb is going to make it as a notebook and I chose YOLO V5 because it's going to have like it has the best out of the box accuracy at detecting the ball which is the main issue that we have so the first thing is that um if you didn't install Ultra litic we can just pip install Ultra litics right here and uh you can also pip install roof flow which is the library that we are going to use to install uh to um uh download our data set\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:23:15---->00:24:00]:\n",
            "Robo flow um afterwards we can get the data set so you can get uh data set and then uh we can um just to get the data set we can open up proof flow uh click on Yo V5 because this is the model that we are going to use make sure that show download code is being checked then press continue and then uh you are presented with a code snippet to download the code automatically for you so you're going to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:24:00---->00:24:45]:\n",
            "have an API key so uh I am going to uh blur mine but yeah your API key is going to be different than mine so you can just paste it in like that and we can remove the first line because we already installed it and yeah um we can now run this so you can see right now that it is downloading the uh football data set so now it's finished and you when you can go back and refresh it you can\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:24:45---->00:25:30]:\n",
            "see that football player detections D1 is going to appear this folder will have our data set and you will have the testing training and validation sets you can open the validation for example and uh you can see the images right here uh it's the same type of images for the um video that we are trying to predict the same camera angle and everything um and uh you can also see the labels let me also explain the labels a little bit so the first one is going to be the class ID and this is what is going to say that\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:25:30---->00:26:15]:\n",
            "this is a person or referee a player or a ball and two here uh is just the um class ID so if you can see here we have the names so 0 1 2 so this is the player so two is for the player uh then it is followed by the bounding box um and then this bounding box is going to be the X Center y center then the width and height um relative to the width and height of the frame so 0.32 is just like uh it's telling you that it's almost 32% of the way of the width of the frame um but yeah you don't have to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:26:15---->00:27:00]:\n",
            "worry about this um but I just wanted to to show you how what are the labels um of this model and as you can see it's just the bounding box and the class name uh now you can close it and you also have the data. yaml that is very important at um like telling you what are the class names and where the images are so you can also close this and yeah right now you have the data set right here so you can have the data set. location for example and it will tell you where this um where this data set resides in our local machine um\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:27:00---->00:27:45]:\n",
            "one more thing before we proceed with this is that um we need to uh we need to move this uh data set into another folder with the same name so football players detection -1 uh we need to put it in also another folder called football players detections -1 and put the test training and validation sets there um this is what the uh training code expects and it crashes without it so uh right now we just um we just need to uh do this simple step so I can do it manually just like that like I create a\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:27:45---->00:28:30]:\n",
            "new folder that is called football um players detection -1 and then I move the testing right here and then the training and then the validation you can do it manually if you want uh but I want to make the code reproducible so I'm going to do it with python so what I'm going to do is just import uh a library called shuttle shuttle helps us to move and copy files and folders in our uh in our machine uh using python so we can have shuttle dot move and then we can uh provide football players detection then slash\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:28:30---->00:29:15]:\n",
            "train and where do we want to get it like this is what we want to move where do we want to move it we want to move it football players detection D1 then football players detection D1 that is all what we want to do um again this is just a requirement uh for the uh training code for uh alter litics so I'm going to do the same thing for the test and the same thing for the validation like that um and you can just run it so now it's running I'm going to also create um training uh training\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:29:15---->00:30:00]:\n",
            "section so now it's done and uh to start training all we have to do is we are going to write a terminal command with ultral litics and we write YOLO then we can tell it that the task is going to be detect uh because we're detecting then we are going to set the mode for training so we can say mode equal train and then we can specify the model that we want to train um we can train uh YOLO V5 and again also y V5 has small Nano large and extra large I am\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:30:00---->00:30:45]:\n",
            "going to train the extra EX extra large version and uh this is like the x is the for the extra large uh then I am going to specify the data set location so it's going to be data set. location and they need this file which is data. yaml so I'm going to also put it right here data DOL um then we can specify the EPO which is the amount of iteration that the model is going to do to learn from the um to learn from the whole data set and then we can specify also the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:30:45---->00:31:30]:\n",
            "image size so image s uh s and Z for the image size and we can specify it to be 640 um now yeah this is all what we have to do to start the training uh to start the training code and to to make our own model so I don't have a GPU on my machine so I will need to train on uh Google collab uh Google collab provides you um with a couple of hours of GPU training per day so uh we will be able to utilize that so in order to use that you can just go to your uh Drive uh Google Drive and then create like a right click and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:31:30---->00:32:15]:\n",
            "then more and then you can create a Google collab uh um like notebook right here uh so after you create a new Google collab notebook um you can just copy paste the cells there are a few amount of cells and also the first cell will install the requirements um so yeah you can just copy paste it uh or you can just uh press file and then uh upload notebook notebook so for me I already did that um I already copy pasted the the code that we written together and I also have run it so we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:32:15---->00:33:00]:\n",
            "don't have to wait but training takes a little bit of time so be patient um this has to be 100 EPO and yeah uh so you're going to find their training steps like every epok and uh the first ook has uh 1.4 uh loss and then you can find the loss is uh decreasing steadily um so after it finishes like this like here uh you're going to also have the runs folder and if you don't see it just press the refresh button right here uh this would uh refresh the um the uh the folders so that it will have the latest uh uh output so you have\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:33:00---->00:33:45]:\n",
            "the runs the protect then train and then you have the weights you have the best weight and the last weight you can download them both you can just press uh those uh three buttons right here and press download um or if you want to move them to your drive you can press this button to mount your drive it's already mounted on my end so it's un mounting it right now but you can uh mount it and then you can uh copy it so basically run a copy command like copy um this uh this file which is in runs detect train weights and best. PT uh two it's going to be the drive so your drive my drive\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:33:45---->00:34:30]:\n",
            "and then you can put it wherever you want so you can just paste it right here but I have chosen uh to paste it in the collab uh folder uh football analysis project and then um a folder called YOLO V5 so I copy pasted it um the best and the last and then you can uh just download them um so after you download them so you can go back again to your project and create a new folder called models like this and here you can paste in the models that you downloaded so just paste it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:34:30---->00:35:15]:\n",
            "in uh so now you have another folder right here that is called models that have best and last and now you can close in this notebook just save it and close it and yeah um that is it uh you can also close the training folder so we're done with the training so right now we just want to see the output of this model so all what we have to do is that instead of giving it yolo VX which is give it the models the/ best. PT uh which is this uh this uh file path which\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:35:15---->00:36:00]:\n",
            "is the best file path and uh just save it then uh clear the output and run it again and it should also take uh a little bit of time but you can see that it's here it's doing the same thing but notice that it is predicting balls goalkeepers players and referees instead of person and sports ball so it has different class names um but yeah again I'm going to uh cut the video and come back when the run is over okay so now it's done so just like the last time it's in the runs now we have predict two which is the latest predict that we have so we can open it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:36:00---->00:36:45]:\n",
            "like this runs detect predict two and then open this so you can see right here that the people outside the court is not being detected the players are being detected as players the referees right there the three referees are being detected as referees the ball is also being detected quite well uh you can see that it is right now it's being detected a little bit more than it used to be um especially when it's it is hit you can see that it it's at least having um detections in this part we didn't have any detections and yeah so yeah that\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:36:45---->00:37:30]:\n",
            "this is how you can use your uh model for like the the the fine-tuned model for predictions and like like just like the model that we have um we also have the um the bounding boxes and the the uh results just in the same format like nothing has changed um and currently right now we just want to write code to make the annotations on the video a little bit neater uh I used to have circles uh like below the players so that we don't have those clunky uh bounding boxes and annotations and uh we had also triangles\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:37:30---->00:38:15]:\n",
            "for the B for for the ball instead of this uh clunky one um because there's a lot because there's a lot happening in this Frame and in the video um having like uh this type of annotation is going to uh overlay a lot of things so for example this player is we can't see it like we can't see from the annotations the referee and the players and it would be better uh for us to have uh the different type of annotations uh I showed you the annotations at the beginning of the video but let me also uh show you what I mean um so this is what I mean instead of having those bounding boxes we just have circles uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:38:15---->00:39:00]:\n",
            "but it's going to represent the same thing and instead of having a bounding box uh on a ball we just have a triangle on top of it uh so this is this is just what I mean right here you can see that that you can like you can see a lot uh better and you can see players a lot better so we're going to uh do those annotations next um so here right now we're just going to close YOLO inference now we know how ultral litics works and how YOLO works so we can do a\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:39:00---->00:39:45]:\n",
            "main.py and we can then uh Define Main and then print hello world and we can also have if name equal equal Main and then we call the uh main um so this is just a hello world application that we are going to build on top of you and yeah you can see the hello world is being done so the first step in uh like doing this and uh creating this project uh we can create a new folder called utils and inside of it we can create a video uh utils.py uh this video utils is going to have the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:39:45---->00:40:30]:\n",
            "utilities to read in the video and save the video and in order for us to do that we are going to use a library called CV2 now CV2 uh you can open it like here open CV um and you can just pip install and you will have the PIP install command to install open CV I already have it but it is just straightforward as copying and pasting it in the terminal so we now have the uh we now want to have a function called read video um this video is going to take in a video\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:40:30---->00:41:15]:\n",
            "path and basically return um the list of frames for the video so a video is just a couple of images uh like presented one by one so you can see uh this is an image this is an image this is an image so uh it is being presented at 24 images per second and we can um call those images as frames so um so this is 24 frames per second and in order for us to uh read in the video we can create um a video capture um object with CV2 and I am going to uh take in the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:41:15---->00:42:00]:\n",
            "predicted code from GitHub copilot uh so those predictions were from GitHub copilot if someone asks uh so it's a video capture then it initializes a an empty list of frames then it Loops over while true uh it Loops over it and it reads in the next frame so uh it reads in the next frame put it puts it right here and it also returns a flag whether there is actually a frame or the video has ended so if it is um um so if it uh so if it is false then the video has ended and we should break uh out of this Loop but if it's\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:42:00---->00:42:45]:\n",
            "true then we can just append the frame to the list of frames and then return it so this is just the list of frames for the video and while we're at it let's also Define the save video function so save video and it it it takes the output uh video frames which is going to be also a list of frames but it it's also going to take an output video path and uh for this we are going to also uh maybe take in the um the same uh like the GitHub co-pilot uh recommendations but we are going to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:42:45---->00:43:30]:\n",
            "change it a little bit so the first thing is we Define an output format and it's going to be an X vid um then we are going to define a video writer that takes in the video path this is just going to be a string then the um uh uh then the uh the output video type and then it takes in the number of frames per second right now it's going to be 24 frames per second then it's going to take in the output uh like the frame width and the frame height and in order for us to do that we can just uh take in the output video frames of zero do shape of one let me write shape\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:43:30---->00:44:15]:\n",
            "correctly and we can also copy paste it right here uh but instead of shape of one uh we are going to put shape of zero and this is going to be the um the uh X and Y position like the width and height of the frame and yeah so what we going to do now we are going to Loop over each frame and basically write the frame to the video writer and that is it so let's test it out so in the in the main uh we can um so yeah before going to the main we can just in the utils uh create a new file called\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:44:15---->00:45:00]:\n",
            "init.py and this init exposes functions from inside the utils to outside the utils so uh right here you can write from um do video yours import uh read video and save video and here you can just write from um you tills import read video and save video uh now let's read in the video so we can just read a video and we are going to have video frames which is equal to read uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:45:00---->00:45:45]:\n",
            "video and we are going to give it the path of the video that we want which is going to be input videos slash and then you can copy paste this um like mp4 file the name of it so uh right like this so input videos and then the mp4 file and let's also save the video so we can uh know that it's working or not so we can save video and then we can also save video give it the video frames and then uh output video and call it output.\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:45:45---->00:46:30]:\n",
            "AI uh output uh video.avi and yeah this is it so uh before running just create the folder that is called output videos so output videos and then uh you can run it um it shouldn't take long to just read and uh write the video so it it should be a a second or two yeah that is it and you can open up the output videos like this and uh open it and you're going to find the same video being outputed again um so yeah nothing special here\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:46:30---->00:47:15]:\n",
            "but now we now we know that there read video and the save videos are working correctly so now what we want to do is we just want to uh write our uh detection and tracking and uh before we can proceed we can I can um I can uh explain what tracking is so um for example um um in this video you're going to find that uh you like in the output videos you have bounding boxes like this one um so like this um uh around the object so every object has bounding boxes like this and this is the current\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:47:15---->00:48:00]:\n",
            "frame so this is the current frame and the bounding box is just um X Y then X then XY uh it's just going to be uh a number a number a number a number and uh and you are also going to have whether this is a person or not um but yeah this is this is going to be it for the detection and you are going to have the same out like the same output uh Val like the same output format for for this also so it's going to be x y x\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:48:00---->00:48:45]:\n",
            "y and it's going to be a person um the problem now is is that uh in the next frame so if you if you go out and go to the next frame like this this Frame and you try now to uh predict the output so let's let's also have it now in red um so you have those bounding boxes right now and the previous bounding boxes were right there like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:48:45---->00:49:30]:\n",
            "this like this for example so uh the blue ones are the bounding boxes that I drew in the frame before and the red bounding boxes are the current bounding boxes so the idea right now is we have nothing that tells us that this bounding box is the same as this bounding box because we only have the XY and then XY uh nothing um nothing is telling us that this bounding box is the same as this bounding box uh now we can say that um the closest bounding box is the bounding box that is the same uh user so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:49:30---->00:50:15]:\n",
            "for example we can say that because in the last frame um this blue one is closest to this uh red one uh so this is the same player basically uh so we can do this we can have this um uh we can we can do this uh like detections but yeah tracking uh or like assigning the same bounding box or the same entity to a bounding box across multiple frames is what we call tracking so we just want to track this uh user and tell this uh player that um he was there and then he is now\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:50:15---->00:51:00]:\n",
            "here um so yeah the problem is is that if you just get the closest bounding box then maybe uh this bounding box right here like this bounding box um um is closest to the blue one instead of the red one so um so a Tracker a smart tracker would also predict the uh movement so the um the trajectory that the user is uh the the object is moving in um and it would also uh like take in some visual features so this uh this user right here was wearing white and this user is now wearing white and it is\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:51:00---->00:51:45]:\n",
            "close enough and this user is wearing green so most probably he is going to be this user so this is what tracking is and uh it just assigns a like uh bounding boxes an ID so that we can understand what like where was the user or the object before in the frame and now where he is in the current frame so in our case we are going to use um a Tracker called bite tracker and um this bite tracker is going to be uh more than enough for us to uh track objects across frames so in order for us to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:51:45---->00:52:30]:\n",
            "write this tracker it will need to have the detection first and then we will need to track it so it will need the bounding boxes and then it would need to match the bounding boxes to an ID um so yeah basically we can create a new F new folder called tracker uh trackers and like we did in the utils we can create the init uh that exposes the classes and functions to outside the tracker um trackers uh folder and then we can create a file called tracker. py like that and then we can create a class called tracker\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:52:30---->00:53:15]:\n",
            "like this and we can then create the init uh this in it gets called when we initialize the um class and uh then we can also have a function called get object tracks object tracks uh this is going to take in the self and then the frames so inside the init we just want to load in the model and load in the tracker so we can just uh first of all\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:53:15---->00:54:00]:\n",
            "import from ultral litics import YOLO like this so we can go here and make self do model equal equal YOLO and then uh give it the model path we can get the model path from here model path uh from the init so this is the model that we are going to use um so right now we just want to get the detections first and then from the detections uh we can get the tracking so detections is equal to self. detect frames and then give it the frames\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:54:00---->00:54:45]:\n",
            "uh we don't have this function yet so we can write it so it's going to be uh detect frames whoops uh so it's going to be detect frames and then we are going to uh basically this code is wrong uh GitHub copilot predicted it wrong but uh we can write the correct one right now um so basically when we when we used to uh write it in the YOLO inference we did it like this self. model. predict and then we just gave it the um basically the um uh the video path and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:54:45---->00:55:30]:\n",
            "now we can just give it the frames um so yeah we can have the results right here or the detections and then return detections and uh this would basically work but um I would also add a batch size for it so that we don't go into memory issues and things like that so batches is going to just instead of uh predicting uh predicting it on the whole frames uh we just gave it 20 by 20 we just give it 20 frames by 20 frames so we can uh Define a batch size of 20 then we can say uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:55:30---->00:56:15]:\n",
            "detections is equal to to an empty list then 4 I in range zero till length of frames then batch size and this is basically going to have I to Z and then the next iteration is going to have I is equal to 20 because it's going to increment by the patch size then the next time is going to be 40 so on and so forth um then we are going to have detections then batch detections of the batch then we are going to get the uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:56:15---->00:57:00]:\n",
            "model uh predict so U we are going to choose the frames um from I till i+ batch size like this and then we are going to get the confidence to be uh 0.1 so this is the minimum confidence that we are setting right now and um yeah I saw that 0.1 is uh good enough for detecting um um the whole uh like a lot of objects and without having many false detections and after we get the detections batch we just add it right here uh to the uh whole detections\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:57:00---->00:57:45]:\n",
            "list so right now we just have the detections and as I showed you before in the YOLO inference it has the boxes and it has the um um uh it has the class names and boxes and yeah it's it's in the same format basically so after this we just want to run a Tracker um actually we can actually run a Tracker right here we can instead of model. predict we can actually run model. track and it would uh provide a track ID uh for the objects um but um we wanted to I wanted to also\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:57:45---->00:58:30]:\n",
            "show you something before we proceed is that in the runs detect and predict to uh you can see that uh here the goalkeeper is being detected as a player and it keeps switching between player and goalkeeper and assume and I I assume that this happens because of the small data set it's just 600 uh images so we are going to um treat the uh goalkeeper as a normal player because we are not doing any special statistics for the goalkeepers um so we can just treat them as a player so in order for me to just uh not uh like overwrite a goalkeeper with a\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:58:30---->00:59:15]:\n",
            "player uh I won't be able to use the track here right like this I'm going to use the predict and then uh override the goalkeeper with the player and then run the um tracker on it so in order for us to run a Tracker uh after the detections we are going to use a library called supervision so import supervision as SV and this will have the tracker so uh like I said we are going to use the bite tracker so we can have the tracker is equal to SV do sv. bite tracker uh bite track like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[00:59:15---->01:00:00]:\n",
            "this and this is the tracker that we are going to use um so before we start tracking uh we just need to override the um goalkeeper with the player so let's do that right now so for frame num and uh detection in uh enumerate detections so uh basically what I'm doing right now is just I'm looping over the detections uh one by one and numerate here just puts in the index of the uh list so it's going to be 0 1 2 3 4 so on\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:00:00---->01:00:45]:\n",
            "and so forth um then what we are going to do is we are going to take the class names like this and uh it's going to take detections do names and we can also have the class names like this and inversed so if you if you saw it before uh like when I showed you this was like this this was like um zero was a person then one was goalkeeper uh and whatever so and so forth uh this was the format of it and I\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:00:45---->01:01:30]:\n",
            "want the inverse of it so I want to have person then it maps to zero and uh and ball maps to for example two and so on and so forth this is just going to be a little bit convenient for us to uh know which is which quite easily without uh having to uh search anything so uh in order for me to do that I'm just going to get 4k and V which is key and value in uh class names do items and I am just going to switch them instead of key and value I'm just going to put the value as the key and the key as the value like this um then what we want to do is that\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:01:30---->01:02:15]:\n",
            "we want to convert um convert the detection to uh supervision detections uh supervision format um supervision detection format and the way that we are going to use that is is that we're going to um write the detection supervision is equal to SV detection uh detections from uh ultral litics and then we are going to provide the detection like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:02:15---->01:03:00]:\n",
            "this um now I can show you what the detection uh has so I can just do like this right here and uh in order for me to not run on all the frames I am just going to break here um like break here like this and we are going to remove the brake when we are ready to run on all the frames so right now I just want to expose this and show you the supervision uh format so in the init in The Trackers I'm going to write the from trackers tracker uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:03:00---->01:03:45]:\n",
            "import tracker like this and then we can write from trackers here which is this folder um import tracker like this um so yeah so we can so first we can initialize the tracker and after initializing it we can run the prediction on it so let's initialize\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:03:45---->01:04:30]:\n",
            "tracker and then we can write tracker is equal to tracker and we can give it the model path which is models SL best. PT and uh then we just want to run the tracker. getet object so we can get tracks and tracker. getet object tracks and provide video frames and yeah that would be it um so let us clear the output then run again and you should see uh you should see the detections uh supervision\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:04:30---->01:05:15]:\n",
            "uh in in supervision format so here it is the supervision format it is in a loop so it will uh keep on repeating but I will take only the last frame and show you guys so detections is going to be an xyxy format and it's going to be a bounding box which is going to be starting from Pixel 500 300 uh 539 um and 700 uh 01 till 600 and 700 and each one of those is going to be a different bounding box with a different object afterwards we have mask which is not going to be used also confidence\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:05:15---->01:06:00]:\n",
            "which we're not going to need and then we have the class IDs so for example two here means that the first object is going to be two and um I can uh print the um I can print out the class names as well so that we can see uh what is two basically so whether two is going to be a person or um a gorke keeper a player or a ball but each bounding box right here has its corresponding class ID 3 2 1 and zero and this is the supervision format um let me now also print to you\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:06:00---->01:06:45]:\n",
            "the class names like like here and uh yeah let me also break so that we don't need to print out all this again it ran and you can see the um the whole list of the classes so right here you can see that zero is ball one is goalkeeper two is player and three Is referee and you can simply say that two here which is the first bounding box is a player and three here which is the second bounding box uh is going to be a referee uh so on and so forth and you can see that this is the class name which is zero and ball and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:06:45---->01:07:30]:\n",
            "class name inverse is going to be ball as key and then zero as uh value uh so let's now convert the goalkeeper to a normal uh player and what we are going to do here is that that every time we see one right here in the output uh right now we have um this one we are going to switch it with a two so this is this is what we are going to do um so um so convert and we are switching it with a two because it's it's the player uh ID so uh convert goal keeper to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:07:30---->01:08:15]:\n",
            "player object and for object index in uh and a class ID in enumerate and it's going to be the detections uh supervision like this uh dot class ID uh like that so we can check if the CLS um class names um of class ID is equal equal to goalkeeper like that so basically I'm checking if this\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:08:15---->01:09:00]:\n",
            "uh what We're looping over so We're looping over this so if I'm checking that two is going to be the class name of goalkeeper then I'm going to switch it um but uh right now we also only have this so we're going to um yeah um run it so give me a minute so here we are going to run detections supervision. class ID of object index then it's going to be uh class names inverse of person and this is going to return two um and and I'm not\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:09:00---->01:09:45]:\n",
            "hardcoding it to two and one and things like that um because this way is going to be more robust so whatever the class names that you got and whatever data set that you have you can just run it and it will work fine um so yeah this is going to replace this one with a two basically so if we run it again uh you're going to find that uh this one is has disappeared and it's now a two uh whoops I I did a small mistake right here uh this is not going to be a player this is going to be uh sorry this is not going to be a person it's going to be a player so you can save it clear the output and run\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:09:45---->01:10:30]:\n",
            "again so yeah uh right here you can see that the one has disappeared and is now replaced by a two and yeah that is going to be it so now we are ready to to uh write our tracker so we can write Here track objects then we can write detections with tracks then it's going to be self of tracker dot um update with uh whips it's\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:10:30---->01:11:15]:\n",
            "going to be up date with yeah detections and yeah we can give it the detection uh supervision and that is it so this is going to add a Tracker object to the uh to the detections so let me uh let me just print it out and let me also not break to show you how we can track across uh uh frame so you can just run it again so the tracker is now running fine and you can see that um we have a new object right here which is called track ID and it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:11:15---->01:12:00]:\n",
            "tells us that the uh player number one is uh track ID one and and this bounding box of two is uh uh tracker two then tracker three and you can see that those trackers right here are going to be consistent throughout the frames so um it's going to match this uh bounding box with a number uh which is going to be a track ID right now you can see that it is in ascending order but note that when the users move this is going to switch according to the bounding box position so if this user for example comes here\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:12:00---->01:12:45]:\n",
            "then this one should be here and it's not just um um a number in ascending order so it's it's going to assign it correctly um so yeah this is the tracker and uh right now uh the tracker is now finished and uh we just need to put this output in a format that we can utilize easily so I will choose a format which is going to be a dictionary of lists which going to have players referees and balls in different objects so that we can uh reference them quite easily so let me show you what I'm talking about so I'm going to make a tracks object right here open it and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:12:45---->01:13:30]:\n",
            "then have the players as an empty list I am going to also have the referees uh like this as an empty list I am also going to have the all as an mty list and for each uh like for each track and for each frame I am going to have the tracks like this of players then we append uh a dictionary um yeah so a dictionary and this dictionary is going to have a key for the um for the track\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:13:30---->01:14:15]:\n",
            "ID like this one and the value is going to be the bounding box and we are going to do the same for the referees and for the ball and we are going to Loop over each detection with track right now which we are going to do Loop over each track so we are going to have the frame the detection in detection with uh tracks then we are going to extract the bounding box which is going to be frame detection uh dot uh frame detection of zero uh do two\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:14:15---->01:15:00]:\n",
            "list uh of zero because uh the first one is detections so uh this is the zero then we have one a mask then two confidence then um three is going to be the class ID uh so we can have here the class ID which is going to be the frame detections of uh three like this and we are also having a track ID we want to get the track ID so the track ID is just after the class ID and it's going to be the frame detections of four like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:15:00---->01:15:45]:\n",
            "this um now we just want to add um like add tracking for both for the players and referees uh but not for the ball uh the ball is just going to be one ball so no need to track it just the bounding box is going to be enough and yeah we just we just know that the bounding box is the same uh it's it's only one across frames so uh we don't need to do it so we can simply say if class ID is equal equal to class names uh inverse of uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:15:45---->01:16:30]:\n",
            "player like this uh then uh we just uh get the tracker so tracks of players of frame number of uh then track ID this Frame number is going to be the list index um so right now it's going to be zero because right now we only have one uh if if we are looping uh over it one by one so this is for the frame number zero this is going to be zero then the track ID which is going to be one of those whoops one of those and then we are going to put a the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:16:30---->01:17:15]:\n",
            "bounding box but because we might have multiple other uh pieces of information other than the bounding box for the user we are going to put it in a dictionary and just put it uh like this bounding box now let's do the same thing for the referees so we have the refere like this and for this referees uh we can just uh put it right here and it's going to be the same so frame number track ID and put it as a bounding box and right now we just don't want to Loop over the detections with tracks for the ball so we are going to Loop over\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:17:15---->01:18:00]:\n",
            "the uh detection uh like without the tracks so in detection supervision and then we are going to also extract the bounding box like this um which is is going to be frame detection of 0.2 list then we are going to also extract the class ID which is going to be frame detection of uh three um then we are going to make if the class ID is equal to the ball like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:18:00---->01:18:45]:\n",
            "that um then we are going to add it uh to the uh ball CHS so like this we are going to just copy paste it like this ball and instead of having the track ID we are just going to hardcode it to one because it's only one ball now now we are done with the tracker um now we just want to see the output um but yeah before we just run the output again um this is going to be the uh our final uh output right here uh like from this uh from this function uh right here we just want to return the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:18:45---->01:19:30]:\n",
            "tracks which is going to be a list of uh dictionaries a dictionary of list of dictionaries um so yeah uh let me also tell you again what's inside of it so this is going to have um like uh for each frame so uh this Frame number like let's say that this is going to be number one uh like it's going to be this then it's going to also have uh the same for track ID 1 like this and then it's going to also have track ID two like this or maybe 21 uh no problem and after\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:19:30---->01:20:15]:\n",
            "after all this after like this is only one frame this is the dictionary of only one frame so it's also going to have the same set of bounding boxes maybe um another user came in like 10 and this user moved out of the frame so it's not there anymore so this is for frame one and this is from for frame zero uh from frame zero and this is for frame one and it's going to be so on and so forth till we finish the frames so yeah this is going to be the output uh right now we run the detections and then the tracker but I would like to also save the results so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:20:15---->01:21:00]:\n",
            "that we don't need to When developing we don't need to run it and wait for like 10 minutes for it to finish so I would uh save this uh tracks object as a pickle file after we finish and if I found it I just can read it and not uh run the whole thing again so let's also add another parameter right here which is called uh read from stop which is going to be false by default which is going to make us uh run uh this uh from scratch and then we can\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:21:00---->01:21:45]:\n",
            "have the stop path uh stop path is going to be just the uh output of it so uh the tracks so basically uh what we want to do is that uh we want to see if the stop path is not known then we can just save it so we can we we'll save it with a pickle um so we can just import pickle right here like this and then we can just say with open stop path then uh WR bites as F then we can pickle dump the tracks so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:21:45---->01:22:30]:\n",
            "we're we're going to uh save it right here so yeah this is how we uh this is how we are going to save it and let's also put here the reading and the first time that we run it uh it's probably not going to read it because it's not there uh but we can say if read from stop is equal to True like this and stop path is not NN and also let's check if the stop passing this so import OS so we can also\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:22:30---->01:23:15]:\n",
            "see that um o s. path. exist stop path and if it exists we can just open up the stop path then load the tracks and then return it then after we return we don't need to run this again so yeah that is going to be it and now we can remove The Brak so let's now provide also uh read from from read from stop is equal to true and let's also provide a stop path which is going to be stop path which is going to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:23:15---->01:24:00]:\n",
            "be equal to stops SL track tubs dot pickle yeah this is going to be underscore not uh bracket and yeah and right now it's it's set to true but it's not going to find the um the pickle file so it's going to run it so let's also create the tops folder right there and then run it so let's just make sure that it's going to run and um yeah it's going to take a while so I'm going to cut the video and come back when it's done\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:24:00---->01:24:45]:\n",
            "so now it's finished and uh you can see that it uh took a while to run but uh we can just uh run it again and make sure that it's uh reading from a file and not running so I just run it again and it should take only a couple of seconds to finish and uh yeah just give it a minute yeah so now it's finished very quickly because it went and read the file instead of just predicting it again uh so right now we just have the predictions in the tracks uh but we just want to see it and visualize it like we uh saw the bounding boxes and instead of the bounding boxes we just want to have\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:24:45---->01:25:30]:\n",
            "those circles so let's now uh write the code to have the circles in place and let's close down this function and this function and let's now write Define draw annotations and it's going to take in self video frames and it's going to also take the tracks uh then it will have the output uh video frames and this is going to be the video frames after drawing the output on uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:25:30---->01:26:15]:\n",
            "then we are going to to run the um Loop over each uh frame so we can have frame number then frame and enumerate like this then video frames and then uh we can uh start the drawing process first thing we can copy it frame. copy so that we do not uh like um uh pollute the the uh video frames that are coming in uh we can just copy it right here so that the original uh list is not uh drawn\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:26:15---->01:27:00]:\n",
            "on and uh right now we just want to draw a um uh a circle uh beneath the user so let's extract the player dict like this and uh we can get the tracks of uh layers of frame number like this and then we can do the same for ball and referee so ball and refer uh referees like this so This Is referee and this is also going to be the ball and now we have the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:27:00---->01:27:45]:\n",
            "uh dictionary of the uh objects or the players and the balls and the referees um for this Frame so um we can start by drawing players basically so for track ID um and player in player dict like this dot items like this um then uh what we want to do is that uh we want to draw an ellipse basically so we can have uh frame is equal to self Dot Door\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:27:45---->01:28:30]:\n",
            "ellipse and ellipse is like a uh is like a circle but with different uh diameters in um like horizontally and vertically uh I'm going to show you uh guys what it is in a bit uh but the draw ellipse function takes in the bounding box like this and it also takes in a color um and let's right now give it a color the color is in BGR so you can give it a color of red right now so uh 255 and we can also give it the track ID\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:28:30---->01:29:15]:\n",
            "so like this uh so this is the draw ellipse but we don't have it so we need to write it so we can Define it so Define draw ellipse then it takes self uh then frame then bounding box then color and then uh track ID which is uh that is correct um and then we want it to be put at the bottom um like at the at the bottom of of the uh bounding box which is the Y2 Y2 is at the bottom so we can just do it like this BB box of three and we cast it to an\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:29:15---->01:30:00]:\n",
            "integer um we also want to have it to be Center like at the middle uh of the X so we want the center of the um we want the center of the uh Circle to be the center of the bounding box uh x y at least so we can go to utils uh and we can create a new uh file called bbbx utils.py so bbbx utils.py and we can create a function right there called get Center of box so Define get Center of BB box and you can give it the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:30:00---->01:30:45]:\n",
            "the VB box which is the bounding box then you can give it the X then y 1 X2 Y2 and um uh you can have it as the BB box right there uh then we just want to have the center of it uh so you can just do it like this this is going to be the center of X and this is going to be the center of Y but before we return it we just going to cast it as an integer like [Applause] that and while we're at it I will also need to get the width of the bounding box so let's also have the uh function that is called\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:30:45---->01:31:30]:\n",
            "get with uh get sorry uh bounding box width like this that is going to take a bounding box and it's going to return it's width so return BB box of two minus BB box of zero which is going to be X2 - X1 this is going to be basically it uh so let's expose those function outside the utils from dot uh BB box utils import get Center of bounding box and get bounding box width we can then go to tracker and then um we can import\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:31:30---->01:32:15]:\n",
            "CIS then CIS do path. append then we can go one folder before it so we can go to this folder and we can from now we can just import actually from utils import um uh get Center of bounding box and get bounding box width I think I misspelled width maybe so let me just go and fix it so right here it's width and also in the bounding box right here it's called width um so yeah um now that we're done\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:32:15---->01:33:00]:\n",
            "with that we can uh continue with uh writing the uh draw ellipse function so uh right now we just want to get the X Center of the bounding box so it returns X Center and Y center of the get uh Center of bounding box function but uh we are not using the Y Center anymore so let's use uh the X Center uh because the Y we're going to it as the bottom y not the center y um then we want also the width uh and the width here is going to help us to be one of the uh radiuses of\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:33:00---->01:33:45]:\n",
            "the ellipse so get uh uh bounding box width like this and um uh let's draw the ellipse so we are going to Dore the ellipse with the CV2 function uh we did not import CV2 so let's import it first then CV2 ellipse then give it the frame give it the center which is going to be X Center and Y2 then give it the um uh axis and axis here is going to be just um like the U radius of circle but um since an ellipse has two then we will need to provide two so let me show you what I'm talking\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:33:45---->01:34:30]:\n",
            "about um so right there we will need to provide the minor axis which is this one and we will need to provide the major axis which is the wider one so you can see that um we will need to provide uh this and this th those are the two values so we can simply say int width this is as one of the axes and the other one we can just have um like 35% of this axis like this width which is going to be um like uh the other one uh you can play around with the shape if you want\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:34:30---->01:35:15]:\n",
            "but this shape is what um you saw earlier and uh we can then Define the angle and then the start angle the start angle right here is going to be uh 45 and the end angle is going to be 235 and I'm I'm going to tell you that those will mean that um a bit of the circle is not going to be drawn and I think this is uh and this is um this looks good uh for my end so you can see right here that for example this uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:35:15---->01:36:00]:\n",
            "Circle doesn't have this bit drawn and it's because we draw from 45 and end at 235 and don't draw the other uh bit um if you want to draw it you can draw it but I think this gives it gives it like a gamified uh gamified look so we can have it like this uh yeah after this we can give it the color to color and then we can give it the thickness uh and two is quite good then we can also uh give it a line type and I'm going to choose line four this looks good for me and uh at the end we are just going to return the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:36:00---->01:36:45]:\n",
            "frame like this so right now we are drawing the players and let's uh run it uh yeah before we run it let's call it first so uh we can say draw output right here then we can draw for object tracks and then we can write output video frames equal tracker dot draw\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:36:45---->01:37:30]:\n",
            "annotations give it the video frames and the tracks and then instead of uh um like saving the video frames as we H as as we read them we just uh save the output video frames so yeah that is it so let's run it again and see the output so uh I got an error right here because through annotations I forgot to return back the uh frame uh so the output frames right here so after we have the frame and like this we just append it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:37:30---->01:38:15]:\n",
            "uh append the frame then we return it back the output video frames like this so we can clear uh we can clear the output right here let's clear it and let us run it again so it's done right now and we can go and open it so you can uh go to the output video files open it right here and you can see that it is uh being uh drawn like semicircles being drawn quite correctly but I think the uh the part that is not being drawn that it is uh way too much I think I'm going to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:38:15---->01:39:00]:\n",
            "revisit the angles uh give me a minute uh so it's going to be 45 till 235 not 45 five so yeah with that being said we can just run it again let's see the output now so yeah this is the output that we expect and you can see the ellipses are being drawn on the users and um for example when you concentrate on this user and when he is running and the bounding box is wide you can see that the the ellipse is wide and then when he\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:39:00---->01:39:45]:\n",
            "starts to be straight then the ellipse is going to be narrower uh because uh this is going to be related to the width of the uh user uh the player or or the object so yeah this is going to be it we're going to do the same thing for referees so we can do this as a referee like this and and uh we are going to get the referee dick and then refere like this and we are going to provide it with the bounding box and instead of it being\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:39:45---->01:40:30]:\n",
            "red uh I want it to be yellow so that it can have a different color and this is just going to be 255 in the uh green and red so this is going to give us a yellow so if we can uh run it again let's see the results so you can see that the referees right now has a yellow uh uh Circle below them um so yeah right now we want to have an identifying like the track ID below the users below the players so I want to show you the output that we we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:40:30---->01:41:15]:\n",
            "are going to do so this is what I'm referring to so for example this all always has six be like below the user this always has 11 this always guy has 12 and this has 13 um so we want to add this and this will be in two steps basically the first step is drawing this rectangle the second step is just writing the number so let's do that um so uh for here uh uh we can uh go back again to the draw ellipse and now we just want to draw the rectangle so we can Define some things\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:41:15---->01:42:00]:\n",
            "for the rectangle first so rectangle width is equal to 40 then we have rectangle uh height is equal to 20 and uh X1 rect is going to be uh by the way uh we are going to right now uh Define the X Y and XY positions which is the corners of the um the the top left corner and the bottom right corner of the rectangle so uh it's going to be X1 X1 uh X1 y1 X then X2 Y2 so X Center like this\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:42:00---->01:42:45]:\n",
            "and I am going to which is X Center is going to be the center of the center of the bonding box in terms of X um minus rectangle width / by two and this is just going to uh move uh half of the width uh from the center of the um center of the rectangle and uh I am going to do the same thing for two so this is going to be two but instead of subtracting I'm going to be adding uh this way the B like the the rectangle is going to be centered um on the user then I am going to have the uh y1\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:42:45---->01:43:30]:\n",
            "uh which is going to be um basically Y2 which is uh the bottom of the um the bottom of the uh player then I am going to get the angle height then divided it uh by two and plus uh 15 which is just a random buffer I am going to do the same thing with Y 2 but instead of subtracting I am going to be adding and also this uh is going to be a a buffer that makes things a little bit neater and I am going to check if track ID is not none so there is a track ID then I am going to draw a\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:43:30---->01:44:15]:\n",
            "rectangle like this and I am going to give it the frame then give it the X1 rect then X um X2 rect we can just make sure that those are ins like this I think they are but yeah U we can just make sure about that um we can also give it the same color that we uh got and this uh -1 uh is going to be we can replace it with CV2 fil which is going to make the rectangle\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:44:15---->01:45:00]:\n",
            "fil um now we just want to write the number so um we can Define where the number is going to be written so the exposition of it is going to be X1 re of the rectangle I am just going to add a little bit of padding which is 12 pixels and if the track ID is greater than 99 I am going to make the X1 text um minus equals 10 because it's going to be a bigger number so I want it to start uh maybe a little bit on the left so that it can be also centered in the rectangle uh this is just for visual stuff but if you're getting confused uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:45:00---->01:45:45]:\n",
            "don't worry about it just the you can delete it and the number will be a little bit on the right for the bigger numbers so in order for us to put the text we just uh write put text then give it the frame then uh give it what to write so I am going to give it that it's going to be the track ID like this um and then uh we are also going to give it the position so it's going to be um X1 text like the the the one that we defined here then uh it's going to be y1 uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:45:45---->01:46:30]:\n",
            "rectangle uh then plus 15 which is going to be also uh some padding uh afterwards we are going to define the font side like the font type and I think uh Hershey uh Hershey Simplex is quite nice and we can then Define the font size which is going to be 0.6 um like this and uh let's also Define the um color uh it's going to be black so that we can see it quite easily and then the thickness uh we can make it two and then we return back the same frame so with that being done we can run again and see the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:46:30---->01:47:15]:\n",
            "results so I got an error here because track ID doesn't have um a default type so we can just make it none like this uh because I'm not going to give the track ID for the referees I don't care about their uh like track and individual statistics uh so I'm just not going to provide it and it's not going to be drawn um because we just check if the track ID is not none then we draw it if it's none then we don't draw it uh just save it clear the output and run\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:47:15---->01:48:00]:\n",
            "again it's done now so we can go here we open it and we can see it quite nicely so you can see the you can see number six is being tracked across frames and this bounding box is saying that this is number six and this is number six so this is what tracking is uh so now we just want to have the pointer on the uh ball so let's now do that um so before we do that we I just want to to show you how are we going to make this pointer this pointer is going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:48:00---->01:48:45]:\n",
            "basically a triangle so let me draw it right here so it's going to be something like this um where we have three points to make this uh thing happen which is going to be this one this point then this point then this point so uh it's going to be an inverted triangle so we have uh three points so this is going to be X and then y then it's going to come up a little bit then we subtract a little bit from the X whatever and then we also subtract from\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:48:45---->01:49:30]:\n",
            "the Y um so you understand what it is uh we also have like uh like this color and we also have a border and the border is going to be just the the same angle again but it's not going to be filled so let me show you how it's um like what I'm talking about so we are going to make the uh Define draw triangle we're going to take in the self the frame the BB box and the color uh we are going to define the Y which is the bottom uh the bottom point of the Y which is going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:49:30---->01:50:15]:\n",
            "int BB box of one and I am choosing y1 because right now we just want the triangle to be on top of the ball so this is going to be y1 and not Y2 and uh I will also want to have the X to be the accenter so we can call the same function and give it the bounding box and this is the xenter we can also right right now Define the three points of the triangle so triangle points which is going to be equal to the numpy array of the three points let's import the N array first like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:50:15---->01:51:00]:\n",
            "here like this um and then we just want to give it the X and Y this is the first point the second point like I showed you let me draw it again uh so uh right here this like this like this so this is going to be X and Y uh let's me try and write it like this and this is going to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:51:00---->01:51:45]:\n",
            "be x - 10 and uh y - 20 this point and this point is going to have the same y y - 20 but the x is going to be + 10 instead of - 10 so this is going to be it and this is also going to be uh filled like this and yeah those are the points so let me now write it here so x - 10 then y - 20 and let's have the same point but add the plus\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:51:45---->01:52:30]:\n",
            "10 and in order for me to draw it we can just write uh CV2 draw uh Contours and we give the frame then we give it the triangle points like this and uh then we uh give it the uh color index then we give it the color like this and then we say if it's uh uh filled or not so uh right now it's going to be uh filled so you can also do it like this\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:52:30---->01:53:15]:\n",
            "filled and afterwards we also uh want to have a border and drawing a border is quite easy we're just going to draw another rectangle another triangle and it's going to have the same coordinates it's going to have a color of zero but it's not going to be filled it's just going to uh like draw the um draw the edges of it and yeah that is it so at the end of the day we return the frame and the function is ready so right now we can also here draw\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:53:15---->01:54:00]:\n",
            "ball and then we can Loop over uh the balls so we can for track ID again we won't need the track because it's only one ball uh in ball uh dict do items like this um and uh then we just take the frame equals self dot dra triangle and then we give it the frame we give it uh the uh ball bounding box\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:54:00---->01:54:45]:\n",
            "BB box like this and we also give it a color and so B grr like this this is going to be a green color and yeah this is it for the ball so let us clear the output and draw again now it's done so we can open it and you can see that the uh ball is being detected and it has the green annotations quite good um so yeah that is it now we just want to um like assign each player their team\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:54:45---->01:55:30]:\n",
            "according to the t-shirt that they are wearing and we are going to do that with um like color clustering and then uh understanding which color that the user is wearing and then assign it to a team so in order for us to do that um like in a clear manner um I will start by doing it in a notebook so that you can see the stepbystep process and the output of each step and then we can move those uh we can move the same code that we are developing uh into a module that we can use um with a class uh so let's create a new folder called\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:55:30---->01:56:15]:\n",
            "development and Analysis um and we can add here as much uh python notebooks as we want so we can have the color assignment. iynb and yeah this is going to be the notebook that we are going to use uh so we can return back here and let's just um take any of those um any of those players like this and then um uh like try and uh take their uh image and save it so let's save the cropped image of\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:56:15---->01:57:00]:\n",
            "the uh uh of the user so of of a player so right now we have the video frames like cropped image of a player uh let's also do it after we get the tracks because uh we will know uh where uh to crop so basically um we want to have for um player in tracks of players of um zero which is\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:57:00---->01:57:45]:\n",
            "going to be frame number zero and um this player is going to have uh basically um uh an item like items this is going to have a check ID and the player so it's going to have the bounding box like this and then we want to have the frame of the uh first one which is like this so we want now to crop the bounding box from the frame from frame like this and what we are going to do we are going to get the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:57:45---->01:58:30]:\n",
            "cropped image and then the frame and we are going to uh start from y1 to Y2 and um X1 to X2 then we want to just save it save the cropped image so right now we just want to save the cropped image so we write uh CV2 IM right output and then maybe videos because we have this already um and we can call it cropped image. gpg and we can give it the uh cropped image let's just import the CV2 two first because we did not import it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:58:30---->01:59:15]:\n",
            "and let's also break don't forget to break we just want one image and yeah just run it again uh so there's an error because uh I did not specify here the bounding box so just specify the bounding box and make this as an image uh but it's it doesn't matter but I'm going to keep it as an image um and yeah just clear the output and run again so it tells us that the slices must be uh must be integers so just make\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[01:59:15---->02:00:00]:\n",
            "sure that they are integers before we proceed so like this here here and and here uh let's run again and uh see the output so now it's done so you can open the output videos see the cropped image and you can see that there's a guy that is cropped according to the bounding box and uh yeah we can now try and segment the color of his T-shirt so we don't need this anymore the save cropped image code and you can\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:00:00---->02:00:45]:\n",
            "delete it like this we uh we just wrote it one time and you can open back again the color assignment and just import CV2 we are going to need that then import matplot li. pip plot as PLT and this is going to help us uh visualize the images inside the notebook and let's also import the number p as NP like this and uh let's now uh specify the image path so image path is equal to back then output\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:00:45---->02:01:30]:\n",
            "videos then it's going to be uh cropped image. jpg and then it's going to be image we just read it with CB2 IM read then it reads the image um then it reads it as BGR so we can convert it to RGB and yeah that is going to be it um the oh yeah I misspelled videos right here so don't do that uh so yeah now let's uh show the image it's going to be shown with uh PLT so uh like this PLT do\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:01:30---->02:02:15]:\n",
            "show and you will now going to see the image wonderful so the thing is is that we have the whole image but the t-shirt is in the top half of the image almost always because a user will not be upside down in this uh case and this use case so let's just take the top half of it so take the top half of the image and uh so top half like this um\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:02:15---->02:03:00]:\n",
            "image is equal to uh image till the uh shape of uh of uh like till the height divided by two and I'm going to take the the whole X then let's also show it so I am show then this one then uh PLT do show like this then we have the top half of the image now if you get the average color of this the problem is that there is a lot of background like you can see the background pixels which is the green pixels um they are uh a lot um and we will need to uh segment m m or remove uh the background from the T-shirt color so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:03:00---->02:03:45]:\n",
            "in order for us to do that we can cluster the image into two colors so that we can have the background color um in one cluster and the uh t-shirt color in another cluster and we can simply uh get the average of the T-shirt color the average color of the T-shirt color uh so yeah so for order for us to C cluster the image into two clusters like this uh we just want first to reshape the image into a 2d\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:03:45---->02:04:30]:\n",
            "array um because we don't need to have it to be um in this uh format um image 2D is equal to uh image3 shape -1 and 3 uh then we perform uh K means clustering uh with two clusters and this would require us to import uh K means from es skarn so let's import s Kearn do clusters import actually it's from\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:04:30---->02:05:15]:\n",
            "import K means like this let's run it let's go here and uh let's uh now uh write K means with um two clusters and a random state of zero so that we can replicate the results and then we can fit it uh you can write it like this uh or you can have it like this if you want so have it in two different steps uh whatever you want uh then we can uh get the labels so get the cluster labels so for each pixel we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:05:15---->02:06:00]:\n",
            "are going to have whether this is like cluster one or cluster two um so labels is equal to K means of labels and uh then we are going to have uh to reshape it again to be a uh an image so reshape the labels into the [Applause] original image shape so clustered image is equal to labels do Tre shape and then we can uh uh reshape\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:06:00---->02:06:45]:\n",
            "it um then we just want to display the results so display the clustered image and let's uh show it but let's not show it in a gray scale um and let's PLT do show like this oh yeah so we took the image of it but let's take the crft image top half image like this then we run it again and there you go you see that um the the the background pixels are in uh blue and are\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:06:45---->02:07:30]:\n",
            "in yellow and the uh t-shirt pixels are in uh purple so now we can just segment it so we can say um okay so clusters that are this we can get all the pixel colors right here and then get the average color of it so this is going to be very easy for us but in order for us to know like if the background pixel is going to be zero or one um uh uh uh this is going to be a little bit um like we we're going to use our brains right here U because uh we don't know like CES can assign background to one or zero uh for for anything so usually the background is\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:07:30---->02:08:15]:\n",
            "going to be in the corners like those four corners so what I'm going to do is I'm going to get the class for the corners and the most uh like abundant class or the class that um was um was there the most like for example right here if it's class Z 0 0 0 then class Zer is the background and class one is the foreground so yeah so I'm going to get the corners of it so Corner clusters so it's going to be clustered image of 0 0 and then 0 and negative 1 which is this one then it's going to be 1 and zero then it's going\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:08:15---->02:09:00]:\n",
            "to be1 and negative 1 which is this one and uh we are just going to get the non-player cluster which is going going to be uh the number that appeared the most on those four corners so print non-player cluster like this and the non-player cluster is cluster one and to get the player cluster it's just going to be player cluster it's just going to be 1 minus non-player cluster so like this so it's going going to be zero so if it's if the non-player cluster is is is one this is going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:09:00---->02:09:45]:\n",
            "zero and if it's zero this is going to be one uh now this is very simple for us and uh let's now choose the color so we can get the K means cluster centers of then get the player cluster now we know that uh zero is the player that the is the color that we want so yeah you have 171 235 and 142 you can just go here write rgp color and uh see whether this color is green so we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:09:45---->02:10:30]:\n",
            "can have 171 then 235 then 142 and indeed it is green and it is the same color that the uh that the player is wearing so very close so the clustering is now working fine and we can now put it into a module that we can use to assign player teams uh according to the to their t-shirts so let's create so let's save this first and uh maybe close it and let's also create here a new folder called um team\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:10:30---->02:11:15]:\n",
            "assigner like this awesome and let's also create um a file called init.py to expose it and let's also create a file called team assigner py so team assigner dopy and inside of it we are going to create the class for team assigner so class team assigner and then we Define the init and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:11:15---->02:12:00]:\n",
            "self uh you can close this exit I opened it by mistake um then we can pass it till now uh then we want to uh basically uh get the player color and assign team colors so let's first assign every team a color so let's say team one is going to get color uh White and team uh two is going to get color green so we are going to assign uh team color and we are going to have it uh for\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:12:00---->02:12:45]:\n",
            "self and uh frame and uh player detections uh this is the input of it and uh we are going to get the player colors in a list so for each player we are just going to put every color into a list so four and player because we don't care right now about the track ID so uh player detection and player detections do items and then we can get the bounding box which is player\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:12:45---->02:13:30]:\n",
            "detections of BB box and uh then we are going to get the player color and uh we are going to do that with another function um player uh color and we are going to give it the frame and the bounding box so uh again this function is going to be very similar to the code that we already written in the notebook so let's us uh do it so uh get player color and then it takes self and it takes frame and bounding box and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:13:30---->02:14:15]:\n",
            "right now we will do the same steps as before so we will take the image and uh first of all crop it and don't forget to uh convert it to integers and not do the same mistake that I did before like [Applause] this uh uh and after the image we can get the top half of the image again so top half image is equal to image and then we can uh get the top half of it then we want to get the clustering\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:14:15---->02:15:00]:\n",
            "model and the clustering model is going to be K means self. get clustering model that will take the top half of the image and this is going to be another function again I'm just making it into functions so that we can uh utilize it and make it uh quite easy but it's the same code that I have for in The Notebook so we can get the get clustering model then self then image and uh what I want to do is first\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:15:00---->02:15:45]:\n",
            "reshape the image into 2D array then image 2D is equal to image. reshape and then we can perform K means um with two clusters and this is going to be uh K means equal K means and let's import it first so uh from SK learn do clusters import K means and let's here use the number of clusters is equal to two let's also use the uh K\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:15:45---->02:16:30]:\n",
            "means Plus+ and the uh n in it is going to be one um this is just um I wanted to have the least number iterations so K means Plus+ would help us to get um better clusters faster uh but it's the kind of the same clustering uh algorithm uh just uh a different way of initializing the Clusters um then we can fit it and then we can return it back like this so K means so right here we have the K means clusters is and uh we can uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:16:30---->02:17:15]:\n",
            "return back to the get player color so after we get the uh after we get the key means uh we just want to get the cluster labels get the cluster labels for each pixel and we can get the labels is equal to K means do labels like this and just like we did last time we just reshape it back again to the dimensions of the image so uh reshape the labels to the image uh shape and it's going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:17:15---->02:18:00]:\n",
            "clustered image is equal to like this and uh we can get the uh get the player cluster get the player cluster and it's going to be the same thing which is getting the corner clusters first like this and it's going to be yep like this and uh we are going to choose the non player cluster which is or the cluster that appears most in the corners uh then uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:18:00---->02:18:45]:\n",
            "the player cluster is going to be 1 minus it one we can actually have this if condition but we can also have this one minus number player cluster this is going to be working fine uh then we are going to get the player color which is going to be K means uh then get the centers of the player cluster I skimmed over this code because we already covered it in the notebook so if you're quite um like not catching up what what I'm doing here just go to the notebook and I think um it will be um more visual so you can see the output of each step so right now we have the player\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:18:45---->02:19:30]:\n",
            "color uh of it so the uh player color of um of each player that we have and right now we can just add let's rename this so player colors we can take this add here do append and add player color so now we are going to have all the player colors within the first frame that we are going to pass and yeah we just now want to divide those colors into two so one is going to be white and and the other one is going to be green\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:19:30---->02:20:15]:\n",
            "so again what we uh in order for us to do that we can just K means do another K means um we can cluster the uh into n and clusters which is going to be two also uh let's also choose C++ and let's choose the initialization to be one um let's also uh fit it to the player colors that we have and uh yeah then we will have two colors so uh let's first in the init uh let's define self. team colors which is going to be a\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:20:15---->02:21:00]:\n",
            "dictionary and let's here uh Define self. team colors and we can Define one for K means of zero and uh two for C of one so now we have the colors for each team uh so now we have like a color for each team now we just want to match this color with the um like the the play like the player color with the team color so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:21:00---->02:21:45]:\n",
            "before moving on we can just get this K means and uh save it in the self so that we can uh refer to it later now let's create also another function that assigns um that assigns players to teams because right now we only have the color of the team so let's uh do this so let's get player team now we are going to match the um the the player with a team so self then we are going to give the frame then uh give the player BB box and we are going to give the player ID as\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:21:45---->02:22:30]:\n",
            "well and uh what we do is that if player we just want to have uh basically um memory of if the player ID we already know the team of then we don't have to run the K me on it this is going to save us a little bit of time so we can have the player team dict which is going to be a dictionary where uh we are going to have the player ID and whether he is uh like whether he is in uh player one or player two like team one or team two I mean um so what we do is basically first we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:22:30---->02:23:15]:\n",
            "check the player ID like this if the player ID uh is in self. player team dict uh the first time we are on this uh like the first frame we on this we shouldn't uh enter this uh if condition uh but if we do uh we just return back the team of it and and uh if not we just get the player color and uh player color uh get player color would require us to run the K means and uh would require us to run this get player color uh function uh give it the frame give it the player bounding box and it returns\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:23:15---->02:24:00]:\n",
            "back the color then we can get the team ID uh by running the self. K means which is this one model uh do predict dot predict and then uh we can give it the uh uh like the player color player color do reshape of one of1 and then we take the uh zero zero with index and because the team ID is going to be zero or one and I would like to for it to have it one or zero so I'm going to get the team ID plus equals 1 so this is going to be one\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:24:00---->02:24:45]:\n",
            "or zero um so yeah uh before just returning the team ID I am going to save it first in the uh player team dict which is this one and I am going to say player ID and it is equal to the team ID so next time in the next frame if I already have it like if I already have this uh in I won't have to run the C's clustering I would just return it back and yeah that would be it so uh I can return back the team ID okay so now we're done uh we can just uh call those functions from the main and start\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:24:45---->02:25:30]:\n",
            "assigning uh T like assigning um players teams so yeah let's go back to the main and uh right here we can just uh yeah before we go here we can go to init and from Team assigner import team assigner and then we can go back to main here then uh from Team assigner import team assigner and then here we assign player teams so we can get here team assigned\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:25:30---->02:26:15]:\n",
            "let's initialize it first and have the team assigner initialized like this then we want to uh assign teams their colors so get the team assigner assign team color give it the first frame and give it also the tracks uh but uh don't give them all tracks let's also give it only the tracks for the play ERS in the first frame so uh this is uh this is going to get the uh colors of the uh players of the in the first frame and assign it to the teams and let me also get you back here so team colors will be uh will have uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:26:15---->02:27:00]:\n",
            "their own uh colors right here so right now we just want to Loop over each player in each frame and assign them to the team so for uh frame number and player track in enumerate and it's going to be tracks of players like this um then we want to look over each player in the frame so player ID then\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:27:00---->02:27:45]:\n",
            "track like this in um player track. items uh what I want to do is that I want to get the team so uh we have the team assigner do get player team and uh give it the video frame right now that we are standing on and uh we also give it the uh bounding box and we give it the player ID uh so this will return back the team and uh in order to save it we can just\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:27:45---->02:28:30]:\n",
            "take the players like this tracks of players of uh frame number then of uh player ID uh then we assign it we introduce a new value to the dictionary that is called team and then we give it the team uh we also can give it also a color so we can have like this so team color and we can also have Team assigner uh of team colors of Team uh which basically is going to get us the uh color that we have put here in the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:28:30---->02:29:15]:\n",
            "Clusters so that is going to be it now note that in order for us to add anything to the uh tracks uh we would add um a key just a key and then add the values so right here I added it into the main but in in in future modules I will be adding it into uh the module itself so that we have less code in the main but I wanted to show you guys this is happening in the main so that you can um understand and um like connect the dots uh so right now we can uh whips we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:29:15---->02:30:00]:\n",
            "can now run it uh let's clear the output and run it from here uh so I'm getting an error right here for the team assigner and I have misspelled the init so just um uh just do the init uncore uncore afterwards and yeah just run it again and it should work Byer to uh also I think I have mispell the K means I think it should have a capital K\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:30:00---->02:30:45]:\n",
            "so let me go and fix that right now uh so right here uh we can have it as a capital k um let's so it's capital K and M so let's uh do it like this clear then run again and hopefully no more errors okay so now it worked fine now to utilize the color in the drawing so what we have to do is go back to the tracker and instead of giving it a hardcoded uh color uh we just give it the color of the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:30:45---->02:31:30]:\n",
            "team so let's give it the color of the team which is color uh of player. getet team color and if it doesn't if it doesn't found any if it doesn't find any tee color we just write the uh red uh basically but um it should have uh all team colors uh in in place so yeah that is going to be it and let's clear the output and run it again now it's finished so let's go back here\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:31:30---->02:32:15]:\n",
            "go to the output videos and now we can see that this uh player has a white circle and this player has a green circle which is quite correct so you can see that all players are assigned to the correct teams and now we can differentiate teams from each other uh so each player will have the team color and uh the team number um as well as the bounding box so uh right now also let me open this up and let me uh point you to the next uh thing that we are going to do um so here for example you're going to find\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:32:15---->02:33:00]:\n",
            "that the ball is being detected then it is being not detected in this Frame then the next frame and then the next frame is being detected so uh the idea is is that um those two frames we can interpret where the ball is so um so for example let me uh try and uh visualize it for you guys so right here we have this detection then we have this detection in uh like in um in a next uh frame in like in the uh like this this is frame number one frame number two it's not being detected frame number three it's not being detected but frame\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:33:00---->02:33:45]:\n",
            "number four it is being detected in so the idea is that uh the ball is going to move in a straight line so it's going to move like this this is the ball so it's it's going to be easier for us if we can just say okay so in this like this distance divided by two and just put a rectangle in uh in like here and here which is basically the same line that we are going to be moving in and it's going to uh divide the distance equally between the frames that are missing so that we can put it right\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:33:45---->02:34:30]:\n",
            "here um this would help us to uh add in the missing values of the ball um and have our like our detections more complete um now this is uh now we can do this by hand but there is a faster way to do it with pandas so I'm going to show you that so we are going to go in the trackers again and let me close down all those unnecessary uh files so that we can uh do it uh clean things up a little bit and let's let me create a new uh a new function that is called interpolate\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:34:30---->02:35:15]:\n",
            "bows and interpolation is just missing in the missing values and uh pandas is going to help us uh interpolate those missing values because it already has a function for that so let us interpolate uh ball positions and the way that we are going to do that we are going to give it the self then ball positions then we are going to give it the ball positions um we are going to convert the ball positions format into a data frame a pandas data frame so uh in order to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:35:15---->02:36:00]:\n",
            "first do that we just needed to be in a list so for X in ball positions what we want to do is that we want to get uh uh to get number one which is the track ID one and then uh if there is no track ID then it's going to be an empty dictionary then we are we want uh the BB box and if there is no BB box because it's an empty dictionary right here uh then we put an empty list and this empty list is going to be interpolated by the pandas data frame also while we're at it let's import pandas so import pandas as\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:36:00---->02:36:45]:\n",
            "PD uh then let's convert it to the uh pandas data frame so ball positions equal PD do data frame like this give it the ball positions and then give it the columns to be equal x X1 y1 then X2 Y2 and then we interpolate missing values which is going to be uh DF ball positions like this then DF ball positions do\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:36:45---->02:37:30]:\n",
            "interpolate like this um now we have interpolated it uh just the there's an edge case where the if the missing detection is the first one then it's not going to interpolate it um so um we can like do that by replicating the the nearest uh detection that we can find so what we can do is that we just can backfill it basically but interpolate should fill in almost 99% of the missing uh detections and back filling is just for the first frame or the first two three frames something like that if it's missing then we can uh return back the B positions to the original\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:37:30---->02:38:15]:\n",
            "format which is the list of dictionaries of lists um which is going to be like this um so let us Loop over it first so for X in DF do ball positions dot 2 numpy do 2 list and uh I am going to put it into a dictionary where one is going to be the track ID then the value is going to be a dictionary of bounding boxes and the value is going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:38:15---->02:39:00]:\n",
            "X then we return back the ball positions uh so this is it for uh the ball positions and now we can just go to the main and call it there so let's go back to the main and after we do the tracks we can just uh interpolate ball positions like this and uh what we can do is we can give it the tracks of ball which is equal to tracker do interpolate ball positions and give it the tracker uh tracks. ball so that is it so we can just run\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:39:00---->02:39:45]:\n",
            "it so now it's done and uh and we can go right here and now we can see the output so you can you can like uh in the previous detections you saw that here it started to not show so right now you can see that it is showing in every frame and it is quite accurate so this is a filled uh those two are filled uh positions and you can see that those filled positions point to the ball quite exactly Al although this might not be the case if you can go right here and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:39:45---->02:40:30]:\n",
            "you might find that in some scenarios um it might be a little bit laggy like this uh but it is close enough for use case and in most cases it is um it's going to be pointing at the ball correctly now what we want to do is we want to um assign uh which player has The Bard right now so uh I can go and show you the final output video again and show you that the player that has the ball has a red triangle on top of him like this one and BAS basically what I want to do is that uh I want to replicate this logic again so you can\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:40:30---->02:41:15]:\n",
            "see that um the player that has the ball basically has um has this red uh triangle on top of him so what I'm going to do in this scenario I am just going to see the closest uh the closest foots um uh the closest foot position to the ball and which player does it belong to so in this case it belongs to this player so this player has the bar right now and um so the idea here is that um if the ball is far enough like this uh I don't want to assign it to anyone so I am going to also have um a maximum uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:41:15---->02:42:00]:\n",
            "distance between the player and the ball for assignment and if it increases on top of it then no one will have the ball uh till it is again very close to the to the to the player till uh so it can get assigned so in order for me to work on this I would need to have also a new folder let's create a new module uh that is called player ball assignment so we can create a new folder right here that is called player ball assigner like this so this one is going to have a file that is going to be in it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:42:00---->02:42:45]:\n",
            "again and it is going to have a file that is called player ball assigner dopy and in this player ball assigner we are going to write the logic for this and it's going to be uh super simple uh so it's going to be like we we want to import sis then from yours import uh we we will want get Center of bounding box and uh yeah so for we are going to create the class player\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:42:45---->02:43:30]:\n",
            "ball assigner like this and we are going to have the init um in the init we are going to set the maximum distance between the player and the ball so max player ball distance to be uh 70 uh pixels anything above that then the ball is not going to be assigned to anyone um then we have a function that is called assign ball to player uh this this function is going to take in the self the players and the ball\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:43:30---->02:44:15]:\n",
            "box BB box um so for uh so I will need the ball position so ball position uh which is going to be the center of it so get center of a ball box um and then I am going to Loop over each player like player ID in player in players. items then I want to have the player BB box which is going to be player of bounding box um then I want to have the distance like here uh I want to have the distance of this foot and the the distance between\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:44:15---->02:45:00]:\n",
            "this foot and the ball and the distance between this foot and the ball and this is going to help me uh like get the distance to the nearest Foot so if the user is running this way or running this way um I will get the minimum distance between them so uh in order for me to get the distance we are going to go back to theils then uh bounding boxes and then I'm going to write a function called measure distance um so I'm going to Define measure distance like this it's going to take P1 and uh P2 and it is going to return the distance between those two points uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:45:00---->02:45:45]:\n",
            "which is going to be return and uh then uh it will be like this P1 uh yeah so this is this is the correct one so this is the uh equation for me to get the distance between any two points um and it is just the the the points then uh like power two uh X is power two then plus y power two then um get the root of that um again let's return back to the uh uh utils in it and then let's add\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:45:45---->02:46:30]:\n",
            "this which is going to be measure distance and let's get back to the ball assigner let's also add the measure distance and let's then uh return back uh to the uh for Loop that we were in so we are just going to have the distance left which is on the left foot so measure distance and we are going to get it the player BB box of uh zero and uh player uh BB box of uh -1 which is Y2 which is the bottom y so this is going\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:46:30---->02:47:15]:\n",
            "to be the X and the bottom Y and then uh this is going to be one point then the other point is going to be the ball position like this and the distance is going to be this is distance left then we want distance right which is going to be instead of zero it's going to be two uh then the distance is going to be the minimum distance between the two like this um then I want to get the closest player so in order for me to get the closest player I need to get the minimum distance so like this and assigned um\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:47:15---->02:48:00]:\n",
            "assigned player which is going to be negative 1 uh when it's uh like uh at the beginning it's going to be negative 1 so if the distance is less than self dot Max uh This One max player uh ball distance then we can uh start proceeding on and see if the distance is less than the minimum distance then minimum distance is equal to distance assigned player is equal to the player ID the then at the end we just return back the assigned player and this is how we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:48:00---->02:48:45]:\n",
            "assign uh players to bols so we can go back here and say from um player ball assigner uh import the ball assigner then um here we can also import it so from player po assigner import player assigner um and then we can just write here uh like assign ball to assign ball acquisition basically acquisition so we can have the player\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:48:45---->02:49:30]:\n",
            "assigner which is equal to player uh ball assigner which is uh we're just initializing it so team uh let's let's ignore that for now but uh let's have the four frame number in player and enumerate player tracks for tracks of players like this and then we are going to get the uh ball uh BB box and it's going to be the tracks of\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:49:30---->02:50:15]:\n",
            "ball of frame number of one which is the uh uh the track ID then the box and then we want to get the assigned player so assigned player is equal to player assigner do assign ball to player give it the uh player track then the ball box and uh player assigner I think I misspelled it give me a minute yeah I I misspelled it like there is a G that I needed to put so this is the uh assigned player that\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:50:15---->02:51:00]:\n",
            "we assigned so if the assigned player is equal to1 then there is no player that was assigned so uh we can basically uh don't do anything but if there is like this so if the tracks of players of then of frame number of um ass assigned player um then we will make um a new parameter in the dictionary called has ball and we will make it is equal to true and yeah that is going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:51:00---->02:51:45]:\n",
            "it um so we are going to have this has ball uh this has ball attribute and according to it we can also add something in the draw annotations so let's open it go down to the draw players and we can simply say if player. getet has ball then um if we don't find it we just return false uh then we can just draw the rectangle self dot draw triangle sorry\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:51:45---->02:52:30]:\n",
            "and we are going to give it the frame we are going to give it the uh player bounding box and we are going to give it a color of red like this and let's clear this and run it again so now it's finished so we can open it and see so yeah you can see that uh it's working now quite fine oh yeah sorry I opened I think the wrong uh video let me open the correct one uh the output videos this one and yeah we can see the uh red one uh the red triangle is being assigned quite correctly and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:52:30---->02:53:15]:\n",
            "yeah I think we can move on so the next part is going to be uh this part which is going to uh assign the team ball control so it's the percentage of time that the team has the PO has the ball and you can see that team one because the white one is having the B most of the time right now and it's going to increase till the other team gets the ball so uh we are going to return back to our code and we are going to add it uh in the main right here so we can have something like team\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:53:15---->02:54:00]:\n",
            "ball control and it's going to be simple so I don't want to make a new module for it but uh yeah we just can uh uh have it right here and we can draw it in the same uh tracker do annotations but yeah if if you are doing things uh by the book it's better for it to have its own module so we can have this uh uh Team ball uh control. append and we can add the um the team the the the the the player that has the ball the the assigned team that has the ball so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:54:00---->02:54:45]:\n",
            "tracks of players of frame number which is the one that we are say then the assigned uh Team or assigned team player like this one and uh we are going to get his team so if his team is one or his team is zero we are just going to add it into this list so this list has the frame number and the frame number is going to be assigned uh a team and else uh we are just going to add the um like this we are just going to add the last one that is going to had the ball so if there is a pass or anything like that um we are going to include it to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:54:45---->02:55:30]:\n",
            "the uh last person that has the ball so yeah so this is it this is the uh Team board control basically um so this array will have uh in each frame whether uh like team number one or team number two has it and like I said we can uh put it in the draw annotations so we can just do like this like here and the draw annotations function uh we can add code for it to have this uh semi-transparent rectangle uh so that we we can um uh sum yeah so we can have the summer transparent rectangle at the bottom right position and we can write the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:55:30---->02:56:15]:\n",
            "statistics uh there so at the bottom before we append uh the um before we open the results we can write draw team uh b control and we can have the frame self dot draw team bow control that takes in the frame and it takes in the frame uh number um as well as the team bow control so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:56:15---->02:57:00]:\n",
            "team bow control and uh Team B control needs to be taken inside uh as a parameter like this and let's now uh draw it so we can go here Define and then uh this uh that it takes in the frame frame number and Team B control so frame frame number and team Bol control and let's now um like um draw the semi semi-transparent uh rectangle so draw uh sui uh transparent\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:57:00---->02:57:45]:\n",
            "rectangle like this and we can have an overlay of uh like uh this overlay is just going to help us with the transparency but we are just uh going to draw a normal rectangle uh on the overlay like this and we are going to make the position of it as this X and uh this um so yeah um in order to find those numbers I just played around with those numbers till I found something that I like uh but it's like U like finding the position so you can draw\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:57:45---->02:58:30]:\n",
            "whatever rectangle keep moving it till you find the position that you like and uh it was white so the the rectangle was white like this and it's filled so it's -1 or you can write CV2 field if you want uh then um you can have Alpha uh and this Alpha is for the transparency and it is 40% then uh in order for us to uh add it to the original frame we take the overlay the Alpha and the frame we add it together and uh yeah now it added a semi-transparent uh rectangle um now we just want to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:58:30---->02:59:15]:\n",
            "calculate the percentage of time that the that a team has the ball so we can have the team bow control uh till frame and uh we can have the team bow control like this and uh give it the frame number + one so this is going to be just a list till the frame number that we are\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[02:59:15---->03:00:00]:\n",
            "standing on um then we want to get the b control for each team so get the number of time um each team has the ball each team had the ball so team one num frames is going to be uh which is going to be uh like this and uh we are going to choose Team B control and uh to like off uh where the uh where the value is going to be one and then we are going to make it shape of\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:00:00---->03:00:45]:\n",
            "zero like this so This requires it for the uh for the list to be a numpy so if we didn't uh make it a numpy so we have to make it so yeah right here this is a list right now so just convert it to a number Pi array like that and also import nonp here so we can use those uh functions with ease uh let's return back to the uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:00:45---->03:01:30]:\n",
            "tracker so right here uh we can use this with e if it's a NP array so that is working fine now let's do the same thing but for a team two so just copy it paste it underneath make it team two and make this also team two and then uh you can make the statistic which is team one is going to be team one number of frames over Team 1 plus uh Team two then uh same for team two and right here so this is going to be uh basically it for the statistics now we just want to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:01:30---->03:02:15]:\n",
            "write this so CV2 dot uh put text then put it on the frame what do you want to put we want to put uh something like this so team one uh b control then we want to add um team one uh multiply by 100 uh to make it a percentage then uh we can just limit it to two decimal points and then add the percentage\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:02:15---->03:03:00]:\n",
            "sign then we are going to add a position which is going to be um 1,400 and 900 then we are going to add the uh CV2 and um specify the font so it is going to be simplex and we are also going to specify the color and the thickness and let's make the thickness uh three let's do the same for team two and put here team two and let's make it uh 950 in y so that it can be underneath it and yeah that is going to be it are we returning the frame no we're not so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:03:00---->03:03:45]:\n",
            "let's return the frame back and now we returned it so we can clear this run it and see the output uh so numpy is not defined so make sure and when you go back to main import numpy as NP just save it clear the results run it again and you should get\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:03:45---->03:04:30]:\n",
            "it so uh now it's done so you can go back again to your folder uh go to Output video so you can open up the video and see the team player acquisition and right now only the white team has the ball till the ball is reached the the goalkeeper and the goalkeeper here is being detected as the opposite uh player uh opposite um opposite opposite team because he doesn't have uh the color white or green and we don't have to bother a lot with the goalkeepers right now so we can just hardcode him to be the white team so let's fix that and come\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:04:30---->03:05:15]:\n",
            "back again so in the team assigner um so we just uh here here in the assign team uh Team assigner uh on the left team assigner team assigner right here uh you can just say if the player ID is equal to 91 which is the uh player ID of the um of the uh goalkeeper then we can specify that the team ID is equal to two we can save it we can clear the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:05:15---->03:06:00]:\n",
            "results and run it again so uh now it's done and uh we can now see the results so um we can see that actually it needs to be team uh one not two uh that is a mistake so let me change it right here and let me also be more confident in clustering the player colors so let's make it instead of in it uh one just make it 10 and run it again so that we can uh get the results\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:06:00---->03:06:45]:\n",
            "so now it's done we can open it again we can see the output right here and we can see that the the goalkeeper is being detected as the white team the other goalkeeper is also being detected as green so everything is good here and we are good to go and we can see that the percentages are moving quite fine so everything is good right here uh so the next step right now is to measure the uh camera motion as you can see the motion of the camera is being uh like the camera is moving all around and because the camera is moving the bounding box will move even if the players are not moving so we will need to um counteract the the the player\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:06:45---->03:07:30]:\n",
            "bounding box movement with the camera movement so that uh we don't exaggerate the player movement um this will be essential when we are trying to measure the player speed and the distance that the player have covered uh because it will be contaminated if the camera is uh moving a lot okay so now what we want to do is we want to uh make the camera uh movement module and let me explain what are we going to do first and then uh uh go back and uh codee it so the idea right now is that I want\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:07:30---->03:08:15]:\n",
            "to detect some features and specifically I'm going to detect Corners right here for example this corner Corners at the top and Corners at the bottom so I'm going to choose um basically features from the top here and features from the bottom and ignore the features uh from the middle because everyone is moving in the middle uh and hopefully those two things shouldn't move in reality uh and the only thing that will make them move is that if the camera moved so I will choose this part right here and the part that is down and try to extract some features um and those\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:08:15---->03:09:00]:\n",
            "are going to be Corner features and basically uh from each frame I am going to the detect how much they moved with Optical flow so I'm going to try and see uh how much it moved right here how much it moved right here and this uh this movement this feature movement is going to be the camera movement at the end so it is going to be uh quite simple so let us open it again here and create a new module called camera movement so so let's open up the uh new folder called camera movement uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:09:00---->03:09:45]:\n",
            "estimator then we are going to make a new file called init then we are going to uh put the init here then uh we are going to make the camera movement uh estimator estimator py and this will of course have the class uh so let's create the class so class camera movement estimator which is going to be having uh the init function like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:09:45---->03:10:30]:\n",
            "this uh which is going to have the self and we are going to pass it right now uh then then we want to also have a function called get camera movement so Define get camera movement that is going to take the self then it's going to take the frames uh then uh it's also going to uh read from stops because this is going to be a time consuming uh a timeconsuming operation so I would do it only also the first run and then save it and then the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:10:30---->03:11:15]:\n",
            "second runs would read from the toop so read from stub is equal to false and then stub path is equal to none and because we're using uh because we're going to save it then it's going to be pickle so I'm going to import pickle and yeah so that is it uh right here we are going to read uh the stub when we have it uh but let's ignore that for now and let's now uh create a camera movement per frame so for each frame we're going to calculate the camera movement so [Music]\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:11:15---->03:12:00]:\n",
            "camera movement is going to be movement for x and movement for y so this uh first list is going to be for the frames then the second list is going to be for the X and Y positions then I multiply this um with the length of the frames that we have so this is going to be 0 0 for all frames at least uh when we initialize it then I want to extract uh I want to uh sorry um convert the image into a gray image so that we can and extract the features so I am going to call it old gray because it is going to be the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:12:00---->03:12:45]:\n",
            "previous frame uh so I'm going to call all previous frames as um as old so CV2 dot uh CVT color then uh frames of zero uh then we also have the CV2 color RGB to Gray and let's also import the uh CV2 like this and afterwards for the old features let's also extract old features so from this image we are going to extract features so we are going to say CV2 then\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:12:45---->03:13:30]:\n",
            "uh good features to track this is going to extract Corner features we we also give it the old gray and we can also give it whatever parameters we want but H let me uh let me explain the parameters so um let me also put it in the init so uh for the parameters let's here take a frame like this and let me also uh this is going to be the first first frame so first frame gray scale let's also make it as a gray\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:13:30---->03:14:15]:\n",
            "scale uh I would put the features in the end it just to make things uh more uh uh cleaner so CV2 CVT color and uh convert from RGB to gray then I am going to just uh choose the uh banners uh like uh I am going to choose this part uh as my uh extracting of features and also I'm going to choose this part uh to extract features and it's going to be before what I write here so uh you can ignore this so those two places shouldn't move much like this and this so I am going to choose\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:14:15---->03:15:00]:\n",
            "them so uh mask features which is going to just uh choose the top and bottom so it's going to be np. Z at the beginning and let's import numpy as NP uh so NP of zeros uh like and then I give it the uh the gray image then I give it the mask features which is going to be uh this one and uh uh first 20 uh first 20 uh rows of pixels so I'm going to take it like this and I want to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:15:00---->03:15:45]:\n",
            "have also the last ones which is going to be 900 till 1,50 uh so this is going to take the banner from the up and banner from uh the bottom then self do features is going to be add that is going to be uh first of all we are going to specify the maximum Corners so maximum Corners is going to be 100 this is the maximum amount of Corners that we can utilize for the good um for the good uh uh features then the quality level is going to be 0.3 and of course the higher the quality level the better\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:15:45---->03:16:30]:\n",
            "the features but the lesser the amount of features that you can get so 0.3 is quite uh good for our use case uh then we can specify the minimum distance between the features which is going to be uh three pixels then we are going to specify the block size which is the search size uh of the um of the features then we are going to give it the mask that has the top and bottom um like rectangles so that it knows where to extract the features from and right here from the good features track we can just get here self features and because it's a dictionary then we can have the double star before\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:16:30---->03:17:15]:\n",
            "it so that it can expand the dictionary into the uh parameters let's also remove this extra bracket and right now we can uh loop over each frame so frame number in uh range one because we already used the zero here so this is the previous frame use I said and this is going to be the current one uh then length Len of frames like this then I'm going to get the frame\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:17:15---->03:18:00]:\n",
            "gray uh CB2 dot frame numbers and make it uh gray scale uh then I want to see where the new features are so so the new features is going to be here so new features then I'm going to use Optical flow and give it the old gray uh basically then the gave it give it the uh new uh new gray which is the new gray image then I want to give it the old features and then I want to give it some parameters so um it's going to be self do uh LK params and let us also fill it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:18:00---->03:18:45]:\n",
            "up so in the init let's also fill up the LK parameters so right here you can have the self do uh LK params as a dictionary and the first one is window size which is the window that we are going to search like the size of the window that we are going to search so wind size is equal to 15 and 15 uh then uh we for this we have pyramids for this Optical flow we have pyramids that is going to downscale the image to get larger features so in order to do that uh we have a max level of two\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:18:45---->03:19:30]:\n",
            "so it can downscale the image uh up to twice um so so uh this is going to be big enough for our use case so uh no need for us to increase it on top of that so uh uh for the stopping then we Define the stopping criteria and it is uh it is either we uh loop over like we um Loop over it and search the number of times that is in the stopping criteria or we have 10 times uh we looped 10 times but didn't find anything below uh sorry above the quality score that we have have so uh either of those uh will be our stopping\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:19:30---->03:20:15]:\n",
            "criteria and yeah this is going to be Optical flow so it's going to provide us with the uh new features basically and we now want to measure the distance between the old features and the new features and to see whether the distance is going to be um uh like whether there is distance the camera movement or not so in order for us to do that we are going to measure the maximum distance because each uh image or each frame is going to have multiple uh features and we just want to get the maximum distance between any two features so maximum distance is now zero\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:20:15---->03:21:00]:\n",
            "and the camera movement of X and the camera movement of Y is going to be 0 and zero and for I in new and old so in enumerate and we give it old features so new features so uh yeah so let's just uh swap them so new features then old features like this and we can then get the new\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:21:00---->03:21:45]:\n",
            "features uh point so we can know the the point of it so we can get a new travel and then um uh old features also do travel and then we can get the distance between the features and it's going to be measure distance and give it the new feature points and then the old feature points and it is going to be from utils from so uh import CIS and sis. path. append and it's going to be one before that\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:21:45---->03:22:30]:\n",
            "from utils import measure distance like that and uh here we have the distance and if distance is greater than the max distance then max distance is equal to distance and then we can get the camera movement X and camera movement y uh we can just do it like this uh or we can just have a function in the utils that can do this for us so let's uh create it so the Define measure uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:22:30---->03:23:15]:\n",
            "XY distance then uh give it um the uh 01 then 0 2 and we return back um we return back the distance of Y the distance between the two uh x's and the distance between the two y's so it's P1 of 0 minus P2 of 0 and P1 uh and P1 of 1 minus P2 of one uh this is going to be it we just expose it in the init like this and then uh we return back to the camera movement import it right here uh like this and we just have\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:23:15---->03:24:00]:\n",
            "it like here and give it the uh new features than the old features and yeah that is it so it's going to be like this uh let's just uh swap also all features with new features uh because yeah this is the correct one um now we just want to just make sure that um the camera movement is not so little so we can Define uh something like self do uh the minimum camera movement that is required so minimum distance which is going to be five so at least if it moves five then it's statistically significant for us if\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:24:00---->03:24:45]:\n",
            "it doesn't move five then we can consider that the camera didn't move at all so uh basically we can say if max distance is greater than self do minimum distance like this and then we can specify that the camera movement of frame number it's it's going to be camera movement of X then camera movement of Y then we are going to make the old features equals to the uh CV2 dot uh good features to track then give it the frame gray and give it the uh features parameter and this is going to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:24:45---->03:25:30]:\n",
            "get the features for the current frame that we are on and put it in the old frames because now we are going to Loop and it's going to be the previous one and uh at the end we are going to get the override the old gray with the frame gray. copy and uh yeah that is going to be it at the end we just want to return the camera movement so this camera movement is going to be returned right here uh so that is going to be it and yeah uh the only thing that is missing right now is that we are not utilizing\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:25:30---->03:26:15]:\n",
            "the stops so let's add it and uh uh let's add it in so if stop path is not none then we pickle dump it like this then at the top let's also add if read from stop and stop path is not equal to none and the OS let's import OS and the file also exists um then we can save it uh then we can uh read it and yeah this is it for us so we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:26:15---->03:27:00]:\n",
            "can open again the init here and expose it so from camera movement import camera movement estimator and we can go back to the main so we can have here from camera movement import camera movement and right now we just want to calculate the camera movement and then we can actually also draw it so uh uh let's let's first um uh add it so camera movement estimator right here [Applause]\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:27:00---->03:27:45]:\n",
            "um and you can initialize it with the uh first frame uh then uh camera movement per frame is equal to the camera movement estimator then uh get movement of video frames then um we can make it read from stop then uh we can specify that the stops to be uh stops of\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:27:45---->03:28:30]:\n",
            "camera movement uh let's say stub of pickle so this is uh going to be it but before we close it I just want to also draw the camera movement so that we can see it um so right here we can just have here draw camera [Applause] movement and output video frames is equal to the uh camera movement estimator do uh draw camera movement and give it the output frames and then give it the uh camera movement per frame uh of course this function is not uh done\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:28:30---->03:29:15]:\n",
            "yet so we can go and uh uh work on it so right here we can have a new function called Define draw camera movement and it is going to have the self then frame then camera movement per frame like this then I'm going to have the output uh frames is equal to an empty list uh then I'm going to uh run over each frame so frame number in frame do enumerate um then frames let's call this frames right here and it's going to be\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:29:15---->03:30:00]:\n",
            "frames right here uh then I'm going to frame equal uh frame do copy in order to not contaminate the what uh was uh like uh inputed to the function and we are going to utilize the same overlay that we utilized before so I am just going to skim over it basically which is frame. copy then I'm going to use the rectangle function and give it the overlay give it 0 0 then make the position uh 500 and 100 then uh make the um color of it to be white like this and uh keep\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:30:00---->03:30:45]:\n",
            "it as fil which is -1 and I'm going to specify also the alpha which is going to be 0.6 this is um for transparency then add weight overlay and Alpha to frame and uh this is going to be the uh White uh rectangle so also I am going to have movement X movement so X movement uh and Y movement from cab movement per frame and take the frame number and uh then I am going to uh put some text which is going to put the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:30:45---->03:31:30]:\n",
            "camera movement for x and the camera movement for y so CV2 dop put text and is going to put the frame then I am going to put a camera movement X then I want to put the X movement and I want it to be only two decimal places so like this and um I am going to uh put it at the position of 10 and 30 uh then I'm going to also give it the uh font uh type and also give it the uh size the color and the thickness of it I\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:31:30---->03:32:15]:\n",
            "am going to do the same thing for the Y so just copy paste it at the Y here and add the Y here and um also uh we're going to push it a little bit down so to be shown um and then we are just going to add it to the output list and return it so this is it so we return back and make sure that we are utilizing it and draw camera movement is just draw camera whoops draw camera movement so uh I think it's it's fine right\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:32:15---->03:33:00]:\n",
            "now uh so everything is good let's save save and run it again so in the main I have put a dot right here just remove it uh clear the output and run it again so I had two mistakes right now so the first mistake is that I didn't put the zip here so you need in order for you to Loop over any two lists you have to zip them first first so zip them and then uh you will be able to do it and in here I did uh like I put the bracket\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:33:00---->03:33:45]:\n",
            "right here you need it you need to put it uh right here so that you can multiply by the N of uh frames so this is now uh done and you can open it right here and you can see that the camera is not moving and as soon as it moves you can see that the values is also moving so this is working uh fine right now um so right now we just want to make the uh player POS player positions robust to the camera movement so the first thing that we want to do is that we want to uh get the player positions so we want to add the positions uh to the tracks first and then we want to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:33:45---->03:34:30]:\n",
            "subtract the camera movement from the player positions so in the tracker we are going to add a function called add positions to tracks so let's do that right now uh you can go here open the tracker and go here to any place I like uh uh it doesn't matter but I would prefer to have it at the top so that I don't write between the drawing uh logic add uh position to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:34:30---->03:35:15]:\n",
            "tracks and then I give it the self then I give it the tracks and uh then for object object tracks in tracks do items and uh four frame number in uh track in numerate and give it the object uh tracks uh and uh we are just going to get uh the player position relative to the bounding box so let's also have the track ID and the bounding box it's not\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:35:15---->03:36:00]:\n",
            "going to be the bounding box right here um it's going to be the track uh like um track info and um is also going to have a bounding box so let's get the bounding box of it like this track info of BB box um now we have the uh bounding box now we uh can also have the uh if object which is what We're looping over over here is equal equal to uh ball uh we want to have the position to be the center of it so let's get the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:36:00---->03:36:45]:\n",
            "position to be the center of the bounding box and uh like that and else uh we want to get the foot position which is uh the center of the bounding box but the uh the center in terms of X but the bottom in terms of Y so we can add also a function right here that is called get foot um position that is that takes in the bounding box and it is going to uh basically have the uh end position of Y and the middle position of X and let's\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:36:45---->03:37:30]:\n",
            "also have it right here as an integer and um yeah uh where am I I think I'm in the tracker um yep so yeah so uh position then get um get foot position sorry about that so we have to get foot position and there you go and at the end of the day we just want to um write it basically so we're going to have the tracks then the object then the the frame number then the track ID then the position is going to be the position so that is going to be it and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:37:30---->03:38:15]:\n",
            "uh in order for us to utilize it basically we go back to the main and we just after getting the tracks right here we just write tracks uh uh sorry tracker do add position tracks and give the tracks and we can also have a comment that is called get object positions let's run it and make sure that nothing broke so uh it it uh we have the uh get foot position we need to put it in the uh init to expose it and then we can\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:38:15---->03:39:00]:\n",
            "clear we can run it again we can also go back to the main so now it's working fine uh what I need right now is that I need to go to the camera movement and adjust the positions that we have just uh added um to the uh to the tracks um so right now it it's working fine and now we just want to go to the camera movement again and adjust the positions that we have just added right here and adjust it according to the uh camera movement so you can go back to the camera uh move mement estimator and we want to also add a function called add adjust positions to tracks that is going to adjust the position according to the camera movement so\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:39:00---->03:39:45]:\n",
            "Define adjust uh positions uh to tracks and this is going to be uh self then it's going to be track and then it's going to be uh camera movement per frame and add here so don't forget to add adjust camera mve into frames so uh the idea here is that we are going to Loop over each of the uh objects so each of the objects and each of the tracks we are going to Loop over them then we are going to Loop over each uh frame in the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:39:45---->03:40:30]:\n",
            "tracks and for each uh frame we are going to Loop over each track ID we are going to get the position and uh then we are going to adjust the position so this position adjustment uh was uh done uh uh in um was predicted wrong so let me uh write the correct one so camera uh uh movement is equal to the camera movement per frame of the frame number then we have the position adjusted which is equal to the the position of zero which is uh the X uh plus uh sorry we we have to minus it um then we also have to\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:40:30---->03:41:15]:\n",
            "minus that so um um so we are going to subtract the X from the camera movement X and the Y from the uh movement of Y and let's call the camera movement correctly like this so that it can be readable and then after we do this we just uh assign it so we have the tracks doobs of frame number of track ID then we call it position adjusted and add the position adjusted right here and in order for us to uh call it we uh we will go back to the main and we will go uh right here\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:41:15---->03:42:00]:\n",
            "underneath the camera movement so let me uh copy it um camera movement estimator like this do add adjust positions to tracks give it the tracks and give it the camera movement per frame and now it should be working fine so let me clear the output and run it again so everything is working fine now uh it run without any crashes and the position now is adjusted according to the camera movement so uh we are good now so we are ready to move to the next step and the next step is going to be the um uh view Transformer basically we are going to do some perspective\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:42:00---->03:42:45]:\n",
            "transformation so that uh we know basically how much a user have um uh have uh like uh moved in in meters and in the real world so I'm going to tell you what exactly what I mean um so in this image uh you you know that uh this is the middle of the court so in reality uh this middle is going to be uh like the number of meters here is going to be exactly the same as the number of meters here so um if this camera was actually um uh was doesn't had any uh like was\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:42:45---->03:43:30]:\n",
            "point was shot at an angle uh that is perpendicular you can have uh the uh you can have the pixels corresponding or like uh very proportional to the actual world but uh here you can see that this half court is only 214 pixels while this half court is 543 pixels so if a user here uh if a user here move 54 pixels it's going to be the same as a user here that it moved around uh 200\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:43:30---->03:44:15]:\n",
            "pixels so with this in mind we will need to adjust this um adjust this to be um to be basically uh uh the to be in the same uh way that it is presented in the real world so we will need for the computer to understand that those amounts of pixels is equal to those amounts of pixels so this is what we call perspective transformation which is going to basically let me uh do this\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:44:15---->03:45:00]:\n",
            "uh it is going to do something like this uh it is going to take in uh this point this point and at this point this point and this point like it is going to take this sh trapezoidal shape um and convert it into a rectangle so that all this uh all this place in blue let me get it for you um this place in uh blue which is this one that is only uh 200 pixels out of\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:45:00---->03:45:45]:\n",
            "700 this is only 200 out of 750 even so this blue uh this blue height or this blue thing is going to take in half of the rectangle like this and this um let's say orange this orange bit right here press okay uh so this orange bit right here is going to take in the rest of the rectangle so this is what is called a perspective transformation where um where where the pixels doesn't correspond one to one\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:45:45---->03:46:30]:\n",
            "with the uh actual distance being covered in meters so we will need to uh transform it somehow into this rectangle shape where uh where the where the place here is is is going to be uh proportional so this place is going to be half of the court and this place is going to be half of the Court which is in actual reality and in actual meters this is exactly what is happening um so this can be done uh with also open TV where we can give it um uh some things like those uh positions uh of the uh red trapezoidal shape and we\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:46:30---->03:47:15]:\n",
            "can also give the uh positions or the meters of this um uh of this rectangle and uh this rectangle is quite easy to get so again this is the width of the quart and we can from a quick Google search we can get the width of the C and I am going to get it with you guys so this is going to be the width and we can write it for example um give me a minute I'm going to get it so this width of the quart is going to be 68 uh Point uh this is going to be 68 M\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:47:15---->03:48:00]:\n",
            "uh the width of the court is uh 68 M and the uh court length is also like this one uh can be also calculated quite easily because we know the width of this uh of of this half court and it is divided equally between those uh rectangles so uh 1 2 3 4 5 6 7 and 8 and N9 so we can divide the width of this uh with u the uh width of the rectangle so uh yeah so let me open this up and write football Court uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:48:00---->03:48:45]:\n",
            "Dimensions so yeah uh so let uh so let us do a quick Google search with football core dimensions and you can see that it is 105 M uh times 608 so 105 is the length and 68 is the width so again this width is 68 M and uh all the height here is 105 so we can open a calculator me like this uh so half a quart is going to be 52 M and because it's divided equally between those rectangles like 1 2 3 4 5 6 7 8 and 9 so we can also divide it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:48:45---->03:49:30]:\n",
            "with nine so one uh so one rectangle uh like this is going to be 5.83 uh M basically um so if we are going to have um like the same one that we uh that I showed you before so we are going to have the same trapezoidal shape uh which is going to be starting from here here here here and here so uh we can have\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:49:30---->03:50:15]:\n",
            "this uh trapezoidal shape and we can know uh exactly how uh like the width of it is is 600 but the length of it is going to be uh 5.8 uh three which is going to be the uh length of one rectangle of those uh then Time 1 2 3 and 4 which is going to be 23.5 so if we know those two pieces of information which is the uh those vertices those trapezoidal vertices and the actual dimensions in meters for the uh for the thing that we are measuring we can transform it to have uh for those to have the positions according to uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:50:15---->03:51:00]:\n",
            "basically a rectangle which is going to be the real world position so yeah this is what are we going to do right now is to um convert the adjusted position of after the camera movement to real world positions so that we can then track uh speed and distance quite easily so let me close this uh come back here and uh create a new folder called uh view Transformer and let's also create a new uh file that is called init like this\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:51:00---->03:51:45]:\n",
            "py and let's create a new uh file also that is called uh view trans for. py and in this view transformer. py we can create a Class View trans for that will have the uh uh init like this and it will have the actual diamensions basically so the quart width like we said is going to be 68 M and the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:51:45---->03:52:30]:\n",
            "quart uh length is going to be the 23.5 uh 23 32 M uh this is the actual uh meters of it then we can Define the um uh the vertices that I showed you of the trapezoid so we can have the pixel uh vertices uh equal to NP and let's import NP what at it numai as NP uh array like this and uh we can give it uh like the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:52:30---->03:53:15]:\n",
            "um positions of the points so the positions of the points I knew them uh because because I just uh tried a couple of points and just mve them till I uh reach the correct point that I want uh but you can actually do a program where you click and get the uh like print the uh print the pixel vertices um or you can basically try and estimate it uh with another Tool uh with a screenshot tool or anything like that but what I did is that is that I just drew this vertex and just keep moving it so for example I started with 100 and I was like okay 100 is far so let's try\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:53:15---->03:54:00]:\n",
            "105 and then 110 it was on the spot so I kept it and and so on and so forth and yeah so let's define the rest of them so it's going to be 265 and 275 um let's also Define the rest uh 910 we're 260 and the last one is going to be 1,640 and uh 915 uh this is going to be the uh pixel vertices now let's also have the target vertices so self.\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:54:00---->03:54:45]:\n",
            "Target uh vertices uh which is also going to be an ire and it is going to start from zero and uh till uh cord width uh then it's going to be 0 0 and then it's going to be quart length and then uh zero and it's going to be a quart length also and then cord width uh th this is a rectangle so this is basically a rectangle and this is a trapezoid and this is what we want the trapezoid to be after we transform\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:54:45---->03:55:30]:\n",
            "it um then we can just uh cast it to be a float so for this we can have this like this and we can get the pixels do as type float and we can do the same thing for the Target vertices like that then we can uh initialize the perspective transformation Transformer so this is going to be CV2 do get perspective transform and let's on Port CV2 get perspective transform and then give it the uh pixel vertices uh followed by the Target vertices and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:55:30---->03:56:15]:\n",
            "there you go you have the model that will uh switch between the pixel vertices to real word vertices just like that um afterwards we just want to uh transform the uh adjusted points to the uh uh to the uh points after the transformation of the prospective transformation so what we do is that we can just do something like this trans formed uh position to tracks uh so add transform position to tracks which is going to take self and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:56:15---->03:57:00]:\n",
            "is going to take tracks and is then going to Loop over let's take it like this uh objects and tracks and then it's going to Loop over the frame number and the track and then the track ID then the track info and we are going to take the position adjusted like this uh then uh we are going to uh transform this position to an npire AR like this and uh then we want to transform the position uh so the position\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:57:00---->03:57:45]:\n",
            "transformed is equal to self dot TR form uh Point like this and give it the position uh so this is how we uh transform the position and let us uh if the position uh transformed like this um is not none uh then we can have the position transformed which is going to be equal to the uh position transformed uh dot uh squeeze because we are going to return it back to a uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:57:45---->03:58:30]:\n",
            "numpy and then we are going to um uh basically uh write it uh in this so tracks object object then frame number uh track ID then position transformed then the position uh transformed so this is going to be it and we can uh put none here and then uh deal with it later so the only only thing that is left is that we need to write the transform Point function so we can go up here and write\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:58:30---->03:59:15]:\n",
            "trans transform point and this transform Point uh maybe maybe this is not going to cut it so uh we can basically say uh p is equal to int of uh point zero then we can have the int of 0.1 and we can also put it into a tuple like that so that's the point and uh let's check also if the point is inside the trapezoid and if it's not then we can ignore calculating the speed of it so is\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[03:59:15---->04:00:00]:\n",
            "inside so we can have the CV2 do points polygon test and give it the pixel vertices and give it the point and uh say that it is greater than zero and if it is inside if it is inside uh not inside like this uh we can just uh return none and if it's not inside so sorry if it uh if if it's not inside then we return none and if it is inside then we reshape get the reshaped point um which is going to be uh Point dot\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:00:00---->04:00:45]:\n",
            "reshape and uh we are going to reshape it in a format that is uh uh read by the uh prospective Transformer uh this is just uh some reshaping it's not going to do anything to the numbers um and we are we want to also have it as float uh then we transform the number transform point and we do a CV2 uh dot perspective transform and then give it the reshape point then give it the uh perspective Transformer and\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:00:45---->04:01:30]:\n",
            "then at the end of the day we just want to return it back and and reshape it before we return it so do reshape um this and uh do 1 and two so this is going to uh calculate the uh the view trans like the uh the position uh relative to the actual Court um in meters so uh before moving we can go back to the init and write uh from view like this from view Transformer import then uh we import The View\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:01:30---->04:02:15]:\n",
            "Transformer I have written it wrong Transformer like this um and yeah uh we can go back to the main and we can also so try and import it so from uh view Transformer import view Transformer and let's also uh copy paste it right here and after the camera movement\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:02:15---->04:03:00]:\n",
            "estimator we can add the view Transformer and we can have the view Transformer right there and uh let's call it add uh transform positions to tracks and you can give it the tracks and it would uh uh calculate the transform position of the tracks uh we have a small error let me trace it back and come back to you so in the init I misspelled the Transformer again so just copy and paste it right here clear it and run it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:03:00---->04:03:45]:\n",
            "again also in the main you can have the view Transformer uh writed correctly and uh come here and write also write it correctly here and clear it and run it again then I have I have another issue which is I added uh curly braces here I should have added square brackets so you can come to the view Transformer and in this array you can just add uh square brackets instead of dictionaries and you can also make sure that nothing has this mistake uh in the rest of the code don't think so then clear and run it again\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:03:45---->04:04:30]:\n",
            "so I currently I have another error which is a transform point so you can just uh copy it and paste it right here I'm just copying I'm just writing it so fast so I might make uh spinning mistakes so you can clear it and run it again and also I misspelled uh pixel vertices so you can just also copy it right here\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:04:30---->04:05:15]:\n",
            "and paste it right here so uh do this and clear and run again and I also uh did not spell perspective Transformer quite correctly so you can also do that so everything now is working fine and the last step of this uh project is just to calculate the speed after we have added the position transformed um now it's in meters so it's going to be quite easy to calculate\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:05:15---->04:06:00]:\n",
            "the me the the distance covered and the uh speed in kilm per hour uh so let's do that next uh so let's close down uh all of those and let's also close down those and just to clean things up and make things more tidy for us to develop in then uh let's also create a new folder let's call it speed and distance uh estimator like this uh let's also uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:06:00---->04:06:45]:\n",
            "create a new file called init uh do uh py and let's create a file another file that is called speed and distance St made t. py um let's go here calculate uh uh sorry create a class called speed and distance um estimator like this let's define the init function\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:06:45---->04:07:30]:\n",
            "and inside the init um we are going to calculate the current speed of the uh user which is going to be uh within a certain uh frame number so let's call this a frame window and let's calculate the fra like the speed of the user every five frames so Frame Window is going to be five and let's also Define the our frame rate which is going to be be uh 24 uh frames per second uh 24 frames per\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:07:30---->04:08:15]:\n",
            "second uh then let's uh have a function that is called add speed and uh distance uh to uh distance like this to tracks then add the self then also add the tracks and let's now uh try and calculate the speeds for each object so uh this is not correct so we can just uh uh do this for object and then object uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:08:15---->04:09:00]:\n",
            "tracks in tracks. items and uh we want to uh detect only the speed and the distance for the players so if the object is equal equal to uh ball uh or object is equal equal to a referee uh then we continue we don't need to do anything but otherwise it's going to be a player and we want to detect the uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:09:00---->04:09:45]:\n",
            "the speed and the uh distance of the player so what we want to do is that uh let's get the number of uh frames which is going to be length of objects object tracks and let's now Loop over the frame number then uh in range uh from zero uh till the number of frames uh like this right right here and uh let's uh have our batches of frame window so this is going to be zero then five then 10 and so on and so forth it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:09:45---->04:10:30]:\n",
            "adds Frame Window every Loop it does add one it just adds uh five uh then we can Define the last frame because sometimes when you add it uh add this Frame uh like the the last frame might be + 5 or it might be less than that so we can get uh the minimum so frame number plus Frame Window which is self. frame or a maximum of number of frames so whatever is the minimum of those and this is just to not go out of bounds um so for\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:10:30---->04:11:15]:\n",
            "track ID uh and uh let's uh take in the object uh object uh tracks of frame uh number uh dot items like this uh we can then if track ID uh not in uh basically uh uh object tracks so if it's not in object tracks of uh last frame then uh we can\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:11:15---->04:12:00]:\n",
            "continue uh this basically says is that if the track is in the first frame but it's is not in the last frame then we continue and we don't do anything uh with with this uh player calculation but uh in order to measure the distance and in order to measure the um the speed uh we will need to for the user or the player to exist in the first frame of the of of this batch and the last frame of this batch so uh then uh we can simply quite easily say the start position is equal to object tracks\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:12:00---->04:12:45]:\n",
            "of uh is going to be frame number of track ID then it's going to be position transformed and this is going to be the uh position in meters and we want to have the end position as well so the end position position is going to be not for the frame number but for the last frame and it's going to be position transformed as well and for any reason if the start and the end position is going to be none and it is none when the user is outside the trapezoidal shape that is outside the uh view Transformer uh then we don't want to measure the speed because we measure the speed only in the trapezoidal shape\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:12:45---->04:13:30]:\n",
            "which is most of the uh which is most of the qu anyways and most of the frame anyways so if the this one is none or end position is none we just continue um otherwise we just want to measure the distance covered measure distance covered and uh we can do that uh like um uh with the utils function measure distance so import CIS sis. path. append and then uh put this then from utils um import measure\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:13:30---->04:14:15]:\n",
            "distance and yeah this is it so we can come here and say measure distance and it's going to be start position and end position um then uh time elapsed and it's going to be the last frame minus the first frame over the frame rate which is the 24 frames per second and this is going to be time elapse per seconds then the speed M uh per second uh because now we have the distance covered in meters and this is uh seconds so the distance in meters per second is going to be distance over time\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:14:15---->04:15:00]:\n",
            "now we want the speed in kilometers per hour because this is uh more widely used and in order to convert from m/s to km/ hour we just multiply by 3.6 and then we just want to save the uh total distance uh of the user so uh at the top we can just have the uh total distance total distance uh of this uh and we can make it a dictionary so uh if if\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:15:00---->04:15:45]:\n",
            "object so if the object is not in the total distance we can make it a a dictionary and then if the track if the track ID is not in the total distance of object uh then we can uh start and uh put the uh track total distance of object of track ID and then initialize it with a zero and then we can simply add the total distance so of object of track ID plus equals uh distance covered and let's put this a z0 and um then we basically want to have um\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:15:45---->04:16:30]:\n",
            "like some annotations basically label the speed and label the distance so that we can go ahead and write it so uh we can just go for frame num batch um in range from uh so basically from frame num to last frame then we want to have if the track ID is not in the uh tracks of object of uh frame number\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:16:30---->04:17:15]:\n",
            "batch then we uh continue uh otherwise uh we just want to have the tracks of uh the object like this of the frame number batch of track ID of speed to be the speed in kilm per hour and let's do also the distance so we will have the distance right here like this to be the uh total distance of the object of the track ID like this and yeah that would be it so this\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:17:15---->04:18:00]:\n",
            "is adding the speed to the packs and last but not least we just want to um just uh add this to the annotations so we can go here and uh write draw speed and uh distance and uh we will just write the distance and speed uh underneath uh the player so we can go self video frames and then we going go tracks and then we have the output video uh output\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:18:00---->04:18:45]:\n",
            "frames which is equal to an empty list then for uh frame number and frame in enumerate uh basically uh frames let's call this frames like that and uh then we can Loop over the objects so uh so we can Loop over this so for objects and object tracks and the uh tracks do items then we can see if the if the object is not bolds or\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:18:45---->04:19:30]:\n",
            "referees uh if so we can continue without doing anything and if it is a player then we continue uh we take the track um let's call it uh track info and we were not going to do anything with the track ID right here so uh and let's uh Delete the rest so uh track info H in object tracks of frame number of items and then we can call if speed is in track info uh then we can uh write it down so speed is equal\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:19:30---->04:20:15]:\n",
            "to track info dot get and then speed and if there is no speed we can just put none and we can do the same for the uh distance so distance right here distance right here and if speed is none or distance is none we can continue uh otherwise we just want to uh write the speed and distance below the bounding box so let's get the bounding box first so this is going to be track info of bounding box uh let's get the position where we want to write it so it's going to get foot position\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:20:15---->04:21:00]:\n",
            "get uh foot position like this and uh get foot position of BB box and we wanted to also have uh basically um a buffer underneath the uh underneath the uh bounding box so we will just add position of one we just want to add it uh like 40 pixel buffer so that way it can be a little bit uh at the bottom and not overlap with the circle or the rectangle that have the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:21:00---->04:21:45]:\n",
            "track ID uh and then we can um basically make the Tuple um like this like cast it to a tuple and a map and then position and then we can put it as a text so CV2 let's also import CV2 at the at the top import CV2 so we want to add the text like this and we want to put the frame we want to have the uh speed so like\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:21:45---->04:22:30]:\n",
            "this speed and a we want to put uh 2 F so we can only um have the uh two decimal points and kilm per hour we add kilm per hour uh then we add the position then we add the um uh font uh font type then we add the font size it's going to be 0.5 and then we add the color let's make it black and let's add the thickness to be two and and let's do the same thing for the distance and this is the distance right here and instead of uh like the uh position um we can have a little bit uh\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:22:30---->04:23:15]:\n",
            "like the same position but a little bit down so we can have the uh same um X but for the Y I am just going to add 20 like that and everything else is going to be the same and uh yeah so at the end we just want to uh uh append it and then return it basically so this is it and let us expose it so from dot speed it's not completing so I'm going to uh copy it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:23:15---->04:24:00]:\n",
            "import speed and distance estimator so we can copy this and we can go here we can also import it don't uh don't forget to remove the dot and now we can use the speed estimator in the main um so you can go uh right here above the assigned players and we can uh speed and distance estimator like this and we can write\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:24:00---->04:24:45]:\n",
            "speed and distance estimator to be speed and distance estimator and um we can then add speed and distance tracks to the tracks um and at the bottom uh we just want to now draw it so let's draw the speed and distance so right here you can draw speed and distance and it is going to be speed and distance estimator to draw speed and distance and uh basically give it the output frames and tracks so save it and run it\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:24:45---->04:25:30]:\n",
            "so uh there is an error right here that has the uh last frame is out of range and uh right here uh we just uh need to uh subtract one from the length of the frames so uh right here we can just go here and subtract one and yeah because it's zero index so we can run it again and see how it goes so it's now done let's go and check the output uh we can open it from here output videos and then output here and uh you can see the uh output of the\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:25:30---->04:26:15]:\n",
            "kilom per hour but I don't see the distance uh the distance is is not uh correct I think um let me trace it and come back to you give me a minute so the last problem that we had is that I used to do this position of zero then plus two and then I needed to have position of one cuz this is the Y and you can also uh change this kilomet per hour I used to have it right here 2 m and then if you run it again uh it should be uh now working fine so give it a couple of seconds\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:26:15---->04:27:00]:\n",
            "so now it's finished and you can open the uh final output again and you can see that um the uh users in the trapezoidal area is being detected for the uh speed right here and then you can see if the user is running like this one the speed is 17 km/ hour and then when he slows down it goes down as well uh and then this is the total number of me that he covered um and yeah so this is the whole project and uh in this project uh you did a lot\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:27:00---->04:27:45]:\n",
            "you uh you did um you worked with object detections and you also trained your own object detector and uh with yolo which is a state-of-the-art model you also knew what are trackers and how you can utilize them and you also knew how to Cluster an image into different colors and how you can uh specify what color you want from an image you then worked on a little bit of logic for ball acquisition and know how to do a little bit of statistics on that for a team uh player movement um you also knew how you can calculate camera movement and how you can um like deal with a camera\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:27:45---->04:28:30]:\n",
            "movement for estimating U uh players's uh speed and meters then you also worked at the view uh uh perspective Transformer that would help us to move from a camera uh like shifted view to the normal uh real world view of uh in meters and then we calculated the speed and the uh meters covered for a user and yeah this is all the project it's a beefy project that is going to beef up your resume and yeah if you if you stuck through all this project then you have ton of experience and whether you're Advanced or you're a beginner or you're a machine experienced machine learning engineer this is going to be very\n",
            "\n",
            "Video Extract : Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\n",
            "[04:28:30---->End of Video]:\n",
            "important for you congratulations for staying this long and yeah see you see you in the next one bye\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from io import StringIO\n",
        "\n",
        "# Assuming your chunk objects are stored in a list called `chunks`\n",
        "\n",
        "# Create a StringIO object to hold the content in memory\n",
        "memory_file = StringIO()\n",
        "\n",
        "# Define the output file name\n",
        "output_file = 'youtube_script.txt'\n",
        "# Open the file in write mode\n",
        "\n",
        "chunks = transcript_documents\n",
        "with open(output_file, 'w') as file:\n",
        "    # Iterate over each chunk in the list with an index to access the next chunk\n",
        "    for i in range(len(chunks)):\n",
        "        # Extract metadata and content from the current chunk\n",
        "        title = chunks[i].metadata.get('title', 'Unknown Title')\n",
        "        start_timestamp = chunks[i].metadata.get('start_timestamp', 'Unknown Timestamp')\n",
        "        page_content = chunks[i].page_content\n",
        "\n",
        "        # Determine the end timestamp\n",
        "        if i + 1 < len(chunks):\n",
        "            end_timestamp = chunks[i + 1].metadata.get('start_timestamp', 'Unknown Timestamp')\n",
        "        else:\n",
        "            end_timestamp = \"End of Video\"  # or some appropriate end marker\n",
        "\n",
        "        title = chunks[i].metadata.get('title')\n",
        "        # Prepare the content to write\n",
        "        content = (\n",
        "            f\"Video Extract : {title}\\n\"\n",
        "            f\"[{start_timestamp}---->{end_timestamp}]:\\n{page_content}\\n\"\n",
        "            f\"\\n\"\n",
        "        )\n",
        "\n",
        "        # Write the content to both the file and the StringIO object\n",
        "        file.write(content)\n",
        "        memory_file.write(content)\n",
        "\n",
        "# Get the content from the StringIO object as a string\n",
        "memory_content = memory_file.getvalue()\n",
        "\n",
        "# Print confirmation message\n",
        "print(f\"Script written to {output_file}\")\n",
        "\n",
        "# You can use `memory_content` as needed, it's the full script content in memory\n",
        "print(memory_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LDA8hXw-Ntit"
      },
      "outputs": [],
      "source": [
        "out = loader.load_and_split(text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KyHJKJn0Fpeu"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0,max_tokens=4096)\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=4000,\n",
        "    chunk_overlap=1000,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ML3nNeznFmam"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "chunks = text_splitter.split_text(memory_content)\n",
        "documents = text_splitter.create_documents(chunks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UDxB1qn6lDZP"
      },
      "outputs": [],
      "source": [
        "\n",
        "initial_chunk_prompt_template = \"\"\"\n",
        "Task: Here is a partial transcript of a youtube video with timestamps. Your task is to destile the information from  the raw transcript and write a concise well structure article that summarizes the\n",
        "      new or intersting information. The timestamps where the information is being extracted should be added at the end. In case part or the whole of the video is a tutorial it is important to list the steps provided. \n",
        "      The reader might then go to the timestamps to check so include a chronological list or timeline summarizing the sequence of topics discussed in the video.\n",
        "Part of Transcription:\n",
        "{text}\n",
        "PARTIAL ARTICLE:\n",
        "\n",
        "[start_timestamp---->end_timestamp]\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ypWCJIEIBJ_"
      },
      "outputs": [],
      "source": [
        "\n",
        "intial_prompt = PromptTemplate.from_template(initial_chunk_prompt_template)\n",
        "\n",
        "refine_chunk_template = \"\"\"\n",
        "We are working on processing the information from a long youtube transcript and destile it into one well structure cohesive text either a Medium Article or a scientific paper whatever fits best. The idea is that a reader with a technical backgroud\n",
        "can easily read our composition and understand what new information/technology/insight is being presented in the video.\n",
        "\n",
        "We are working by parts because the transcript it's a very big.\n",
        "We have made a partial document up to a certain point of the transcript: {existing_answer}\n",
        "\n",
        "Your job is to merge and maybe if appropiate summarize this partial articles together into a consice coherent document with an abstract, introduction, body paragraphs and conclusion\n",
        "\n",
        "Requirements for the document:\n",
        "Focus on what new knowledege is the video presenting. Explain what new or intersting, technologies,tool,algorithm,discovery, are being discussed and how do they work and why are they relevant\n",
        "\n",
        "The document should have an abstract,introduction, and a paragraph for each key topic/tool/algorithm/etc.\n",
        "The body paragraphs should  reference the timestamps of the transcript that support what they are saying as if they were citations.\n",
        "The reader might then go to the timestamps to check so include a chronological list or timeline summarizing the sequence of topics discussed in the video\n",
        "\n",
        "In case the video or part of the video is a tutorial list the steps provided.\n",
        "\n",
        "\n",
        "Continue and refine the existing partial article with  more context below if appropiate in a way that is still coherent. Otherwise return the current partial article\n",
        "------------\n",
        "{text}\n",
        "------------\n",
        "\n",
        "\n",
        "PARTIALLY PROCESS DOCUMENT:\"\"\"\n",
        "refine_chunk_prompt = PromptTemplate.from_template(refine_chunk_template)\n",
        "\n",
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    question_prompt=intial_prompt,\n",
        "    refine_prompt=refine_chunk_prompt,\n",
        "    return_intermediate_steps=False,\n",
        "    input_key=\"input_documents\",\n",
        "    output_key=\"output_text\",\n",
        ")\n",
        "result = chain.invoke({\"input_documents\": documents}, return_only_outputs=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0jx-ajKAlhtM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abstract:\n",
            "The video tutorial \"Build an AI/ML Football Analysis system with YOLO, OpenCV, and Python\" provides a comprehensive guide on creating a football analysis project using YOLO for object detection. It covers various aspects such as training YOLO, object tracking, and drawing enhancements to improve visual representation in video frames. The tutorial caters to both beginners and experienced machine learning engineers, offering insights into advanced techniques like smart tracking and trajectory prediction. This document aims to summarize and analyze the key topics discussed in the video, highlighting the new technologies and methodologies presented.\n",
            "\n",
            "Introduction:\n",
            "The tutorial focuses on building an AI/ML Football Analysis system with YOLO, OpenCV, and Python, emphasizing the importance of object detection and tracking in sports analytics. It provides step-by-step instructions on setting up the project, importing the video dataset, and utilizing YOLO for accurate object detection. The video also delves into advanced techniques such as color clustering for team assignment and image cropping for efficient analysis.\n",
            "\n",
            "Body:\n",
            "1. Object Detection with YOLO:\n",
            "Object detection is achieved through a neural network that can draw bounding boxes around objects in images and identify them. YOLO, known for its efficiency and accuracy, is imported using Ultralytics for object detection (00:03:00---->00:04:30).\n",
            "\n",
            "2. YOLO Model Selection:\n",
            "Different versions of the YOLO model are available, with varying parameters affecting accuracy and computational resources. The appropriate YOLO model is selected based on hardware resources, with larger models offering higher accuracy but requiring more computational power (00:05:15---->00:06:45).\n",
            "\n",
            "3. Object Detection Inference and Results:\n",
            "The process of running object detection inference using YOLO on a video dataset is demonstrated, visualizing detections made by the model (00:06:45---->00:11:15).\n",
            "\n",
            "4. Output of Object Detection:\n",
            "The output of object detection includes bounding boxes, class names, and confidence levels for detected objects, simplifying the output to two main components (00:10:30---->00:12:45).\n",
            "\n",
            "5. Mapping Results and Class Names:\n",
            "The results of object detection include boxes and class names, aiding in interpreting detected objects in video frames (00:13:30---->00:15:00).\n",
            "\n",
            "6. Tracking and Bounding Boxes:\n",
            "The concept of tracking in object detection and the representation of bounding boxes are discussed, emphasizing accurate predictions for player and ball detection (00:14:15---->00:18:45).\n",
            "\n",
            "7. Model Refinement and Data Set Utilization:\n",
            "Refining the model to detect and track specific objects in the football analysis project is emphasized, utilizing a dataset from Roboflow for training and fine-tuning the YOLO model (00:18:00---->00:21:45).\n",
            "\n",
            "8. Training and Fine-Tuning YOLO Model:\n",
            "Guidance is provided on training and fine-tuning the YOLO model for accurate object detection results (00:21:45---->00:23:15).\n",
            "\n",
            "9. Dataset Acquisition and Preparation:\n",
            "Instructions are given on obtaining the dataset, selecting the YOLO V5 model, and organizing the dataset for effective training (00:23:15---->00:26:15).\n",
            "\n",
            "10. Label Interpretation and Dataset Organization:\n",
            "The labeling and organization of the dataset for training are explained, ensuring smooth functioning of the training code (00:25:30---->00:26:15).\n",
            "\n",
            "11. Data Set Movement and Folder Structure:\n",
            "The dataset is moved into a specific folder structure for training, emphasizing the importance of this structure for the training code to function correctly (00:26:15---->00:27:45).\n",
            "\n",
            "12. Training Process Initialization:\n",
            "Setting up the terminal command with Ultralytics for YOLO training to initiate the training process is detailed (00:27:45---->00:29:15).\n",
            "\n",
            "13. Training Execution and Model Optimization:\n",
            "The execution of the training process using Ultralytics and YOLO V5 for object detection refinement is demonstrated (00:29:15---->00:30:00).\n",
            "\n",
            "14. Model Predictions and Annotations:\n",
            "The fine-tuned model is showcased for predictions with neater annotations, providing a clearer representation of objects in video frames (00:36:45---->00:39:00).\n",
            "\n",
            "15. Code Implementation and Video Utilities:\n",
            "The implementation of code for neater annotations and setting up video utilities using the CV2 library is introduced (00:39:00---->00:41:15).\n",
            "\n",
            "16. Video Processing and Tracking:\n",
            "The implementation of video processing and tracking using OpenCV and Python is discussed (00:44:15---->00:47:15).\n",
            "\n",
            "17. Object Detection and Frame Comparison:\n",
            "The process of object detection and frame comparison in video analysis is explained (00:47:15---->00:48:45).\n",
            "\n",
            "18. Object Tracking and Prediction:\n",
            "The importance of object tracking and prediction in video analysis is discussed (00:48:00---->00:49:30).\n",
            "\n",
            "19. Object Tracking and Bounding Box Association:\n",
            "The challenge of associating bounding boxes across frames for accurate object tracking is discussed (00:48:45---->00:49:30).\n",
            "\n",
            "20. Object Tracking and Prediction Techniques:\n",
            "The techniques used for object tracking and prediction in video analysis are elaborated on (00:49:30---->00:50:15).\n",
            "\n",
            "21. Smart Tracking and Trajectory Prediction:\n",
            "The intricacies of smart tracking, involving predicting object trajectories and matching bounding boxes based on visual features, are delved into (00:50:15---->00:51:00).\n",
            "\n",
            "22. Object Tracking Implementation:\n",
            "A step-by-step guide on implementing object tracking using the Bite Tracker in Python is provided (00:51:00---->00:51:45).\n",
            "\n",
            "23. Tracker Class Creation and Functionality:\n",
            "Guidance is given on creating a Tracker class in Python for object tracking in the football analysis system (00:51:45---->00:52:30).\n",
            "\n",
            "24. Tracker Initialization and Object Tracking:\n",
            "The initialization of the Tracker class and the process of tracking objects in video frames are demonstrated (00:52:30---->00:53:15).\n",
            "\n",
            "25. Object Tracking and Drawing:\n",
            "Enhancements in drawing objects, including players and referees, with annotations and track IDs are discussed, improving the visual representation of objects in the video frame (01:27:00---->01:31:30).\n",
            "\n",
            "26. Center Calculation and Width Determination:\n",
            "Functions for calculating the center of a bounding box and determining the width of the bounding box are introduced to facilitate accurate object tracking and visualization in the football analysis system (01:30:00---->01:31:30).\n",
            "\n",
            "27. Object Tracking and Drawing Enhancements:\n",
            "Further enhancements in drawing objects, including players and referees, with annotations and track IDs are discussed, improving the visual representation of objects in the video frame (01:36:00---->01:41:15).\n",
            "\n",
            "28. Cropping and Saving Images:\n",
            "The process of cropping images based on bounding boxes and saving the cropped images is demonstrated, enhancing the efficiency of object detection and analysis (01:57:45---->01:58:30).\n",
            "\n",
            "29. Color Assignment and Segmentation:\n",
            "The assignment of colors to objects, specifically focusing on T-shirt colors for team assignment, is discussed through color clustering and segmentation techniques (01:58:30---->02:03:00).\n",
            "\n",
            "30. Team Color Assignment:\n",
            "The process of segmenting T-shirt colors from the background and assigning team colors based on clustering techniques is explained (02:06:45---->02:11:15).\n",
            "\n",
            "31. Team Assigner Class Creation:\n",
            "A team assigner class is created to assign team colors based on player detections and frame analysis (02:10:30---->02:11:15).\n",
            "\n",
            "32. Player Color Assignment:\n",
            "The assignment of colors to players based on their detections and bounding boxes is discussed, enhancing team color assignment (02:11:15---->02:12:00).\n",
            "\n",
            "33. Player Color Detection:\n",
            "The process of detecting player colors from the frame and player detections is explained, utilizing a function for player color assignment (02:12:00---->02:12:45).\n",
            "\n",
            "34. Player Color Clustering:\n",
            "The clustering of player colors using K-means algorithm is introduced, enhancing the segmentation of team colors for assignment (02:12:45---->02:13:30).\n",
            "\n",
            "35. Image Reshaping and Clustering:\n",
            "The reshaping of images and applying K-means clustering for color segmentation is demonstrated, improving the accuracy of team color assignment (02:13:30---->02:14:15).\n",
            "\n",
            "36. Clustering Model Initialization:\n",
            "The initialization of the clustering model using K-means for color segmentation is explained, optimizing the clustering process for team color assignment (02:14:15---->02:15:00).\n",
            "\n",
            "37. Clustering Algorithm Parameters:\n",
            "The parameters of the K-means clustering algorithm, including the number of clusters and initialization method, are discussed for efficient color segmentation (02:15:00---->02:15:45).\n",
            "\n",
            "38. Clustering Model Fitting:\n",
            "The fitting of the K-means clustering model to the image data for color segmentation is demonstrated, improving the accuracy of team color assignment (02:15:45---->02:16:30).\n",
            "\n",
            "39. Interpolation for Ball Tracking:\n",
            "The video introduces the concept of interpolation for filling in missing values in ball tracking. By utilizing pandas, missing ball positions can be interpolated to enhance the completeness of detections (02:33:00---->02:38:15).\n",
            "\n",
            "40. Ball Position Interpolation and Player Assignment:\n",
            "The process of interpolating ball positions and assigning the player closest to the ball is discussed, enhancing the accuracy of player tracking and ball possession identification (02:37:30---->02:42:45).\n",
            "\n",
            "41. Player Ball Assigner Logic:\n",
            "The video introduces the concept of a player ball assigner class to determine the closest player to the ball based on distance calculations. The logic involves setting a maximum distance between the player and the ball, looping over each player to calculate distances, and assigning the ball to the closest player within the specified distance threshold (02:42:00---->02:46:30).\n",
            "\n",
            "42. Camera Movement Estimation:\n",
            "The integration of camera movement estimation enhances the accuracy of player speed and distance measurements in the AI/ML Football Analysis system. By tracking camera movement using corner features and optical flow, the system can provide more precise analysis of player movements on the field (03:06:45---->03:11:15).\n",
            "\n",
            "43. Speed and Distance Estimation:\n",
            "The creation of a class for speed and distance estimation is discussed, focusing on calculating player speed within a frame window and adding speed and distance to tracks for accurate measurements (04:06:00---->04:12:00).\n",
            "\n",
            "44. Annotation of Speed and Distance:\n",
            "The video further explores the annotation of speed and distance for objects in the video frames, enhancing the visual representation of player movements and interactions on the field. By labeling the speed and distance below the bounding boxes, the system can provide valuable insights into player performance and game dynamics (04:15:00---->04:21:00).\n",
            "\n",
            "Conclusion:\n",
            "The integration of speed and distance estimation, along with enhanced annotations for player tracking, adds significant value to the AI/ML Football Analysis system. By accurately measuring player speed and distance, the system can provide detailed insights into player performance and game strategies. The tutorial offers a comprehensive overview of object detection, tracking, color assignment, image processing, and camera movement estimation, making it a valuable resource for both beginners and experienced machine learning engineers in the field of sports analytics.\n",
            "\n",
            "Timeline Summary:\n",
            "- Object Detection with YOLO (00:03:00---->00:04:30)\n",
            "- YOLO Model Selection (00:05:15---->00:06:45)\n",
            "- Object Detection Inference and Results (00:06:45---->00:11:15)\n",
            "- Output of Object Detection (00:10:30---->00:12:45)\n",
            "- Mapping Results and Class Names (00:13:30---->00:15:00)\n",
            "- Tracking and Bounding Boxes (00:14:15---->00:18:45)\n",
            "- Model Refinement and Data Set Utilization (00:18:00---->00:21:45)\n",
            "- Training and Fine-Tuning YOLO Model (00:21:45---->00:23:15)\n",
            "- Dataset Acquisition and Preparation (00:23:15---->00:26:15)\n",
            "- Label Interpretation and Dataset Organization (00:25:30---->00:26:15)\n",
            "- Data Set Movement and Folder Structure (00:26:15---->00:27:45)\n",
            "- Training Process Initialization (00:27:45---->00:29:15)\n",
            "- Training Execution and Model Optimization (00:29:15---->00:30:00)\n",
            "- Model Predictions and Annotations (00:36:45---->00:39:00)\n",
            "- Code Implementation and Video Utilities (00:39:00---->00:41:15)\n",
            "- Video Processing and Tracking (00:44:15---->00:47:15)\n",
            "- Object Detection and Frame Comparison (00:47:15---->00:48:45)\n",
            "- Object Tracking and Prediction (00:48:00---->00:49:30)\n",
            "- Object Tracking and Bounding Box Association (00:48:45---->00:49:30)\n",
            "- Object Tracking and Prediction Techniques (00:49:30---->00:50:15)\n",
            "- Smart Tracking and Trajectory Prediction (00:50:15---->00:51:00)\n",
            "- Object Tracking Implementation (00:51:00---->00:51:45)\n",
            "- Tracker Class Creation and Functionality (00:51:45---->00:52:30)\n",
            "- Tracker Initialization and Object Tracking (00:52:30---->00:53:15)\n",
            "- Object Tracking and Drawing (01:27:00---->01:31:30)\n",
            "- Center Calculation and Width Determination (01:30:00---->01:31:30)\n",
            "- Object Tracking and Drawing Enhancements (01:36:00---->01:41:15)\n",
            "- Cropping and Saving Images (01:57:45---->01:58:30)\n",
            "- Color Assignment and Segmentation (01:58:30---->02:03:00)\n",
            "- Team Color Assignment (02:06:45---->02:11:15)\n",
            "- Team Assigner Class Creation (02:10:30---->02:11:15)\n",
            "- Player Color Assignment (02:11:15---->02:12:00)\n",
            "- Player Color Detection (02:12:00---->02:12:45)\n",
            "- Player Color Clustering (02:12:45---->02:13:30)\n",
            "- Image Reshaping and Clustering (02:13:30---->02:14:15)\n",
            "- Clustering Model Initialization (02:14:15---->02:15:00)\n",
            "- Clustering Algorithm Parameters (02:15:00---->02:15:45)\n",
            "- Clustering Model Fitting (02:15:45---->02:16:30)\n",
            "- Interpolation for Ball Tracking (02:33:00---->02:38:15)\n",
            "- Ball Position Interpolation and Player Assignment (02:37:30---->02:42:45)\n",
            "- Player Ball Assigner Logic (02:42:00---->02:46:30)\n",
            "- Camera Movement Estimation (03:06:45---->03:11:15)\n",
            "- Speed and Distance Estimation (04:06:00---->04:12:00)\n",
            "- Annotation of Speed and Distance (04:15:00---->04:21:00)\n",
            "\n",
            "Additional Content:\n",
            "The final segments of the video tutorial focus on troubleshooting and refining the AI/ML Football Analysis system. Errors related to frame indexing and distance calculation are addressed, leading to improved accuracy in speed and distance estimation. The completion of the project is celebrated as a significant achievement, offering valuable experience for both beginners and experienced machine learning engineers. The tutorial concludes by highlighting the importance of staying engaged throughout the project for skill development and resume enhancement.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(result['output_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DUhpmyXsycX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Ct6Lpmtw9p",
        "outputId": "ea66b2ff-f331-443c-9e39-afdd27d55fa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTbCjSF9wxGd",
        "outputId": "0315cfb8-583a-4cfd-ccf6-992366099cfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\", the speaker discusses the use of prefabricated pipelines for MLOps in Google Cloud\\'s Vertex AI platform. The speaker highlights the benefits of using these pre-built pipelines for machine learning operations, such as saving time and resources by streamlining the process of deploying and managing machine learning models (00:00-02:30).\\n\\nThe speaker explains how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity (02:30-04:45).\\n\\nFurthermore, the speaker discusses how prefabricated pipelines in Vertex AI are customizable and can be tailored to specific use cases and business needs. This flexibility allows teams to adapt the pipelines to their unique requirements, ensuring that they can meet the demands of their projects effectively (04:45-06:15).\\n\\nOverall, the use of prefabricated pipelines for MLOps in Vertex AI offers a convenient and efficient solution for managing machine learning workflows. By leveraging these pre-built pipelines, teams can streamline their processes, save time and resources, and focus on driving innovation and achieving business goals (06:15-08:00).\\n\\nTimeline:\\n- 00:00-02:30: Introduction to the benefits of using prefabricated pipelines for MLOps in Vertex AI.\\n- 02:30-04:45: Explanation of how prefabricated pipelines can help data scientists and machine learning engineers focus on model building.\\n- 04:45-06:15: Discussion on the customization and flexibility of prefabricated pipelines in Vertex AI.\\n- 06:15-08:00: Summary of the overall advantages of using prefabricated pipelines for MLOps in Vertex AI.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows.\\n\\nConclusion:\\nThe use of prefabricated pipelines for MLOps in Vertex AI offers a convenient and efficient solution for managing machine learning workflows. By leveraging these pre-built pipelines, teams can streamline their processes, save time and resources, and focus on driving innovation and achieving business goals. The customization and flexibility of these pipelines further enhance their utility, allowing teams to adapt them to specific use cases and business needs effectively.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration.\\n\\nConclusion:\\nThe use of prefabricated pipelines for MLOps in Vertex AI offers a convenient and efficient solution for managing machine learning workflows. By leveraging these pre-built pipelines, teams can streamline their processes, save time and resources, and focus on driving innovation and achieving business goals. The customization, flexibility, automation, and orchestration capabilities of Vertex AI make it a valuable tool for data scientists and machine learning engineers looking to optimize their workflows and improve productivity in the field of AI and ML.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can better manage and maintain their code, ensuring operational efficiency in ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can better manage and maintain their code, ensuring operational efficiency in ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can better manage and maintain their code, ensuring operational efficiency in ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:31:22-00:34:07):\\nThe video concludes by summarizing the process of creating a workflow in Vertex AI, deploying a model, and making it available for predictions. Prefabricated training pipelines simplify complex workflows, making tasks such as model development, dataset creation, training, tuning, and deployment straightforward. The speaker emphasizes the ease of custom or continuous evaluation by exporting evaluations to BigQuery and providing the data to Vertex AI for automated monitoring and retraining based on specified metrics. The video also highlights the simplicity of adding advanced capabilities to a basic workflow in Vertex AI. The document further elaborates on the importance of following the provided GitHub repository for additional resources and guidance on utilizing Vertex AI effectively.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:31:22-00:34:07):\\nThe video concludes by summarizing the process of creating a workflow in Vertex AI, deploying a model, and making it available for predictions. Prefabricated training pipelines simplify complex workflows, making tasks such as model development, dataset creation, training, tuning, and deployment straightforward. The speaker emphasizes the ease of custom or continuous evaluation by exporting evaluations to BigQuery and providing the data to Vertex AI for automated monitoring and retraining based on specified metrics. The video also highlights the simplicity of adding advanced capabilities to a basic workflow in Vertex AI. The document further elaborates on the importance of following the provided GitHub repository for additional resources and guidance on utilizing Vertex AI effectively.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:34:11-00:37:45):\\nThe video addresses questions related to the granular level control of endpoint deployment within a region in Google Cloud, the use of Jupyter Lab in Vertex AI, the pricing model of Vertex AI, and the comparison of Vertex AI with other frameworks like Spark, H2O, and ML Flow. The speaker emphasizes the enterprise capabilities of Vertex AI Workbench for development environments and the importance of integrated frameworks for machine learning operations. The discussion highlights the unified orchestration and scheduling features of Vertex AI, making it a comprehensive solution for machine learning workflows. By emphasizing the efficiency and integration of Vertex AI, the video showcases the benefits of using a well-integrated platform for machine learning operations.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:31:22-00:34:07):\\nThe video concludes by summarizing the process of creating a workflow in Vertex AI, deploying a model, and making it available for predictions. Prefabricated training pipelines simplify complex workflows, making tasks such as model development, dataset creation, training, tuning, and deployment straightforward. The speaker emphasizes the ease of custom or continuous evaluation by exporting evaluations to BigQuery and providing the data to Vertex AI for automated monitoring and retraining based on specified metrics. The video also highlights the simplicity of adding advanced capabilities to a basic workflow in Vertex AI. The document further elaborates on the importance of following the provided GitHub repository for additional resources and guidance on utilizing Vertex AI effectively.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:34:11-00:37:45):\\nThe video addresses questions related to the granular level control of endpoint deployment within a region in Google Cloud, the use of Jupyter Lab in Vertex AI, the pricing model of Vertex AI, and the comparison of Vertex AI with other frameworks like Spark, H2O, and ML Flow. The speaker emphasizes the enterprise capabilities of Vertex AI Workbench for development environments and the importance of integrated frameworks for machine learning operations. The discussion highlights the unified orchestration and scheduling features of Vertex AI, making it a comprehensive solution for machine learning workflows. By emphasizing the efficiency and integration of Vertex AI, the video showcases the benefits of using a well-integrated platform for machine learning operations.\\n\\nAutomating Data Preparation with Data Proc and BigQuery:\\nThe video introduces the concept of using Data Proc and BigQuery for data preparation in machine learning workflows. Data Proc allows users to process large datasets with Spark, while BigQuery offers a cloud-based solution for data preparation. The speaker emphasizes the importance of feature engineering, training, and the unified orchestration of different frameworks within Vertex AI. By utilizing Data Proc and BigQuery in conjunction with Vertex AI, users can streamline the data preparation process and enhance the efficiency of their machine learning workflows. The document further elaborates on the benefits of using Data Proc and BigQuery for data preparation, showcasing how these tools can be integrated seamlessly within Vertex AI for optimal performance.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment. Additionally, the document delves into the automation of data preparation using Data Proc and BigQuery, highlighting the seamless integration of these tools within Vertex AI for enhanced machine learning workflows.',\n",
              " 'Abstract:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" introduces the concept of prefabricated pipelines in Google Cloud\\'s Vertex AI platform. The speaker discusses the benefits of using these pre-built pipelines for machine learning operations, emphasizing how they can save time, streamline processes, and improve productivity. The video highlights the customization and flexibility of these pipelines, allowing teams to adapt them to specific use cases and business needs. The video also delves into the automation capabilities of Vertex AI in managing machine learning workflows efficiently. The document further explores the process of creating a Keras model, training the model, and deploying it using Vertex AI.\\n\\nIntroduction:\\nIn the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI,\" the speaker, a director for analytics and AI solutions on Google Cloud, introduces Vertex AI as a comprehensive set of tools for machine learning workflows. The platform supports various model types and levels of ML expertise, offering tools for data readiness, training, model serving, monitoring, and management. The speaker emphasizes the versatility of Vertex AI in accommodating different frameworks and expertise levels, from low code to sophisticated programming. The video also highlights the importance of automation in machine learning operations and how Vertex AI facilitates this process. The document further elaborates on the process of training a Keras model and deploying it using Vertex AI.\\n\\nKey Topics:\\n1. Benefits of Prefabricated Pipelines in Vertex AI (00:00-08:00):\\nThe video discusses how prefabricated pipelines in Vertex AI can help data scientists and machine learning engineers focus on building and improving models, rather than spending time on infrastructure and deployment tasks. By leveraging these pre-built pipelines, teams can accelerate the development and deployment of machine learning models, ultimately leading to faster innovation and improved productivity. The customization and flexibility of these pipelines allow teams to tailor them to specific use cases and business needs, ensuring effective project management. The automation capabilities of Vertex AI further enhance the efficiency of machine learning workflows. The document expands on this by detailing the process of creating, training, and deploying a Keras model using Vertex AI.\\n\\n2. Overview of Vertex AI and Machine Learning Workflows (00:00-03:28):\\nVertex AI is presented as a set of tools that support the entire machine learning workflow, including data readiness, training, hyperparameter tuning, model serving, monitoring, and management. The platform caters to different levels of ML expertise, offering options for low code, no code, and sophisticated programming with frameworks like AutoML, TensorFlow, and PyTorch. The speaker highlights the seamless integration of tools within Vertex AI, allowing for a unified workflow that streamlines processes and enhances efficiency. The video emphasizes the importance of orchestration in machine learning pipelines and how Vertex AI facilitates this process. The document further explores the process of data preparation, model training, and deployment within Vertex AI.\\n\\n3. Customization and Flexibility of Prefabricated Pipelines (04:45-06:15):\\nThe video emphasizes the customizable nature of prefabricated pipelines in Vertex AI, enabling teams to adapt the pipelines to their unique requirements. This flexibility ensures that teams can meet the demands of their projects effectively, regardless of the framework or expertise level they are working with. By allowing for tailored solutions, Vertex AI empowers teams to drive innovation, achieve business goals, and optimize their machine learning workflows. The speaker also discusses the challenges of managing complex machine learning pipelines and how Vertex AI simplifies this process through automation and orchestration. The document further elaborates on the customization and flexibility of prefabricated pipelines within Vertex AI, showcasing how teams can adapt these pipelines to their specific needs.\\n\\nNew Insights:\\nThe video introduces the concept of automating machine learning workflows through prefabricated pipelines in Vertex AI. These pipelines allow for the orchestration of complex tasks such as testing, performance monitoring, and experimentation, while also simplifying the process of developing, training, tuning, and deploying machine learning models. The emphasis on making hard tasks possible and simple tasks easy highlights the efficiency and productivity gains that can be achieved with Vertex AI. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI provides users with a versatile platform that caters to different levels of ML expertise, ultimately driving innovation and business success in the field of AI and ML. The document further explores the process of extracting data, training a model, and deploying it using Vertex AI, showcasing the seamless workflow enabled by the platform.\\n\\nAdditional Context:\\nThe document provides a detailed walkthrough of the process of creating a Keras model, training the model, and deploying it using Vertex AI. The speaker demonstrates how users can leverage Jupyter Lab within Vertex AI to access their notebooks, clone repositories, and run machine learning workflows seamlessly. The document highlights the integration of tools like BigQuery, TensorFlow, and Cloud Storage within Vertex AI, showcasing how users can extract data, perform data preparation, and train models efficiently. By showcasing the end-to-end process of machine learning workflow within Vertex AI, the video provides valuable insights into the capabilities and benefits of using prefabricated pipelines for MLOps.\\n\\nNew Technologies and Tools:\\nThe video introduces the concept of using notebooks for development purposes and the importance of transitioning from notebooks to Python files for production purposes. By moving the code from a notebook to a Python file, users can better manage and maintain their code for operational purposes. The speaker demonstrates the process of exporting a notebook as a Python file and removing unnecessary display elements to create a callable code. This transition allows for the automation of machine learning workflows in Python, ensuring a more streamlined and efficient process. The document further elaborates on the benefits of using Python files for ML operations and the importance of maintaining flat files for operational efficiency.\\n\\nAutomating Machine Learning Workflows with Vertex AI:\\nThe video delves into the automation of machine learning workflows using prefabricated pipelines in Vertex AI. By transitioning from manual tasks to automated processes, data scientists and machine learning engineers can focus on model development and improvement rather than infrastructure and deployment. The speaker demonstrates how to automate the deployment of a trained model using Python scripts, ensuring a streamlined and efficient workflow. By orchestrating tasks such as data preparation, model training, and endpoint deployment within Vertex AI, teams can achieve faster innovation and improved productivity in their machine learning operations.\\n\\nContainerization in Vertex AI:\\nThe video introduces the concept of containerization in Vertex AI, where everything is containerized for efficient machine learning operations. The speaker explains how there are pre-built containers for every version of TensorFlow, allowing users to specify the training and deployment images they want to use. By utilizing these pre-built containers, users can create custom training jobs and specify the hardware configurations for training their models. The document further elaborates on the process of containerization in Vertex AI, showcasing how users can leverage pre-built containers to streamline their machine learning workflows and achieve efficient model training and deployment.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:31:22-00:34:07):\\nThe video concludes by summarizing the process of creating a workflow in Vertex AI, deploying a model, and making it available for predictions. Prefabricated training pipelines simplify complex workflows, making tasks such as model development, dataset creation, training, tuning, and deployment straightforward. The speaker emphasizes the ease of custom or continuous evaluation by exporting evaluations to BigQuery and providing the data to Vertex AI for automated monitoring and retraining based on specified metrics. The video also highlights the simplicity of adding advanced capabilities to a basic workflow in Vertex AI. The document further elaborates on the importance of following the provided GitHub repository for additional resources and guidance on utilizing Vertex AI effectively.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:34:11-00:37:45):\\nThe video addresses questions related to the granular level control of endpoint deployment within a region in Google Cloud, the use of Jupyter Lab in Vertex AI, the pricing model of Vertex AI, and the comparison of Vertex AI with other frameworks like Spark, H2O, and ML Flow. The speaker emphasizes the enterprise capabilities of Vertex AI Workbench for development environments and the importance of integrated frameworks for machine learning operations. The discussion highlights the unified orchestration and scheduling features of Vertex AI, making it a comprehensive solution for machine learning workflows. By emphasizing the efficiency and integration of Vertex AI, the video showcases the benefits of using a well-integrated platform for machine learning operations.\\n\\nAutomating Data Preparation with Data Proc and BigQuery:\\nThe video introduces the concept of using Data Proc and BigQuery for data preparation in machine learning workflows. Data Proc allows users to process large datasets with Spark, while BigQuery offers a cloud-based solution for data preparation. The speaker emphasizes the importance of feature engineering, training, and the unified orchestration of different frameworks within Vertex AI. By utilizing Data Proc and BigQuery in conjunction with Vertex AI, users can streamline the data preparation process and enhance the efficiency of their machine learning workflows. The document further elaborates on the benefits of using Data Proc and BigQuery for data preparation, showcasing how these tools can be integrated seamlessly within Vertex AI for optimal performance.\\n\\nConclusion:\\nThe youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI\" provides valuable insights into the benefits of using prefabricated pipelines in Vertex AI for machine learning operations. The customization, flexibility, and automation capabilities of Vertex AI streamline the process of developing, training, and deploying machine learning models, ultimately leading to faster innovation and improved productivity. By transitioning from notebooks to Python files for production purposes, users can enhance the operational efficiency of their ML workflows. The integration of tools like AutoML, TensorFlow, and PyTorch within Vertex AI offers a versatile platform that caters to different levels of ML expertise, driving innovation and business success in the field of AI and ML. The document further explores the concept of containerization in Vertex AI, showcasing how pre-built containers can be leveraged for efficient model training and deployment. Additionally, the document delves into the automation of data preparation using Data Proc and BigQuery, highlighting the seamless integration of these tools within Vertex AI for enhanced machine learning workflows.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:26:02-00:29:27):\\nThe video demonstrates the process of automating model deployment and hyperparameter tuning in Vertex AI. By utilizing Python scripts and REST APIs, users can deploy trained models and tune hyperparameters efficiently. The speaker explains how to create a JSON request for model deployment and how to pass data for predictions using Python or REST. Additionally, the video discusses the importance of tuning hyperparameters within the model code and implementing a hyperparameter tuning callback in Keras. By automating the process of hyperparameter tuning and model deployment, teams can optimize their machine learning workflows and achieve better model performance. The document further elaborates on the steps involved in automating model deployment and hyperparameter tuning within Vertex AI, providing a comprehensive guide for users to enhance their ML operations.\\n\\nAutomating Model Deployment and Hyperparameter Tuning (00:39:43-00:43:39):\\nThe video discusses the deployment options available in Vertex AI, allowing users to choose machine types, scaling options, and accelerator types for efficient model deployment. The speaker explains the flexibility in deployment choices, such as specifying auto-scaling or specific machine types supported on Google Cloud. Additionally, the video introduces the Vertex AI feature store for inferencing and highlights the impact of AutoML on data science and machine learning jobs. The speaker emphasizes that AutoML does not replace data scientists but provides a faster way to market and benchmark models. The video also touches on using Dataflow for IoT applications and showcases companies like Telus using Apache Beam and TensorFlow for processing millions of events per second. The document further elaborates on the deployment options, feature store, and the impact of AutoML on data science jobs, providing insights into the evolving landscape of machine learning operations.\\n\\nRefinement:\\nThe document provides a comprehensive overview of the benefits, tools, and technologies discussed in the youtube video \"DevFest21 - AI/ML: Leveraging Prefabricated Pipelines for MLOps in Vertex AI.\" It highlights the importance of prefabricated pipelines in Vertex AI for accelerating machine learning workflows and improving productivity. The document explores the customization, flexibility, and automation capabilities of Vertex AI, showcasing how teams can adapt these pipelines to their specific needs and drive innovation in AI and ML. Additionally, the document delves into new technologies and tools introduced in the video, such as automating model deployment, hyperparameter tuning, and containerization in Vertex AI. By providing a detailed analysis of these key topics, the document aims to offer valuable insights for readers with a technical background seeking to enhance their understanding of machine learning operations in Vertex AI.']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['intermediate_steps']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTpG1Ry3w1X5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
